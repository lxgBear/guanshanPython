"""
æœç´¢ç»“æœå‰ç«¯APIç«¯ç‚¹

ä½œä¸ºæœç´¢ä»»åŠ¡çš„å­èµ„æºï¼Œæä¾›ä»»åŠ¡ç›¸å…³çš„ç»“æœæŸ¥è¯¢åŠŸèƒ½ã€‚
éµå¾ªRESTfulè®¾è®¡ï¼Œè·¯å¾„æ ¼å¼ï¼š/search-tasks/{task_id}/results
"""

from datetime import datetime
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from src.core.domain.entities.search_result import SearchResult, ResultStatus
from src.infrastructure.database.repositories import SearchTaskRepository, SearchResultRepository
from src.infrastructure.database.memory_repositories import InMemorySearchTaskRepository
from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter(prefix="/search-tasks", tags=["ğŸ“Š æœç´¢ç»“æœæŸ¥è¯¢"])

# ä»“å‚¨å®ä¾‹
task_repository = None
result_repository = None


async def get_task_repository():
    """è·å–ä»»åŠ¡ä»“å‚¨å®ä¾‹"""
    global task_repository
    if task_repository is None:
        try:
            await get_mongodb_database()
            task_repository = SearchTaskRepository()
            logger.info("ä½¿ç”¨MongoDBä»»åŠ¡ä»“å‚¨")
        except Exception as e:
            logger.warning(f"MongoDBä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å­˜ä»“å‚¨: {e}")
            task_repository = InMemorySearchTaskRepository()
    return task_repository


async def get_result_repository():
    """è·å–ç»“æœä»“å‚¨å®ä¾‹"""
    global result_repository
    if result_repository is None:
        try:
            await get_mongodb_database()
            result_repository = SearchResultRepository()
            logger.info("ä½¿ç”¨MongoDBç»“æœä»“å‚¨")
        except Exception as e:
            logger.warning(f"MongoDBä¸å¯ç”¨ï¼Œç»“æœæŸ¥è¯¢å°†å¤±è´¥: {e}")
            raise HTTPException(503, "ç»“æœä»“å‚¨ä¸å¯ç”¨")
    return result_repository


# ==========================================
# Pydantic æ•°æ®æ¨¡å‹
# ==========================================

class SearchResultResponse(BaseModel):
    """æœç´¢ç»“æœå“åº” - ç²¾ç®€ç‰ˆ(ä»…ä¿ç•™å‰ç«¯å¿…éœ€å­—æ®µ)"""
    id: str = Field(..., description="ç»“æœID")
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    title: str = Field(..., description="æ ‡é¢˜")
    url: str = Field(..., description="é“¾æ¥åœ°å€")
    content: str = Field(..., description="å†…å®¹")
    snippet: Optional[str] = Field(None, description="å†…å®¹æ‘˜è¦")
    source: str = Field(..., description="æ¥æº")
    markdown_content: Optional[str] = Field(None, description="Markdownæ ¼å¼å†…å®¹(æœ€å¤§5000å­—ç¬¦)")
    html_content: Optional[str] = Field(None, description="HTMLæ ¼å¼å†…å®¹(ç”¨äºå¯Œæ–‡æœ¬æ˜¾ç¤ºå’Œåˆ†æ)")
    article_tag: Optional[str] = Field(None, description="æ–‡ç« æ ‡ç­¾")
    article_published_time: Optional[str] = Field(None, description="æ–‡ç« å‘å¸ƒæ—¶é—´")
    # å·²ç§»é™¤å­—æ®µ:
    # - published_date, author, language (ä¸šåŠ¡å­—æ®µ)
    # - raw_data (å†—ä½™å¤§å­—æ®µ)
    # - relevance_score, quality_score (è¯„åˆ†å­—æ®µ)
    # - status, created_at, processed_at (çŠ¶æ€å­—æ®µ)
    # - is_test_data (æµ‹è¯•æ ‡è®°)
    # - metadata (åªåœ¨å†…éƒ¨ä½¿ç”¨)


class SearchResultListResponse(BaseModel):
    """æœç´¢ç»“æœåˆ—è¡¨å“åº”"""
    items: List[SearchResultResponse] = Field(..., description="ç»“æœåˆ—è¡¨")
    total: int = Field(..., description="æ€»æ•°é‡")
    page: int = Field(..., description="å½“å‰é¡µç ")
    page_size: int = Field(..., description="æ¯é¡µå¤§å°")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    task_name: str = Field(..., description="ä»»åŠ¡åç§°")


class SearchResultStats(BaseModel):
    """æœç´¢ç»“æœç»Ÿè®¡"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    task_name: str = Field(..., description="ä»»åŠ¡åç§°")
    total_results: int = Field(..., description="ç»“æœæ€»æ•°")
    processed_count: int = Field(..., description="å·²å¤„ç†æ•°é‡")
    pending_count: int = Field(..., description="å¾…å¤„ç†æ•°é‡")
    failed_count: int = Field(..., description="å¤±è´¥æ•°é‡")
    average_relevance_score: float = Field(..., ge=0, le=1, description="å¹³å‡ç›¸å…³æ€§è¯„åˆ†")
    average_quality_score: float = Field(..., ge=0, le=1, description="å¹³å‡è´¨é‡è¯„åˆ†")
    sources_distribution: Dict[str, int] = Field(..., description="æ¥æºåˆ†å¸ƒ")
    languages_distribution: Dict[str, int] = Field(..., description="è¯­è¨€åˆ†å¸ƒ")
    date_range: Dict[str, Optional[datetime]] = Field(..., description="æ—¥æœŸèŒƒå›´")
    last_updated: datetime = Field(..., description="æœ€åæ›´æ–°æ—¶é—´")


class SearchResultSummary(BaseModel):
    """æœç´¢ç»“æœæ‘˜è¦ï¼ˆç”¨äºä»»åŠ¡è¯¦æƒ…é¡µé¢ï¼‰"""
    total_results: int = Field(..., description="æ€»ç»“æœæ•°")
    recent_results: List[SearchResultResponse] = Field(..., description="æœ€è¿‘ç»“æœï¼ˆæœ€å¤š5æ¡ï¼‰")
    stats: SearchResultStats = Field(..., description="ç»Ÿè®¡ä¿¡æ¯")


# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def result_to_response(result: SearchResult) -> SearchResultResponse:
    """å°†ç»“æœå®ä½“è½¬æ¢ä¸ºå“åº”æ¨¡å‹"""
    return SearchResultResponse(
        id=str(result.id),
        task_id=str(result.task_id),
        title=result.title,
        url=result.url,
        content=result.content,
        snippet=result.snippet,
        source=result.source,
        markdown_content=result.markdown_content,
        html_content=result.html_content,
        article_tag=result.article_tag,
        article_published_time=result.article_published_time,
        # å·²ç§»é™¤æ˜ å°„: published_date, author, language, raw_data,
        # relevance_score, quality_score, status, created_at, processed_at,
        # is_test_data, metadata
    )


async def validate_task_exists(task_id: str) -> str:
    """éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨ï¼Œè¿”å›ä»»åŠ¡åç§°"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
    return task.name


def calculate_result_stats(task_id: str, task_name: str, results: List[SearchResult]) -> SearchResultStats:
    """è®¡ç®—æœç´¢ç»“æœç»Ÿè®¡ä¿¡æ¯"""
    if not results:
        return SearchResultStats(
            task_id=task_id,
            task_name=task_name,
            total_results=0,
            processed_count=0,
            pending_count=0,
            failed_count=0,
            average_relevance_score=0.0,
            average_quality_score=0.0,
            sources_distribution={},
            languages_distribution={},
            date_range={"min_date": None, "max_date": None},
            last_updated=datetime.utcnow()
        )

    # çŠ¶æ€ç»Ÿè®¡
    status_counts = {status.value: 0 for status in ResultStatus}
    sources_dist = {}
    languages_dist = {}
    total_relevance = 0.0
    total_quality = 0.0
    min_date = None
    max_date = None

    for result in results:
        # çŠ¶æ€ç»Ÿè®¡
        status_counts[result.status.value] += 1

        # æ¥æºç»Ÿè®¡
        sources_dist[result.source] = sources_dist.get(result.source, 0) + 1

        # è¯­è¨€ç»Ÿè®¡
        if result.language:
            languages_dist[result.language] = languages_dist.get(result.language, 0) + 1

        # åˆ†æ•°ç»Ÿè®¡
        total_relevance += result.relevance_score
        total_quality += result.quality_score

        # æ—¥æœŸèŒƒå›´
        if result.published_date:
            if not min_date or result.published_date < min_date:
                min_date = result.published_date
            if not max_date or result.published_date > max_date:
                max_date = result.published_date

    total_count = len(results)

    return SearchResultStats(
        task_id=task_id,
        task_name=task_name,
        total_results=total_count,
        processed_count=status_counts.get("processed", 0),
        pending_count=status_counts.get("pending", 0),
        failed_count=status_counts.get("failed", 0),
        average_relevance_score=total_relevance / total_count if total_count > 0 else 0.0,
        average_quality_score=total_quality / total_count if total_count > 0 else 0.0,
        sources_distribution=sources_dist,
        languages_distribution=languages_dist,
        date_range={
            "min_date": min_date,
            "max_date": max_date
        },
        last_updated=datetime.utcnow()
    )


# ==========================================
# APIç«¯ç‚¹
# ==========================================

@router.get(
    "/{task_id}/results",
    response_model=SearchResultListResponse,
    summary="è·å–ä»»åŠ¡æœç´¢ç»“æœåˆ—è¡¨",
    description="è·å–æŒ‡å®šæœç´¢ä»»åŠ¡çš„æ‰€æœ‰å†å²æœç´¢ç»“æœï¼Œæ”¯æŒåˆ†é¡µã€è¿‡æ»¤å’Œæ’åºåŠŸèƒ½ã€‚"
)
async def get_task_results(
    task_id: str,
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µå¤§å°"),
    source: Optional[str] = Query(None, description="æ¥æºè¿‡æ»¤"),
    language: Optional[str] = Query(None, description="è¯­è¨€è¿‡æ»¤"),
    min_relevance_score: Optional[float] = Query(None, ge=0, le=1, description="æœ€å°ç›¸å…³æ€§è¯„åˆ†"),
    min_quality_score: Optional[float] = Query(None, ge=0, le=1, description="æœ€å°è´¨é‡è¯„åˆ†"),
    sort_by: str = Query("created_at", description="æ’åºå­—æ®µ: created_at, relevance_score, quality_score, published_date"),
    order: str = Query("desc", description="æ’åºæ–¹å‘: asc, desc")
):
    """è·å–æŒ‡å®šä»»åŠ¡çš„å†å²æœç´¢ç»“æœ - ä»æ•°æ®åº“è¯»å–"""

    # éªŒè¯ä»»åŠ¡å­˜åœ¨
    task_name = await validate_task_exists(task_id)

    # è·å–ç»“æœä»“å‚¨
    result_repo = await get_result_repository()

    # ä»æ•°æ®åº“è·å–æ‰€æœ‰ç»“æœï¼ˆç”¨äºè¿‡æ»¤å’Œæ’åºï¼‰
    all_results, total_count = await result_repo.get_results_by_task(
        task_id=task_id,
        page=1,
        page_size=10000  # è·å–æ‰€æœ‰ç»“æœ
    )

    if not all_results:
        return SearchResultListResponse(
            items=[],
            total=0,
            page=page,
            page_size=page_size,
            total_pages=0,
            task_id=task_id,
            task_name=task_name
        )

    # è¿‡æ»¤
    filtered_results = all_results

    if source:
        filtered_results = [r for r in filtered_results if r.source == source]

    if language:
        filtered_results = [r for r in filtered_results if r.language == language]

    if min_relevance_score is not None:
        filtered_results = [r for r in filtered_results if r.relevance_score >= min_relevance_score]

    if min_quality_score is not None:
        filtered_results = [r for r in filtered_results if r.quality_score >= min_quality_score]

    # æ’åº
    sort_fields = {
        "created_at": lambda r: r.created_at,
        "relevance_score": lambda r: r.relevance_score,
        "quality_score": lambda r: r.quality_score,
        "published_date": lambda r: r.published_date or datetime.min
    }

    if sort_by in sort_fields:
        filtered_results.sort(
            key=sort_fields[sort_by],
            reverse=(order == "desc")
        )

    # åˆ†é¡µ
    total = len(filtered_results)
    total_pages = (total + page_size - 1) // page_size
    start = (page - 1) * page_size
    end = min(start + page_size, total)

    page_results = filtered_results[start:end]

    return SearchResultListResponse(
        items=[result_to_response(r) for r in page_results],
        total=total,
        page=page,
        page_size=page_size,
        total_pages=total_pages,
        task_id=task_id,
        task_name=task_name
    )


@router.get(
    "/{task_id}/results/stats",
    response_model=SearchResultStats,
    summary="è·å–ä»»åŠ¡æœç´¢ç»“æœç»Ÿè®¡",
    description="è·å–æŒ‡å®šæœç´¢ä»»åŠ¡çš„ç»“æœç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ•°é‡åˆ†å¸ƒã€è¯„åˆ†æƒ…å†µã€æ¥æºåˆ†æç­‰ã€‚"
)
async def get_task_result_stats(task_id: str):
    """è·å–ä»»åŠ¡æœç´¢ç»“æœç»Ÿè®¡ - ä»æ•°æ®åº“è¯»å–"""

    # éªŒè¯ä»»åŠ¡å­˜åœ¨
    task_name = await validate_task_exists(task_id)

    # è·å–ç»“æœä»“å‚¨
    result_repo = await get_result_repository()

    # ä»æ•°æ®åº“è·å–æ‰€æœ‰ç»“æœ
    task_results, _ = await result_repo.get_results_by_task(
        task_id=task_id,
        page=1,
        page_size=10000
    )

    return calculate_result_stats(task_id, task_name, task_results)


@router.get(
    "/{task_id}/results/summary",
    response_model=SearchResultSummary,
    summary="è·å–ä»»åŠ¡ç»“æœæ‘˜è¦",
    description="è·å–ä»»åŠ¡æœç´¢ç»“æœçš„æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç»Ÿè®¡æ•°æ®å’Œæœ€è¿‘ç»“æœï¼Œé€‚ç”¨äºä»»åŠ¡è¯¦æƒ…é¡µé¢å±•ç¤ºã€‚"
)
async def get_task_result_summary(task_id: str):
    """è·å–ä»»åŠ¡ç»“æœæ‘˜è¦ - ä»æ•°æ®åº“è¯»å–"""

    # éªŒè¯ä»»åŠ¡å­˜åœ¨
    task_name = await validate_task_exists(task_id)

    # è·å–ç»“æœä»“å‚¨
    result_repo = await get_result_repository()

    # ä»æ•°æ®åº“è·å–æ‰€æœ‰ç»“æœ
    task_results, _ = await result_repo.get_results_by_task(
        task_id=task_id,
        page=1,
        page_size=10000
    )

    # è·å–æœ€è¿‘çš„5æ¡ç»“æœ
    recent_results = sorted(task_results, key=lambda r: r.created_at, reverse=True)[:5]

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = calculate_result_stats(task_id, task_name, task_results)

    return SearchResultSummary(
        total_results=len(task_results),
        recent_results=[result_to_response(r) for r in recent_results],
        stats=stats
    )


@router.get(
    "/{task_id}/results/{result_id}",
    response_model=SearchResultResponse,
    summary="è·å–å•ä¸ªæœç´¢ç»“æœè¯¦æƒ…",
    description="è·å–æŒ‡å®šæœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®Œæ•´å†…å®¹ã€å…ƒæ•°æ®ç­‰ã€‚"
)
async def get_search_result_detail(task_id: str, result_id: str):
    """è·å–å•ä¸ªæœç´¢ç»“æœè¯¦æƒ… - ä»æ•°æ®åº“è¯»å–"""

    # éªŒè¯ä»»åŠ¡å­˜åœ¨
    await validate_task_exists(task_id)

    # è·å–ç»“æœä»“å‚¨
    result_repo = await get_result_repository()

    # ä»æ•°æ®åº“æŸ¥æ‰¾ç»“æœ
    task_results, _ = await result_repo.get_results_by_task(
        task_id=task_id,
        page=1,
        page_size=10000
    )

    for result in task_results:
        if str(result.id) == result_id:
            return result_to_response(result)

    raise HTTPException(404, f"æœç´¢ç»“æœä¸å­˜åœ¨: {result_id}")