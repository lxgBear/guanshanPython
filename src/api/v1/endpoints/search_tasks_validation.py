"""
æœç´¢ä»»åŠ¡å­—æ®µéªŒè¯é€»è¾‘

æä¾› crawl_url å’Œ include_domains çš„äº’æ–¥éªŒè¯
"""

from typing import Optional, Dict, Any, List
from pydantic import BaseModel, field_validator, model_validator
from fastapi import HTTPException


class SearchTaskFieldValidator:
    """æœç´¢ä»»åŠ¡å­—æ®µéªŒè¯å™¨"""

    @staticmethod
    def validate_crawl_url_and_include_domains(
        crawl_url: Optional[str],
        search_config: Dict[str, Any]
    ) -> None:
        """
        éªŒè¯ crawl_url å’Œ include_domains çš„äº’æ–¥å…³ç³»

        è§„åˆ™:
        1. crawl_url å’Œ include_domains å¯ä»¥åŒæ—¶å­˜åœ¨ï¼ˆä¸æŠ¥é”™ï¼‰
        2. ä½†ä¼šè®°å½•è­¦å‘Šæ—¥å¿—ï¼Œæç¤ºç”¨æˆ· include_domains ä¼šè¢«å¿½ç•¥
        3. å»ºè®®ç”¨æˆ·åªè®¾ç½®å…¶ä¸­ä¸€ä¸ª

        Args:
            crawl_url: çˆ¬å–URL
            search_config: æœç´¢é…ç½®

        Raises:
            HTTPException: å¦‚æœé…ç½®ä¸åˆç†
        """
        include_domains = search_config.get('include_domains', [])

        # æƒ…å†µ1: crawl_url å’Œ include_domains éƒ½å­˜åœ¨
        if crawl_url and include_domains:
            # è®°å½•è­¦å‘Šä½†ä¸æŠ¥é”™ï¼ˆå‘åå…¼å®¹ï¼‰
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                f"âš ï¸ crawl_url å’Œ include_domains åŒæ—¶è®¾ç½®ï¼Œ"
                f"include_domains å°†è¢«å¿½ç•¥ã€‚"
                f"å»ºè®®ï¼šä½¿ç”¨ crawl_url æ—¶ï¼Œæ— éœ€è®¾ç½® include_domainsã€‚"
            )

    @staticmethod
    def validate_mode_fields(
        crawl_url: Optional[str],
        query: str,
        search_config: Dict[str, Any]
    ) -> None:
        """
        éªŒè¯æ¨¡å¼å­—æ®µçš„å®Œæ•´æ€§

        Args:
            crawl_url: çˆ¬å–URL
            query: æœç´¢å…³é”®è¯
            search_config: æœç´¢é…ç½®

        Raises:
            HTTPException: å¦‚æœé…ç½®ä¸å®Œæ•´
        """
        # Crawl æ¨¡å¼ï¼šcrawl_url å¿…å¡«
        if crawl_url:
            # Crawl æ¨¡å¼ä¸‹ï¼Œå…¶ä»–å­—æ®µå¯é€‰
            return

        # Search æ¨¡å¼ï¼šquery å¿…å¡«
        if not query or not query.strip():
            raise HTTPException(
                status_code=400,
                detail="Search æ¨¡å¼ä¸‹ï¼Œquery å­—æ®µä¸èƒ½ä¸ºç©º"
            )

        # Search æ¨¡å¼å»ºè®®ï¼ˆä¸å¼ºåˆ¶ï¼‰ï¼šé…ç½® include_domains
        include_domains = search_config.get('include_domains', [])
        if not include_domains:
            import logging
            logger = logging.getLogger(__name__)
            logger.info(
                "ğŸ’¡ å»ºè®®ï¼šSearch æ¨¡å¼ä¸‹é…ç½® include_domains å¯ä»¥æé«˜æœç´¢ç²¾å‡†åº¦ã€‚"
            )

    @staticmethod
    def suggest_optimal_config(
        crawl_url: Optional[str],
        search_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å»ºè®®æœ€ä¼˜é…ç½®ï¼ˆæ¸…ç†å†—ä½™å­—æ®µï¼‰

        Args:
            crawl_url: çˆ¬å–URL
            search_config: æœç´¢é…ç½®

        Returns:
            ä¼˜åŒ–åçš„ search_config
        """
        if crawl_url:
            # Crawl æ¨¡å¼ï¼šç§»é™¤ include_domainsï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰
            optimized_config = search_config.copy()
            if 'include_domains' in optimized_config:
                import logging
                logger = logging.getLogger(__name__)
                logger.info(
                    "ğŸ”§ ä¼˜åŒ–å»ºè®®ï¼šCrawl æ¨¡å¼ä¸‹ï¼Œå·²è‡ªåŠ¨å¿½ç•¥ include_domains é…ç½®ã€‚"
                )
            return optimized_config
        else:
            # Search æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰é…ç½®
            return search_config


def validate_task_creation(
    crawl_url: Optional[str],
    query: str,
    search_config: Dict[str, Any]
) -> None:
    """
    ä»»åŠ¡åˆ›å»ºéªŒè¯ï¼ˆä¸»å…¥å£å‡½æ•°ï¼‰

    Args:
        crawl_url: çˆ¬å–URL
        query: æœç´¢å…³é”®è¯
        search_config: æœç´¢é…ç½®

    Raises:
        HTTPException: å¦‚æœéªŒè¯å¤±è´¥
    """
    validator = SearchTaskFieldValidator()

    # 1. éªŒè¯äº’æ–¥å…³ç³»ï¼ˆè­¦å‘Šä½†ä¸æŠ¥é”™ï¼‰
    validator.validate_crawl_url_and_include_domains(crawl_url, search_config)

    # 2. éªŒè¯æ¨¡å¼å­—æ®µå®Œæ•´æ€§
    validator.validate_mode_fields(crawl_url, query, search_config)


def get_task_mode_description(
    crawl_url: Optional[str],
    search_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    è·å–ä»»åŠ¡æ¨¡å¼æè¿°ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰

    Args:
        crawl_url: çˆ¬å–URL
        search_config: æœç´¢é…ç½®

    Returns:
        æ¨¡å¼æè¿°å­—å…¸
    """
    include_domains = search_config.get('include_domains', [])

    if crawl_url:
        return {
            "mode": "crawl",
            "mode_display": "URLçˆ¬å–æ¨¡å¼",
            "description": f"ç›´æ¥çˆ¬å–: {crawl_url}",
            "api_used": "Firecrawl Scrape API",
            "active_fields": ["crawl_url"],
            "ignored_fields": ["query", "include_domains"],
            "warning": "include_domains åœ¨æ­¤æ¨¡å¼ä¸‹ä¸ç”Ÿæ•ˆ" if include_domains else None
        }
    else:
        return {
            "mode": "search",
            "mode_display": "å…³é”®è¯æœç´¢æ¨¡å¼",
            "description": "åŸºäºå…³é”®è¯æœç´¢å¤šä¸ªæ¥æº",
            "api_used": "Firecrawl Search API",
            "active_fields": ["query", "include_domains"],
            "ignored_fields": ["crawl_url"],
            "warning": None
        }
