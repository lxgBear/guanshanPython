"""
æœç´¢ä»»åŠ¡å‰ç«¯APIç«¯ç‚¹

ä¸“ä¸ºå‰ç«¯è®¾è®¡çš„æ¸…æ´APIæ¥å£ï¼Œéµå¾ªRESTfulè®¾è®¡åŸåˆ™ã€‚
éšè—ç³»ç»Ÿå†…éƒ¨æ¥å£ï¼Œåªæš´éœ²å‰ç«¯å¿…éœ€çš„åŠŸèƒ½ã€‚
"""

from datetime import datetime
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel, Field

from src.core.domain.entities.search_task import SearchTask, TaskStatus, ScheduleInterval
from src.infrastructure.database.repositories import SearchTaskRepository
from src.infrastructure.database.memory_repositories import InMemorySearchTaskRepository
from src.infrastructure.database.connection import get_mongodb_database
from src.services.task_scheduler import get_scheduler
from src.utils.logger import get_logger
from src.api.v1.endpoints.search_tasks_validation import (
    validate_task_creation,
    get_task_mode_description
)

logger = get_logger(__name__)
router = APIRouter(prefix="/search-tasks", tags=["ğŸ” æœç´¢ä»»åŠ¡ç®¡ç†"])

# ä»»åŠ¡ä»“å‚¨å®ä¾‹
task_repository = None


async def get_task_repository():
    """è·å–ä»»åŠ¡ä»“å‚¨å®ä¾‹"""
    global task_repository
    if task_repository is None:
        try:
            await get_mongodb_database()
            task_repository = SearchTaskRepository()
            logger.info("ä½¿ç”¨MongoDBä»“å‚¨")
        except Exception as e:
            logger.warning(f"MongoDBä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å­˜ä»“å‚¨: {e}")
            task_repository = InMemorySearchTaskRepository()
    return task_repository


# ==========================================
# Pydantic æ•°æ®æ¨¡å‹
# ==========================================

class SearchTaskCreate(BaseModel):
    """åˆ›å»ºæœç´¢ä»»åŠ¡è¯·æ±‚"""
    name: str = Field(..., description="ä»»åŠ¡åç§°", min_length=1, max_length=100)
    description: Optional[str] = Field(None, description="ä»»åŠ¡æè¿°", max_length=500)
    query: Optional[str] = Field(None, description="æœç´¢å…³é”®è¯ï¼ˆSEARCH_KEYWORDæ¨¡å¼å¿…å¡«ï¼‰", min_length=1, max_length=200)
    target_website: Optional[str] = Field(None, description="ä¸»è¦ç›®æ ‡ç½‘ç«™ï¼ˆä¾‹å¦‚ï¼šwww.gnlm.com.mmï¼‰", max_length=200)
    crawl_url: Optional[str] = Field(None, description="çˆ¬å–çš„URLï¼ˆCRAWL_WEBSITEå’ŒSCRAPE_URLæ¨¡å¼å¿…å¡«ï¼‰", max_length=500)

    # v2.0.0 æ–°å¢ï¼šä»»åŠ¡ç±»å‹
    task_type: Optional[str] = Field(
        None,
        description="ä»»åŠ¡ç±»å‹ï¼šsearch_keywordï¼ˆå…³é”®è¯æœç´¢ï¼‰ã€crawl_websiteï¼ˆç½‘ç«™çˆ¬å–ï¼‰ã€scrape_urlï¼ˆå•é¡µé¢çˆ¬å–ï¼‰",
        pattern="^(search_keyword|crawl_website|scrape_url)$"
    )

    # é…ç½®å­—æ®µ
    search_config: Optional[Dict[str, Any]] = Field(
        default_factory=dict,
        description="æœç´¢é…ç½®ï¼ˆç”¨äºSEARCH_KEYWORDå’ŒSCRAPE_URLæ¨¡å¼ï¼‰"
    )
    crawl_config: Optional[Dict[str, Any]] = Field(
        default_factory=dict,
        description="ç½‘ç«™çˆ¬å–é…ç½®ï¼ˆç”¨äºCRAWL_WEBSITEæ¨¡å¼ï¼‰"
    )

    schedule_interval: str = Field("DAILY", description="è°ƒåº¦é—´éš”")
    is_active: bool = Field(True, description="æ˜¯å¦å¯ç”¨")
    execute_immediately: bool = Field(True, description="åˆ›å»ºåæ˜¯å¦ç«‹å³æ‰§è¡Œä¸€æ¬¡")
    
    class Config:
        json_schema_extra = {
            "examples": [
                {
                    "name": "ç¤ºä¾‹1ï¼šå…³é”®è¯æœç´¢ä»»åŠ¡",
                    "value": {
                        "name": "AIæ–°é—»ç›‘æ§",
                        "description": "ç›‘æ§äººå·¥æ™ºèƒ½é¢†åŸŸæœ€æ–°è¿›å±•",
                        "query": "äººå·¥æ™ºèƒ½ æ·±åº¦å­¦ä¹  æœ€æ–°è¿›å±•",
                        "task_type": "search_keyword",
                        "target_website": "www.36kr.com",
                        "search_config": {
                            "limit": 10,
                            "language": "zh",
                            "enable_detail_scrape": True,
                            "max_concurrent_scrapes": 3,
                            "include_domains": ["www.36kr.com", "tech.sina.com.cn"]
                        },
                        "schedule_interval": "DAILY",
                        "is_active": True,
                        "execute_immediately": True
                    }
                },
                {
                    "name": "ç¤ºä¾‹2ï¼šç½‘ç«™çˆ¬å–ä»»åŠ¡",
                    "value": {
                        "name": "æŠ€æœ¯åšå®¢å½’æ¡£",
                        "description": "å®šæœŸçˆ¬å–æŠ€æœ¯åšå®¢çš„æ‰€æœ‰æ–‡ç« ",
                        "crawl_url": "https://example.com/blog",
                        "task_type": "crawl_website",
                        "crawl_config": {
                            "limit": 100,
                            "max_depth": 3,
                            "include_paths": ["/blog/*", "/articles/*"],
                            "exclude_paths": ["/admin/*"],
                            "only_main_content": True
                        },
                        "schedule_interval": "WEEKLY",
                        "is_active": True,
                        "execute_immediately": False
                    }
                },
                {
                    "name": "ç¤ºä¾‹3ï¼šå•é¡µé¢çˆ¬å–ä»»åŠ¡",
                    "value": {
                        "name": "å®˜ç½‘é¦–é¡µç›‘æ§",
                        "description": "å®šæœŸç›‘æ§å®˜ç½‘é¦–é¡µå†…å®¹å˜åŒ–",
                        "crawl_url": "https://example.com",
                        "task_type": "scrape_url",
                        "search_config": {
                            "only_main_content": True,
                            "wait_for": 2000,
                            "exclude_tags": ["nav", "footer", "header"]
                        },
                        "schedule_interval": "HOURLY",
                        "is_active": True,
                        "execute_immediately": True
                    }
                }
            ]
        }


class SearchTaskUpdate(BaseModel):
    """æ›´æ–°æœç´¢ä»»åŠ¡è¯·æ±‚"""
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    query: Optional[str] = Field(None, min_length=1, max_length=200)
    target_website: Optional[str] = Field(None, max_length=200)
    crawl_url: Optional[str] = Field(None, max_length=500)

    # v2.0.0 æ–°å¢ï¼šä»»åŠ¡ç±»å‹
    task_type: Optional[str] = Field(
        None,
        description="ä»»åŠ¡ç±»å‹ï¼šsearch_keywordã€crawl_websiteã€scrape_url",
        pattern="^(search_keyword|crawl_website|scrape_url)$"
    )

    search_config: Optional[Dict[str, Any]] = None
    crawl_config: Optional[Dict[str, Any]] = None
    schedule_interval: Optional[str] = None
    is_active: Optional[bool] = None


class SearchTaskStatusUpdate(BaseModel):
    """ä»»åŠ¡çŠ¶æ€æ›´æ–°è¯·æ±‚"""
    is_active: bool = Field(..., description="æ˜¯å¦å¯ç”¨ä»»åŠ¡")


class SearchTaskResponse(BaseModel):
    """æœç´¢ä»»åŠ¡å“åº”ï¼ˆç»Ÿä¸€çš„ä»»åŠ¡ä¿¡æ¯æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„æ‰§è¡Œç»Ÿè®¡å’ŒçŠ¶æ€ä¿¡æ¯ï¼‰"""
    id: str = Field(..., description="ä»»åŠ¡ID")
    name: str = Field(..., description="ä»»åŠ¡åç§°")
    description: Optional[str] = Field(None, description="ä»»åŠ¡æè¿°")
    query: Optional[str] = Field(None, description="æœç´¢å…³é”®è¯")
    target_website: Optional[str] = Field(None, description="ä¸»è¦ç›®æ ‡ç½‘ç«™")
    crawl_url: Optional[str] = Field(None, description="çˆ¬å–çš„URL")

    # v2.0.0 æ–°å¢ï¼šä»»åŠ¡ç±»å‹
    task_type: str = Field(..., description="ä»»åŠ¡ç±»å‹ï¼šsearch_keywordã€crawl_websiteã€scrape_url")
    task_mode: str = Field(..., description="ä»»åŠ¡æ¨¡å¼æè¿°ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰")

    search_config: Dict[str, Any] = Field(..., description="æœç´¢é…ç½®")
    crawl_config: Optional[Dict[str, Any]] = Field(None, description="ç½‘ç«™çˆ¬å–é…ç½®")
    schedule_interval: str = Field(..., description="è°ƒåº¦é—´éš”å€¼")
    schedule_display: str = Field(..., description="è°ƒåº¦é—´éš”æ˜¾ç¤ºåç§°")
    schedule_description: str = Field(..., description="è°ƒåº¦é—´éš”è¯´æ˜")
    is_active: bool = Field(..., description="æ˜¯å¦å¯ç”¨")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    created_by: str = Field(..., description="åˆ›å»ºè€…")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    updated_at: datetime = Field(..., description="æ›´æ–°æ—¶é—´")
    last_executed_at: Optional[datetime] = Field(None, description="æœ€åæ‰§è¡Œæ—¶é—´")
    next_run_time: Optional[datetime] = Field(None, description="ä¸‹æ¬¡è¿è¡Œæ—¶é—´")
    execution_count: int = Field(..., description="æ€»æ‰§è¡Œæ¬¡æ•°")
    success_count: int = Field(..., description="æˆåŠŸæ¬¡æ•°")
    failure_count: int = Field(..., description="å¤±è´¥æ¬¡æ•°")
    success_rate: float = Field(..., description="æˆåŠŸç‡ï¼ˆ%ï¼‰")
    average_results: float = Field(..., description="å¹³å‡ç»“æœæ•°")
    total_results: int = Field(..., description="æ€»ç»“æœæ•°")
    total_credits_used: int = Field(..., description="æ€»æ¶ˆè€—ç§¯åˆ†")


class SearchTaskListResponse(BaseModel):
    """ä»»åŠ¡åˆ—è¡¨å“åº”"""
    items: List[SearchTaskResponse] = Field(..., description="ä»»åŠ¡åˆ—è¡¨")
    total: int = Field(..., description="æ€»æ•°é‡")
    page: int = Field(..., description="å½“å‰é¡µç ")
    page_size: int = Field(..., description="æ¯é¡µå¤§å°")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")


class ScheduleIntervalOption(BaseModel):
    """è°ƒåº¦é—´éš”é€‰é¡¹"""
    value: str = Field(..., description="é—´éš”å€¼")
    label: str = Field(..., description="æ˜¾ç¤ºæ ‡ç­¾")
    description: str = Field(..., description="è¯¦ç»†è¯´æ˜")
    interval_minutes: int = Field(..., description="é—´éš”åˆ†é’Ÿæ•°")


# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def task_to_response(task: SearchTask) -> SearchTaskResponse:
    """å°†ä»»åŠ¡å®ä½“è½¬æ¢ä¸ºå“åº”æ¨¡å‹"""
    interval = task.get_schedule_interval()
    task_type = task.get_task_type()

    # è·å–ä»»åŠ¡æ¨¡å¼æè¿°
    task_mode_map = {
        "search_keyword": "å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–",
        "crawl_website": "ç½‘ç«™é€’å½’çˆ¬å–",
        "scrape_url": "å•é¡µé¢çˆ¬å–"
    }
    task_mode = task_mode_map.get(task_type.value, task_type.value)

    return SearchTaskResponse(
        id=task.get_id_string(),
        name=task.name,
        description=task.description,
        query=task.query,
        target_website=task.target_website,
        crawl_url=task.crawl_url,
        task_type=task_type.value,
        task_mode=task_mode,
        search_config=task.search_config,
        crawl_config=task.crawl_config if hasattr(task, 'crawl_config') else {},
        schedule_interval=task.schedule_interval,
        schedule_display=interval.display_name,
        schedule_description=interval.description,
        is_active=task.is_active,
        status=task.status.value,
        created_by=task.created_by,
        created_at=task.created_at,
        updated_at=task.updated_at,
        last_executed_at=task.last_executed_at,
        next_run_time=task.next_run_time,
        execution_count=task.execution_count,
        success_count=task.success_count,
        failure_count=task.failure_count,
        success_rate=task.success_rate,
        average_results=task.average_results,
        total_results=task.total_results,
        total_credits_used=task.total_credits_used
    )


# ==========================================
# APIç«¯ç‚¹
# ==========================================

@router.get(
    "/schedule-intervals", 
    response_model=List[ScheduleIntervalOption],
    summary="è·å–è°ƒåº¦é—´éš”é€‰é¡¹",
    description="è·å–æ‰€æœ‰å¯ç”¨çš„ä»»åŠ¡è°ƒåº¦é—´éš”é€‰é¡¹ï¼Œå‰åç«¯é€šè¿‡æ­¤æ¥å£çº¦å®šè°ƒåº¦é…ç½®ã€‚"
)
async def get_schedule_intervals():
    """è·å–æ‰€æœ‰å¯ç”¨çš„è°ƒåº¦é—´éš”é€‰é¡¹"""
    return [interval.to_dict() for interval in ScheduleInterval]


@router.post(
    "",
    response_model=SearchTaskResponse,
    status_code=201,
    summary="åˆ›å»ºæœç´¢ä»»åŠ¡",
    description="åˆ›å»ºæ–°çš„å®šæ—¶æœç´¢ä»»åŠ¡ã€‚ä»»åŠ¡åˆ›å»ºåå°†æŒ‰ç…§æŒ‡å®šçš„è°ƒåº¦é—´éš”è‡ªåŠ¨æ‰§è¡Œæœç´¢ã€‚"
)
async def create_search_task(task_data: SearchTaskCreate):
    """åˆ›å»ºæ–°çš„æœç´¢ä»»åŠ¡"""
    try:
        # éªŒè¯è°ƒåº¦é—´éš”
        try:
            ScheduleInterval.from_value(task_data.schedule_interval)
        except ValueError as e:
            raise HTTPException(400, f"æ— æ•ˆçš„è°ƒåº¦é—´éš”: {str(e)}")

        # éªŒè¯ crawl_url å’Œ include_domains çš„äº’æ–¥å…³ç³»
        validate_task_creation(
            crawl_url=task_data.crawl_url,
            query=task_data.query,
            search_config=task_data.search_config
        )

        # ä½¿ç”¨å®‰å…¨IDåˆ›å»ºä»»åŠ¡
        task = SearchTask.create_with_secure_id(
            name=task_data.name,
            description=task_data.description,
            query=task_data.query or "",  # å…è®¸ä¸ºç©ºï¼ˆcrawl/scrapeæ¨¡å¼ä¸éœ€è¦queryï¼‰
            target_website=task_data.target_website,
            crawl_url=task_data.crawl_url,
            task_type=task_data.task_type,  # v2.0.0 æ–°å¢
            search_config=task_data.search_config or {},
            crawl_config=task_data.crawl_config or {},  # v2.0.0 æ–°å¢
            schedule_interval=task_data.schedule_interval,
            is_active=task_data.is_active,
            created_by="current_user",  # TODO: ä»JWT tokenè·å–ç”¨æˆ·ä¿¡æ¯
            status=TaskStatus.ACTIVE if task_data.is_active else TaskStatus.DISABLED
        )

        # å¦‚æœ target_website ä¸ºç©ºï¼Œè‡ªåŠ¨ä» search_config æå–
        task.sync_target_website()

        # ä¿å­˜åˆ°ä»“å‚¨
        repo = await get_task_repository()
        await repo.create(task)

        logger.info(f"åˆ›å»ºæœç´¢ä»»åŠ¡: {task.name} (ID: {task.get_id_string()}, ç›®æ ‡ç½‘ç«™: {task.target_website})")

        # é¦–æ¬¡ç«‹å³æ‰§è¡Œï¼ˆå¦‚æœå¯ç”¨ä¸” execute_immediately=Trueï¼‰
        if task.is_active and task_data.execute_immediately:
            try:
                scheduler = await get_scheduler()
                if scheduler.is_running():
                    # å¼‚æ­¥è§¦å‘é¦–æ¬¡æ‰§è¡Œï¼ˆä¸é˜»å¡APIå“åº”ï¼‰
                    import asyncio
                    asyncio.create_task(scheduler.execute_task_now(str(task.id)))
                    logger.info(f"âœ… å·²è§¦å‘é¦–æ¬¡ç«‹å³æ‰§è¡Œ: {task.name} (ID: {task.get_id_string()})")
                else:
                    logger.warning(f"âš ï¸ è°ƒåº¦å™¨æœªè¿è¡Œï¼Œè·³è¿‡é¦–æ¬¡æ‰§è¡Œ: {task.name}")
            except Exception as e:
                # é¦–æ¬¡æ‰§è¡Œå¤±è´¥ä¸å½±å“ä»»åŠ¡åˆ›å»º
                logger.warning(f"âš ï¸ è§¦å‘é¦–æ¬¡æ‰§è¡Œå¤±è´¥ï¼ˆä¸å½±å“ä»»åŠ¡åˆ›å»ºï¼‰: {e}")

        return task_to_response(task)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(500, f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get(
    "",
    response_model=SearchTaskListResponse,
    summary="è·å–æœç´¢ä»»åŠ¡åˆ—è¡¨",
    description="è·å–æœç´¢ä»»åŠ¡åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µã€çŠ¶æ€è¿‡æ»¤å’Œæ¨¡ç³ŠæŸ¥è¯¢åŠŸèƒ½ã€‚"
)
async def list_search_tasks(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µå¤§å°"),
    status: Optional[str] = Query(None, description="ä»»åŠ¡çŠ¶æ€è¿‡æ»¤"),
    is_active: Optional[bool] = Query(None, description="å¯ç”¨çŠ¶æ€è¿‡æ»¤"),
    query: Optional[str] = Query(None, description="å…³é”®è¯æ¨¡ç³ŠæŸ¥è¯¢")
):
    """è·å–æœç´¢ä»»åŠ¡åˆ—è¡¨"""
    repo = await get_task_repository()
    tasks, total = await repo.list_tasks(
        page=page,
        page_size=page_size,
        status=status,
        is_active=is_active,
        query=query
    )
    
    # è®¡ç®—æ€»é¡µæ•°
    total_pages = (total + page_size - 1) // page_size
    
    return SearchTaskListResponse(
        items=[task_to_response(t) for t in tasks],
        total=total,
        page=page,
        page_size=page_size,
        total_pages=total_pages
    )


@router.get(
    "/{task_id}/status",
    response_model=SearchTaskResponse,
    summary="è·å–ä»»åŠ¡çŠ¶æ€",
    description="æŸ¥è¯¢ä»»åŠ¡çš„è¿è¡ŒçŠ¶æ€ã€æ‰§è¡Œç»Ÿè®¡å’Œèµ„æºä½¿ç”¨æƒ…å†µã€‚è¿”å›å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯ï¼Œä¸“ä¸ºå‰ç«¯çŠ¶æ€ç›‘æ§è®¾è®¡ã€‚"
)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€ä¿¡æ¯ï¼ˆè¿”å›å®Œæ•´ä»»åŠ¡å“åº”ï¼‰"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

    return task_to_response(task)


@router.get(
    "/{task_id}",
    response_model=SearchTaskResponse,
    summary="è·å–æœç´¢ä»»åŠ¡è¯¦æƒ…",
    description="æ ¹æ®ä»»åŠ¡IDè·å–å•ä¸ªæœç´¢ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚"
)
async def get_search_task(task_id: str):
    """è·å–å•ä¸ªæœç´¢ä»»åŠ¡è¯¦æƒ…"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

    return task_to_response(task)


@router.put(
    "/{task_id}",
    response_model=SearchTaskResponse,
    summary="æ›´æ–°æœç´¢ä»»åŠ¡",
    description="æ›´æ–°æœç´¢ä»»åŠ¡çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¦‚åç§°ã€æè¿°ã€æŸ¥è¯¢å…³é”®è¯ã€é…ç½®å’Œè°ƒåº¦é—´éš”ç­‰ã€‚"
)
async def update_search_task(task_id: str, task_data: SearchTaskUpdate):
    """æ›´æ–°æœç´¢ä»»åŠ¡"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

    # æ›´æ–°å­—æ®µ
    if task_data.name is not None:
        task.name = task_data.name

    if task_data.description is not None:
        task.description = task_data.description

    if task_data.query is not None:
        task.query = task_data.query

    if task_data.crawl_url is not None:
        task.crawl_url = task_data.crawl_url

    # v2.0.0 æ–°å¢ï¼šä»»åŠ¡ç±»å‹
    if task_data.task_type is not None:
        task.task_type = task_data.task_type

    # æ ‡è®°æ˜¯å¦æ˜¾å¼æ›´æ–°äº† target_website
    target_website_explicitly_updated = False

    if task_data.target_website is not None:
        task.target_website = task_data.target_website
        target_website_explicitly_updated = True

    if task_data.search_config is not None:
        task.search_config = task_data.search_config
        # å¦‚æœæ›´æ–°äº† search_config ä½†æ²¡æœ‰æ˜¾å¼æ›´æ–° target_websiteï¼Œåˆ™è‡ªåŠ¨åŒæ­¥
        if not target_website_explicitly_updated:
            # å¼ºåˆ¶æ›´æ–° target_website ä¸ºæ–°çš„ç¬¬ä¸€ä¸ªåŸŸå
            task.target_website = task.extract_target_website()

    # v2.0.0 æ–°å¢ï¼šç½‘ç«™çˆ¬å–é…ç½®
    if task_data.crawl_config is not None:
        task.crawl_config = task_data.crawl_config

    if task_data.schedule_interval is not None:
        try:
            ScheduleInterval.from_value(task_data.schedule_interval)
            task.schedule_interval = task_data.schedule_interval
        except ValueError as e:
            raise HTTPException(400, f"æ— æ•ˆçš„è°ƒåº¦é—´éš”: {str(e)}")

    if task_data.is_active is not None:
        task.is_active = task_data.is_active
        task.status = TaskStatus.ACTIVE if task_data.is_active else TaskStatus.DISABLED

    task.updated_at = datetime.utcnow()

    # æ›´æ–°åˆ°ä»“å‚¨
    await repo.update(task)

    logger.info(f"æ›´æ–°æœç´¢ä»»åŠ¡: {task.name} (ID: {task_id}, ç›®æ ‡ç½‘ç«™: {task.target_website})")

    return task_to_response(task)


@router.patch(
    "/{task_id}/status",
    response_model=SearchTaskResponse,
    summary="ä¿®æ”¹ä»»åŠ¡çŠ¶æ€",
    description="å¯ç”¨æˆ–ç¦ç”¨æœç´¢ä»»åŠ¡ã€‚ç¦ç”¨çš„ä»»åŠ¡ä¸ä¼šè‡ªåŠ¨æ‰§è¡Œæœç´¢ã€‚"
)
async def update_task_status(task_id: str, status_data: SearchTaskStatusUpdate):
    """ä¿®æ”¹ä»»åŠ¡å¯ç”¨/ç¦ç”¨çŠ¶æ€"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

    task.is_active = status_data.is_active
    task.status = TaskStatus.ACTIVE if status_data.is_active else TaskStatus.DISABLED
    task.updated_at = datetime.utcnow()

    # æ›´æ–°åˆ°ä»“å‚¨
    await repo.update(task)

    # åŒæ­¥åˆ°è°ƒåº¦å™¨
    try:
        scheduler = await get_scheduler()
        if scheduler.is_running():
            await scheduler.update_task(task)
            logger.info(f"å·²åŒæ­¥ä»»åŠ¡åˆ°è°ƒåº¦å™¨: {task.name}")
    except Exception as e:
        logger.warning(f"åŒæ­¥ä»»åŠ¡åˆ°è°ƒåº¦å™¨å¤±è´¥: {e}")
        # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­è¿”å›

    logger.info(f"ä¿®æ”¹ä»»åŠ¡çŠ¶æ€: {task.name} -> {'å¯ç”¨' if task.is_active else 'ç¦ç”¨'}")

    return task_to_response(task)


@router.delete(
    "/{task_id}",
    status_code=200,
    summary="åˆ é™¤æœç´¢ä»»åŠ¡",
    description="æ°¸ä¹…åˆ é™¤æœç´¢ä»»åŠ¡åŠå…¶ç›¸å…³çš„æœç´¢ç»“æœã€‚æ­¤æ“ä½œä¸å¯æ’¤é”€ã€‚"
)
async def delete_search_task(task_id: str):
    """åˆ é™¤æœç´¢ä»»åŠ¡"""
    repo = await get_task_repository()
    task = await repo.get_by_id(task_id)
    if not task:
        raise HTTPException(404, f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
    
    success = await repo.delete(task_id)
    if not success:
        raise HTTPException(500, "åˆ é™¤ä»»åŠ¡å¤±è´¥")
    
    logger.info(f"åˆ é™¤æœç´¢ä»»åŠ¡: {task.name} (ID: {task_id})")
    
    return {
        "success": True,
        "message": "ä»»åŠ¡åˆ é™¤æˆåŠŸ", 
        "task_id": task_id,
        "task_name": task.name
    }