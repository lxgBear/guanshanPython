"""
Firecrawl æœç´¢æœåŠ¡é€‚é…å™¨
"""

import asyncio
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import httpx
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type

from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.core.domain.entities.search_config import SearchConfigManager, UserSearchConfig
from src.config import settings
from src.utils.logger import get_logger

logger = get_logger(__name__)


class FirecrawlSearchAdapter:
    """
Firecrawl æœç´¢APIé€‚é…å™¨
    """

    def __init__(self):
        self.api_key = settings.FIRECRAWL_API_KEY
        self.base_url = settings.FIRECRAWL_BASE_URL.rstrip('/')
        self.config_manager = SearchConfigManager()
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        # ä¼˜å…ˆä»settingsè¯»å–TEST_MODEï¼Œfallbackåˆ°ç¯å¢ƒå˜é‡
        self.is_test_mode = getattr(settings, 'TEST_MODE',
                                     os.getenv("TEST_MODE", "false").lower() == "true")

        if self.is_test_mode:
            logger.info("ğŸ§ª Firecrawlé€‚é…å™¨è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ - å°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
        else:
            logger.info(f"ğŸŒ Firecrawlé€‚é…å™¨è¿è¡Œåœ¨ç”Ÿäº§æ¨¡å¼ - API Base URL: {self.base_url}")

    def _log_retry_attempt(self, retry_state):
        """è®°å½•é‡è¯•å°è¯•"""
        attempt_number = retry_state.attempt_number
        if attempt_number > 1:
            exception = retry_state.outcome.exception()
            logger.warning(
                f"ğŸ”„ æœç´¢è¯·æ±‚å¤±è´¥ï¼Œç¬¬ {attempt_number - 1} æ¬¡é‡è¯• (å…±3æ¬¡) | "
                f"é”™è¯¯: {type(exception).__name__}: {str(exception)[:100]} | "
                f"å°†åœ¨ 8 åˆ†é’Ÿåé‡è¯•..."
            )

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(480),  # 8åˆ†é’Ÿ = 480ç§’
        retry=retry_if_exception_type((httpx.ConnectError, httpx.TimeoutException, httpx.HTTPStatusError)),
        before_sleep=lambda retry_state: FirecrawlSearchAdapter._log_retry_attempt(None, retry_state)
    )
    async def search(self, 
                    query: str, 
                    user_config: Optional[UserSearchConfig] = None,
                    task_id: Optional[str] = None) -> SearchResultBatch:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            user_config: ç”¨æˆ·æœç´¢é…ç½®
            task_id: ä»»åŠ¡ID
            
        Returns:
            SearchResultBatch: æœç´¢ç»“æœæ‰¹æ¬¡
        """
        start_time = datetime.utcnow()
        
        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        if self.is_test_mode:
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœ - æŸ¥è¯¢: '{query}' (ä»»åŠ¡ID: {task_id})")
            return self._generate_test_results(query, task_id)
        
        # è·å–æœ€ç»ˆé…ç½®
        if user_config is None:
            user_config = UserSearchConfig()
        
        config = self.config_manager.get_effective_config(user_config)

        # æå–è¯­è¨€é…ç½®ç”¨äºåç½®è¿‡æ»¤
        language = config.get('language', 'zh')
        strict_filter = config.get('strict_language_filter', True)

        # æ„å»ºè¯·æ±‚ä½“
        request_body = self._build_request_body(query, config)
        
        # åˆ›å»ºç»“æœæ‰¹æ¬¡
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config=config,
            is_test_mode=False
        )
        
        try:
            # é…ç½®httpxå®¢æˆ·ç«¯ - æ˜¾å¼ç¦ç”¨ä»£ç†ä½†ä¿ç•™DNSè§£æ
            # Firecrawl APIä¸éœ€è¦ä»£ç†ï¼Œç›´æ¥è¿æ¥
            # æ³¨æ„: trust_env=Falseä¼šå¯¼è‡´DNSè§£æé—®é¢˜,å› æ­¤åªæ˜¾å¼è®¾ç½®proxies={}æ¥ç¦ç”¨ä»£ç†
            client_config = {
                "proxies": {},  # ç©ºå­—å…¸ç¦ç”¨ä»£ç†,ä½†ä¸å½±å“DNS
                "timeout": config.get('timeout', 30)
            }

            logger.info(f"ğŸ” æ­£åœ¨è°ƒç”¨ Firecrawl API: {self.base_url}/v2/search")
            logger.info(f"ğŸ“ è¯·æ±‚å‚æ•°: {request_body}")

            # å‘é€è¯·æ±‚
            async with httpx.AsyncClient(**client_config) as client:
                response = await client.post(
                    f"{self.base_url}/v2/search",
                    headers=self.headers,
                    json=request_body,
                    timeout=config.get('timeout', 30)
                )

                logger.info(f"ğŸ“¡ API å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()

                # è§£æå“åº”
                data = response.json()
                logger.info(f"ğŸ“¦ å“åº”æ•°æ®ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

                # å¤„ç†ç»“æœ
                results = self._parse_search_results(data, task_id)
                logger.info(f"âœ… è§£æå¾—åˆ° {len(results)} æ¡æœç´¢ç»“æœ")

                # è¯­è¨€åç½®è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ä¸¥æ ¼è¯­è¨€è¿‡æ»¤ä¸”è®¾ç½®language=enï¼‰
                if language == 'en' and strict_filter:
                    results = self._post_filter_by_language(results, 'en')
                    logger.info(f"ğŸŒ è¯­è¨€åç½®è¿‡æ»¤: ä¿ç•™ {len(results)} æ¡è‹±æ–‡ç»“æœ")

                # æ·»åŠ åˆ°æ‰¹æ¬¡
                for result in results:
                    batch.add_result(result)

                batch.total_count = data.get('total', len(results))
                # v2ä½¿ç”¨creditsUsed, v0ä½¿ç”¨credits_used
                batch.credits_used = data.get('creditsUsed', data.get('credits_used', 1))

        except httpx.HTTPStatusError as e:
            error_detail = f"HTTP {e.response.status_code}"
            try:
                error_body = e.response.json()
                error_detail += f": {error_body}"
            except:
                error_detail += f": {e.response.text[:200]}"

            logger.error(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {error_detail}")
            batch.set_error(error_detail)

        except httpx.TimeoutException as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            batch.set_error(error_msg)

        except Exception as e:
            import traceback
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(f"âŒ æœç´¢å‘ç”Ÿæ„å¤–é”™è¯¯: {error_msg}")
            logger.error(f"å †æ ˆä¿¡æ¯:\n{traceback.format_exc()}")
            batch.set_error(error_msg)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.utcnow()
        batch.execution_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        return batch
    
    def _build_request_body(self, query: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚ä½“ - Firecrawl API v2æ ¼å¼ (å¢å¼ºç‰ˆ)"""
        # Firecrawl API v2: ä½¿ç”¨site:æ“ä½œç¬¦æ¥é™åˆ¶åŸŸå,è€Œä¸æ˜¯includeDomainså‚æ•°
        final_query = query

        # æ™ºèƒ½è¯­è¨€è¿‡æ»¤ï¼šå¦‚æœè®¾ç½®languageä¸ºenä¸”å¯ç”¨strict_language_filterï¼Œè‡ªåŠ¨æ’é™¤ä¸­æ–‡åŸŸå
        language = config.get('language', 'zh')
        strict_filter = config.get('strict_language_filter', True)  # é»˜è®¤å¯ç”¨ä¸¥æ ¼è¯­è¨€è¿‡æ»¤

        if language == 'en' and strict_filter:
            # æ£€æµ‹æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)

            if has_chinese:
                # å¦‚æœæŸ¥è¯¢åŒ…å«ä¸­æ–‡ä½†è¦æ±‚è‹±æ–‡ç»“æœï¼Œè‡ªåŠ¨æ·»åŠ åŸŸåè¿‡æ»¤
                # æ’é™¤å¸¸è§çš„ä¸­æ–‡åŸŸååç¼€
                chinese_domains_exclusion = (
                    '-site:*.cn '
                    '-site:*.com.cn '
                    '-site:*.hk '
                    '-site:*.tw '
                    '-inurl:zh '
                    '-inurl:zh-cn '
                    '-inurl:zh-hans '
                    '-inurl:zh-hant'
                )
                final_query = f"{query} {chinese_domains_exclusion}"
                logger.info(f"ğŸŒ è¯­è¨€è¿‡æ»¤: æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢+è‹±æ–‡è¦æ±‚ï¼Œå·²æ·»åŠ åŸŸåè¿‡æ»¤")

        # å¦‚æœé…ç½®äº†include_domains,æ·»åŠ site:æ“ä½œç¬¦åˆ°æŸ¥è¯¢ä¸­
        if config.get('include_domains'):
            domains = config['include_domains']
            if domains:
                # ä¸ºæ¯ä¸ªåŸŸåæ·»åŠ site:æ“ä½œç¬¦
                site_operators = ' OR '.join([f'site:{domain}' for domain in domains])
                final_query = f"({site_operators}) {final_query}"

        body = {
            "query": final_query,
            "limit": config.get('limit', 20),
            "lang": language
        }

        # æ·»åŠ  sources å‚æ•° (web, images, news)
        if config.get('sources'):
            body['sources'] = config['sources']
            logger.info(f"ğŸ” æœç´¢æ¥æº: {config['sources']}")

        # æ·»åŠ scrapeOptionsä»¥è·å–å®Œæ•´ç½‘é¡µå†…å®¹
        # Firecrawl API v2: é»˜è®¤searchåªè¿”å›å…ƒæ•°æ®(title, url, description)
        # éœ€è¦scrapeOptionsæ‰èƒ½è·å–å®Œæ•´çš„markdown/htmlå†…å®¹
        scrape_formats = config.get('scrape_formats', ['markdown', 'html', 'links'])
        if scrape_formats:
            body['scrapeOptions'] = {
                "formats": scrape_formats
            }

            # HTMLæ¸…ç†é€‰é¡¹ - å®Œæ•´æ”¯æŒ
            if config.get('only_main_content', True):
                body['scrapeOptions']['onlyMainContent'] = True

            # æ³¨æ„ï¼šremove_base64_imagesé»˜è®¤ä¸ºFalseï¼ˆä¿ç•™æ­£æ–‡å›¾ç‰‡ï¼‰
            # é…åˆonlyMainContentå’ŒblockAdsï¼Œå¯ä»¥ä¿ç•™æ­£æ–‡å›¾ç‰‡åŒæ—¶ç§»é™¤å¹¿å‘Š/éä¸»å†…å®¹åŒºåŸŸçš„å›¾ç‰‡
            if config.get('remove_base64_images', False):
                body['scrapeOptions']['removeBase64Images'] = True
                logger.info("ğŸ–¼ï¸  HTMLæ¸…ç†: å·²å¯ç”¨base64å›¾ç‰‡ç§»é™¤ï¼ˆæ€§èƒ½ä¼˜åŒ–æ¨¡å¼ï¼‰")
            else:
                logger.info("ğŸ“· å›¾ç‰‡ä¿ç•™: ä¿ç•™æ­£æ–‡å›¾ç‰‡ï¼ˆé…åˆä¸»å†…å®¹æå–å’Œå¹¿å‘Šå±è”½ï¼‰")

            if config.get('block_ads', True):
                body['scrapeOptions']['blockAds'] = True
                logger.info("ğŸš« HTMLæ¸…ç†: å·²å¯ç”¨å¹¿å‘Šå±è”½")

            # æ ‡ç­¾è¿‡æ»¤ - ç²¾ç»†åŒ–HTMLæ§åˆ¶
            if config.get('include_tags'):
                body['scrapeOptions']['includeTags'] = config['include_tags']
                logger.info(f"âœ… HTMLæ ‡ç­¾: ä»…ä¿ç•™ {config['include_tags']}")

            if config.get('exclude_tags'):
                body['scrapeOptions']['excludeTags'] = config['exclude_tags']
                logger.info(f"âŒ HTMLæ ‡ç­¾: æ’é™¤ {config['exclude_tags']}")

            # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            if config.get('wait_for'):
                body['scrapeOptions']['waitFor'] = config['wait_for']
                logger.info(f"â±ï¸  ç­‰å¾…åŠ è½½: {config['wait_for']}ms")

        # æ·»åŠ å¯é€‰å‚æ•°
        # æ³¨æ„: v2 APIä¸æ”¯æŒincludeDomainså’ŒexcludeDomainså‚æ•°
        # åŸŸåè¿‡æ»¤é€šè¿‡æŸ¥è¯¢ä¸­çš„site:æ“ä½œç¬¦å®ç°(è§ä¸Šæ–¹final_queryå¤„ç†)

        if config.get('time_range'):
            body['tbs'] = self._convert_time_range(config['time_range'])

        return body
    
    def _convert_time_range(self, time_range: str) -> str:
        """è½¬æ¢æ—¶é—´èŒƒå›´ä¸ºFirecrawlæ ¼å¼"""
        mapping = {
            "day": "qdr:d",
            "week": "qdr:w",
            "month": "qdr:m",
            "year": "qdr:y"
        }
        return mapping.get(time_range, "")
    
    def _parse_search_results(self, data: Dict[str, Any], task_id: Optional[str]) -> List[SearchResult]:
        """è§£ææœç´¢ç»“æœ - æ”¯æŒFirecrawl API v2æ ¼å¼"""
        results = []

        # Firecrawl API v2 å“åº”æ ¼å¼: data æ˜¯ä¸€ä¸ªå­—å…¸,åŒ…å« 'web' é”®
        # ä¾‹å¦‚: {"success": true, "data": {"web": [...]}, "creditsUsed": 1}
        data_content = data.get('data', {})

        # å¤„ç†v2æ ¼å¼: data.web æ˜¯ç»“æœåˆ—è¡¨
        if isinstance(data_content, dict) and 'web' in data_content:
            items = data_content.get('web', [])
        # å…¼å®¹v0æ ¼å¼: data ç›´æ¥æ˜¯ç»“æœåˆ—è¡¨
        elif isinstance(data_content, list):
            items = data_content
        else:
            logger.warning(f"æœªçŸ¥çš„å“åº”æ ¼å¼: dataç±»å‹ä¸º {type(data_content)}")
            items = []

        for item in items:
            # 1. æå–æ ¸å¿ƒå­—æ®µ
            title = item.get('title', '')
            url = item.get('url', '')
            description = item.get('description', item.get('snippet', ''))

            # 2. å†…å®¹å­—æ®µä¼˜åŒ–: æˆªæ–­markdown(æœ€å¤§5000å­—ç¬¦),å­˜å‚¨html
            markdown_full = item.get('markdown', '')
            if len(markdown_full) > 5000:
                markdown_content = markdown_full[:5000]
                logger.debug(f"ğŸ“ æˆªæ–­markdown: {len(markdown_full)}å­—ç¬¦ â†’ 5000å­—ç¬¦ (URL: {url[:50]}...)")
            else:
                markdown_content = markdown_full

            # æå–HTMLå†…å®¹
            html_content = item.get('html', '')

            # ä½¿ç”¨æˆªæ–­åçš„markdownä½œä¸ºcontent,æˆ–ä½¿ç”¨description
            content = markdown_content if markdown_content else description

            # 3. æå–metadataå­—æ®µ
            item_metadata = item.get('metadata', {})

            # 4. æ„å»ºç²¾ç®€çš„metadata(åªä¿ç•™æœ‰ç”¨å­—æ®µ,è¿‡æ»¤å†—ä½™å­—æ®µ)
            filtered_metadata = {
                'language': item_metadata.get('language'),
                'og_type': item_metadata.get('og:type'),
            }
            # ç§»é™¤Noneå€¼
            filtered_metadata = {k: v for k, v in filtered_metadata.items() if v is not None}

            # 5. æå–æ–‡ç« ç‰¹å®šå­—æ®µ
            article_tag_raw = item_metadata.get('article:tag')
            if isinstance(article_tag_raw, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç”¨é€—å·è¿æ¥æˆå­—ç¬¦ä¸²
                article_tag = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
            else:
                article_tag = article_tag_raw

            article_published_time = item_metadata.get('article:published_time')

            # 6. æå–æŠ€æœ¯å­—æ®µ
            source_url = item_metadata.get('sourceURL')  # åŸå§‹URL(é‡å®šå‘åœºæ™¯)
            http_status_code = item_metadata.get('statusCode')
            search_position = item.get('position')

            # 7. è§£æå‘å¸ƒæ—¥æœŸ
            published_date = self._parse_date(item.get('publishedDate'))

            # 8. åˆ›å»ºæœç´¢ç»“æœå®ä½“(å·²ç§»é™¤raw_data,ä¿ç•™html_content)
            result = SearchResult(
                task_id=task_id if task_id else "",
                title=title,
                url=url,
                content=content,
                snippet=description,
                source=item.get('source', 'web'),
                published_date=published_date,
                author=item.get('author'),
                language=item_metadata.get('language'),
                # ä¼˜åŒ–åçš„å­—æ®µ
                markdown_content=markdown_content,  # æˆªæ–­ç‰ˆæœ¬(æœ€å¤§5000å­—ç¬¦)
                html_content=html_content,  # HTMLæ ¼å¼å†…å®¹(ç”¨äºå¯Œæ–‡æœ¬æ˜¾ç¤ºå’Œåˆ†æ)
                article_tag=article_tag,
                article_published_time=article_published_time,
                source_url=source_url,
                http_status_code=http_status_code,
                search_position=search_position,
                metadata=filtered_metadata,  # ç²¾ç®€ç‰ˆå…ƒæ•°æ®(~200å­—èŠ‚ vs åŸæ¥çš„2-5KB)
                # ä¸å†å­˜å‚¨: raw_data (~850KB)
                relevance_score=item.get('score', 0.0),
                status=ResultStatus.PENDING
            )

            logger.debug(f"âœ… è§£æç»“æœ: {title[:50]}... (content: {len(content)}å­—ç¬¦, metadata: {len(str(filtered_metadata))}å­—èŠ‚)")
            results.append(result)

        return results
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None

        try:
            return datetime.fromisoformat(date_str)
        except:
            return None

    def _post_filter_by_language(self, results: List[SearchResult], target_language: str) -> List[SearchResult]:
        """åç½®è¯­è¨€è¿‡æ»¤ - æ ¹æ®URLã€è¯­è¨€å…ƒæ•°æ®å’Œæ ‡é¢˜å­—ç¬¦è¿›è¡Œè¿‡æ»¤

        Args:
            results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
            target_language: ç›®æ ‡è¯­è¨€ï¼ˆå¦‚ 'en'ï¼‰

        Returns:
            è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        filtered_results = []

        for result in results:
            # 1. æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡åŸŸåæˆ–è·¯å¾„æ ‡è¯†
            url_lower = result.url.lower()
            chinese_url_indicators = [
                '.cn', '.com.cn', '.hk', '.tw',
                '/zh/', '/zh-cn/', '/zh-hans/', '/zh-hant/',
                'zhongwen', 'hans', 'hant'
            ]

            has_chinese_url = any(indicator in url_lower for indicator in chinese_url_indicators)

            # 2. æ£€æŸ¥è¯­è¨€å…ƒæ•°æ®
            language_metadata = result.language
            is_chinese_language = False
            if language_metadata:
                lang_lower = str(language_metadata).lower()
                is_chinese_language = any(zh in lang_lower for zh in ['zh', 'hans', 'hant', 'chinese'])

            # 3. æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            has_chinese_chars = any('\u4e00' <= char <= '\u9fff' for char in result.title)

            # 4. è¿‡æ»¤é€»è¾‘ï¼šå¦‚æœæ˜¯ä¸­æ–‡å†…å®¹åˆ™è·³è¿‡
            if has_chinese_url or is_chinese_language or has_chinese_chars:
                logger.debug(f"ğŸš« è¿‡æ»¤ä¸­æ–‡ç»“æœ: {result.title[:50]}... (URL: {result.url[:50]}...)")
                continue

            # 5. ä¿ç•™è‹±æ–‡ç»“æœ
            filtered_results.append(result)

        return filtered_results
    
    def _generate_test_results(self, query: str, task_id: Optional[str]) -> SearchResultBatch:
        """ç”Ÿæˆæµ‹è¯•æ¨¡å¼çš„æ¨¡æ‹Ÿç»“æœ"""
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config={'limit': 10},
            is_test_mode=True,
            test_limit_applied=True
        )
        
        # ç”Ÿæˆ10æ¡æ¨¡æ‹Ÿç»“æœ
        for i in range(10):
            result = SearchResult(
                task_id=task_id if task_id else "",
                title=f"æµ‹è¯•ç»“æœ {i+1}: {query}",
                url=f"https://example.com/test/{i+1}",
                content=f"è¿™æ˜¯å…³äº'{query}'çš„æµ‹è¯•å†…å®¹ {i+1}ã€‚" * 10,
                snippet=f"æµ‹è¯•æ‘˜è¦: {query} - ç»“æœ {i+1}",
                source="test",
                published_date=datetime.utcnow(),
                relevance_score=0.9 - (i * 0.05),
                is_test_data=True,
                status=ResultStatus.PROCESSED
            )
            batch.add_result(result)
        
        batch.total_count = 10
        batch.credits_used = 0  # æµ‹è¯•æ¨¡å¼ä¸æ¶ˆè€—ç§¯åˆ†
        batch.execution_time_ms = 100  # æ¨¡æ‹Ÿ100mså“åº”æ—¶é—´
        
        return batch
    
    async def batch_search(self, 
                          queries: List[Dict[str, Any]]) -> List[SearchResultBatch]:
        """
        æ‰¹é‡æœç´¢
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« query å’Œ config
            
        Returns:
            List[SearchResultBatch]: æœç´¢ç»“æœåˆ—è¡¨
        """
        tasks = [
            self.search(
                query=q['query'],
                user_config=UserSearchConfig.from_json(q.get('config', {})),
                task_id=q.get('task_id')
            )
            for q in queries
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        batches = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æœç´¢ç¬¬{i}ä¸ªæŸ¥è¯¢å¤±è´¥: {result}")
                # åˆ›å»ºå¤±è´¥çš„æ‰¹æ¬¡
                batch = SearchResultBatch(
                    task_id=queries[i].get('task_id', ''),
                    query=queries[i]['query'],
                    search_config=queries[i].get('config', {})
                )
                batch.set_error(str(result))
                batches.append(batch)
            else:
                batches.append(result)
        
        return batches