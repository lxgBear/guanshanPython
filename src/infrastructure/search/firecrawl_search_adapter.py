"""
Firecrawl æœç´¢æœåŠ¡é€‚é…å™¨
"""

import asyncio
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.core.domain.entities.search_config import SearchConfigManager, UserSearchConfig
from src.config import settings
from src.utils.logger import get_logger

logger = get_logger(__name__)


class FirecrawlSearchAdapter:
    """
Firecrawl æœç´¢APIé€‚é…å™¨
    """
    
    def __init__(self):
        self.api_key = settings.FIRECRAWL_API_KEY
        self.base_url = settings.FIRECRAWL_BASE_URL.rstrip('/')
        self.config_manager = SearchConfigManager()
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        # ä¼˜å…ˆä»settingsè¯»å–TEST_MODEï¼Œfallbackåˆ°ç¯å¢ƒå˜é‡
        self.is_test_mode = getattr(settings, 'TEST_MODE',
                                     os.getenv("TEST_MODE", "false").lower() == "true")

        if self.is_test_mode:
            logger.info("ğŸ§ª Firecrawlé€‚é…å™¨è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ - å°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
        else:
            logger.info(f"ğŸŒ Firecrawlé€‚é…å™¨è¿è¡Œåœ¨ç”Ÿäº§æ¨¡å¼ - API Base URL: {self.base_url}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def search(self, 
                    query: str, 
                    user_config: Optional[UserSearchConfig] = None,
                    task_id: Optional[str] = None) -> SearchResultBatch:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            user_config: ç”¨æˆ·æœç´¢é…ç½®
            task_id: ä»»åŠ¡ID
            
        Returns:
            SearchResultBatch: æœç´¢ç»“æœæ‰¹æ¬¡
        """
        start_time = datetime.utcnow()
        
        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        if self.is_test_mode:
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœ - æŸ¥è¯¢: '{query}' (ä»»åŠ¡ID: {task_id})")
            return self._generate_test_results(query, task_id)
        
        # è·å–æœ€ç»ˆé…ç½®
        if user_config is None:
            user_config = UserSearchConfig()
        
        config = self.config_manager.get_effective_config(user_config)
        
        # æ„å»ºè¯·æ±‚ä½“
        request_body = self._build_request_body(query, config)
        
        # åˆ›å»ºç»“æœæ‰¹æ¬¡
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config=config,
            is_test_mode=False
        )
        
        try:
            # é…ç½®httpxå®¢æˆ·ç«¯ - æ˜¾å¼ç¦ç”¨ä»£ç†ä½†ä¿ç•™DNSè§£æ
            # Firecrawl APIä¸éœ€è¦ä»£ç†ï¼Œç›´æ¥è¿æ¥
            # æ³¨æ„: trust_env=Falseä¼šå¯¼è‡´DNSè§£æé—®é¢˜,å› æ­¤åªæ˜¾å¼è®¾ç½®proxies={}æ¥ç¦ç”¨ä»£ç†
            client_config = {
                "proxies": {},  # ç©ºå­—å…¸ç¦ç”¨ä»£ç†,ä½†ä¸å½±å“DNS
                "timeout": config.get('timeout', 30)
            }

            logger.info(f"ğŸ” æ­£åœ¨è°ƒç”¨ Firecrawl API: {self.base_url}/v2/search")
            logger.info(f"ğŸ“ è¯·æ±‚å‚æ•°: {request_body}")

            # å‘é€è¯·æ±‚
            async with httpx.AsyncClient(**client_config) as client:
                response = await client.post(
                    f"{self.base_url}/v2/search",
                    headers=self.headers,
                    json=request_body,
                    timeout=config.get('timeout', 30)
                )

                logger.info(f"ğŸ“¡ API å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()

                # è§£æå“åº”
                data = response.json()
                logger.info(f"ğŸ“¦ å“åº”æ•°æ®ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

                # å¤„ç†ç»“æœ
                results = self._parse_search_results(data, task_id)
                logger.info(f"âœ… è§£æå¾—åˆ° {len(results)} æ¡æœç´¢ç»“æœ")

                # æ·»åŠ åˆ°æ‰¹æ¬¡
                for result in results:
                    batch.add_result(result)

                batch.total_count = data.get('total', len(results))
                # v2ä½¿ç”¨creditsUsed, v0ä½¿ç”¨credits_used
                batch.credits_used = data.get('creditsUsed', data.get('credits_used', 1))

        except httpx.HTTPStatusError as e:
            error_detail = f"HTTP {e.response.status_code}"
            try:
                error_body = e.response.json()
                error_detail += f": {error_body}"
            except:
                error_detail += f": {e.response.text[:200]}"

            logger.error(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {error_detail}")
            batch.set_error(error_detail)

        except httpx.TimeoutException as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            batch.set_error(error_msg)

        except Exception as e:
            import traceback
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(f"âŒ æœç´¢å‘ç”Ÿæ„å¤–é”™è¯¯: {error_msg}")
            logger.error(f"å †æ ˆä¿¡æ¯:\n{traceback.format_exc()}")
            batch.set_error(error_msg)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.utcnow()
        batch.execution_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        return batch
    
    def _build_request_body(self, query: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚ä½“ - Firecrawl API v2æ ¼å¼"""
        # Firecrawl API v2: ä½¿ç”¨site:æ“ä½œç¬¦æ¥é™åˆ¶åŸŸå,è€Œä¸æ˜¯includeDomainså‚æ•°
        final_query = query

        # å¦‚æœé…ç½®äº†include_domains,æ·»åŠ site:æ“ä½œç¬¦åˆ°æŸ¥è¯¢ä¸­
        if config.get('include_domains'):
            domains = config['include_domains']
            if domains:
                # ä¸ºæ¯ä¸ªåŸŸåæ·»åŠ site:æ“ä½œç¬¦
                site_operators = ' OR '.join([f'site:{domain}' for domain in domains])
                final_query = f"({site_operators}) {query}"

        body = {
            "query": final_query,
            "limit": config.get('limit', 20),
            "lang": config.get('language', 'zh')
        }

        # æ·»åŠ scrapeOptionsä»¥è·å–å®Œæ•´ç½‘é¡µå†…å®¹
        # Firecrawl API v2: é»˜è®¤searchåªè¿”å›å…ƒæ•°æ®(title, url, description)
        # éœ€è¦scrapeOptionsæ‰èƒ½è·å–å®Œæ•´çš„markdown/htmlå†…å®¹
        scrape_formats = config.get('scrape_formats', ['markdown', 'html', 'links'])
        if scrape_formats:
            body['scrapeOptions'] = {
                "formats": scrape_formats
            }
            # å¯ä»¥æ·»åŠ æ›´å¤šscrapeé€‰é¡¹
            if config.get('only_main_content', True):
                body['scrapeOptions']['onlyMainContent'] = True

        # æ·»åŠ å¯é€‰å‚æ•°
        # æ³¨æ„: v2 APIä¸æ”¯æŒincludeDomainså’ŒexcludeDomainså‚æ•°
        # åŸŸåè¿‡æ»¤é€šè¿‡æŸ¥è¯¢ä¸­çš„site:æ“ä½œç¬¦å®ç°(è§ä¸Šæ–¹final_queryå¤„ç†)

        if config.get('time_range'):
            body['tbs'] = self._convert_time_range(config['time_range'])

        return body
    
    def _convert_time_range(self, time_range: str) -> str:
        """è½¬æ¢æ—¶é—´èŒƒå›´ä¸ºFirecrawlæ ¼å¼"""
        mapping = {
            "day": "qdr:d",
            "week": "qdr:w",
            "month": "qdr:m",
            "year": "qdr:y"
        }
        return mapping.get(time_range, "")
    
    def _parse_search_results(self, data: Dict[str, Any], task_id: Optional[str]) -> List[SearchResult]:
        """è§£ææœç´¢ç»“æœ - æ”¯æŒFirecrawl API v2æ ¼å¼"""
        results = []

        # Firecrawl API v2 å“åº”æ ¼å¼: data æ˜¯ä¸€ä¸ªå­—å…¸,åŒ…å« 'web' é”®
        # ä¾‹å¦‚: {"success": true, "data": {"web": [...]}, "creditsUsed": 1}
        data_content = data.get('data', {})

        # å¤„ç†v2æ ¼å¼: data.web æ˜¯ç»“æœåˆ—è¡¨
        if isinstance(data_content, dict) and 'web' in data_content:
            items = data_content.get('web', [])
        # å…¼å®¹v0æ ¼å¼: data ç›´æ¥æ˜¯ç»“æœåˆ—è¡¨
        elif isinstance(data_content, list):
            items = data_content
        else:
            logger.warning(f"æœªçŸ¥çš„å“åº”æ ¼å¼: dataç±»å‹ä¸º {type(data_content)}")
            items = []

        for item in items:
            # 1. æå–æ ¸å¿ƒå­—æ®µ
            title = item.get('title', '')
            url = item.get('url', '')
            description = item.get('description', item.get('snippet', ''))

            # 2. å†…å®¹å­—æ®µä¼˜åŒ–: æˆªæ–­markdown(æœ€å¤§5000å­—ç¬¦),å­˜å‚¨html
            markdown_full = item.get('markdown', '')
            if len(markdown_full) > 5000:
                markdown_content = markdown_full[:5000]
                logger.debug(f"ğŸ“ æˆªæ–­markdown: {len(markdown_full)}å­—ç¬¦ â†’ 5000å­—ç¬¦ (URL: {url[:50]}...)")
            else:
                markdown_content = markdown_full

            # æå–HTMLå†…å®¹
            html_content = item.get('html', '')

            # ä½¿ç”¨æˆªæ–­åçš„markdownä½œä¸ºcontent,æˆ–ä½¿ç”¨description
            content = markdown_content if markdown_content else description

            # 3. æå–metadataå­—æ®µ
            item_metadata = item.get('metadata', {})

            # 4. æ„å»ºç²¾ç®€çš„metadata(åªä¿ç•™æœ‰ç”¨å­—æ®µ,è¿‡æ»¤å†—ä½™å­—æ®µ)
            filtered_metadata = {
                'language': item_metadata.get('language'),
                'og_type': item_metadata.get('og:type'),
            }
            # ç§»é™¤Noneå€¼
            filtered_metadata = {k: v for k, v in filtered_metadata.items() if v is not None}

            # 5. æå–æ–‡ç« ç‰¹å®šå­—æ®µ
            article_tag_raw = item_metadata.get('article:tag')
            if isinstance(article_tag_raw, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç”¨é€—å·è¿æ¥æˆå­—ç¬¦ä¸²
                article_tag = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
            else:
                article_tag = article_tag_raw

            article_published_time = item_metadata.get('article:published_time')

            # 6. æå–æŠ€æœ¯å­—æ®µ
            source_url = item_metadata.get('sourceURL')  # åŸå§‹URL(é‡å®šå‘åœºæ™¯)
            http_status_code = item_metadata.get('statusCode')
            search_position = item.get('position')

            # 7. è§£æå‘å¸ƒæ—¥æœŸ
            published_date = self._parse_date(item.get('publishedDate'))

            # 8. åˆ›å»ºæœç´¢ç»“æœå®ä½“(å·²ç§»é™¤raw_data,ä¿ç•™html_content)
            result = SearchResult(
                task_id=task_id if task_id else "",
                title=title,
                url=url,
                content=content,
                snippet=description,
                source=item.get('source', 'web'),
                published_date=published_date,
                author=item.get('author'),
                language=item_metadata.get('language'),
                # ä¼˜åŒ–åçš„å­—æ®µ
                markdown_content=markdown_content,  # æˆªæ–­ç‰ˆæœ¬(æœ€å¤§5000å­—ç¬¦)
                html_content=html_content,  # HTMLæ ¼å¼å†…å®¹(ç”¨äºå¯Œæ–‡æœ¬æ˜¾ç¤ºå’Œåˆ†æ)
                article_tag=article_tag,
                article_published_time=article_published_time,
                source_url=source_url,
                http_status_code=http_status_code,
                search_position=search_position,
                metadata=filtered_metadata,  # ç²¾ç®€ç‰ˆå…ƒæ•°æ®(~200å­—èŠ‚ vs åŸæ¥çš„2-5KB)
                # ä¸å†å­˜å‚¨: raw_data (~850KB)
                relevance_score=item.get('score', 0.0),
                status=ResultStatus.PENDING
            )

            logger.debug(f"âœ… è§£æç»“æœ: {title[:50]}... (content: {len(content)}å­—ç¬¦, metadata: {len(str(filtered_metadata))}å­—èŠ‚)")
            results.append(result)

        return results
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        try:
            return datetime.fromisoformat(date_str)
        except:
            return None
    
    def _generate_test_results(self, query: str, task_id: Optional[str]) -> SearchResultBatch:
        """ç”Ÿæˆæµ‹è¯•æ¨¡å¼çš„æ¨¡æ‹Ÿç»“æœ"""
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config={'limit': 10},
            is_test_mode=True,
            test_limit_applied=True
        )
        
        # ç”Ÿæˆ10æ¡æ¨¡æ‹Ÿç»“æœ
        for i in range(10):
            result = SearchResult(
                task_id=task_id if task_id else "",
                title=f"æµ‹è¯•ç»“æœ {i+1}: {query}",
                url=f"https://example.com/test/{i+1}",
                content=f"è¿™æ˜¯å…³äº'{query}'çš„æµ‹è¯•å†…å®¹ {i+1}ã€‚" * 10,
                snippet=f"æµ‹è¯•æ‘˜è¦: {query} - ç»“æœ {i+1}",
                source="test",
                published_date=datetime.utcnow(),
                relevance_score=0.9 - (i * 0.05),
                is_test_data=True,
                status=ResultStatus.PROCESSED
            )
            batch.add_result(result)
        
        batch.total_count = 10
        batch.credits_used = 0  # æµ‹è¯•æ¨¡å¼ä¸æ¶ˆè€—ç§¯åˆ†
        batch.execution_time_ms = 100  # æ¨¡æ‹Ÿ100mså“åº”æ—¶é—´
        
        return batch
    
    async def batch_search(self, 
                          queries: List[Dict[str, Any]]) -> List[SearchResultBatch]:
        """
        æ‰¹é‡æœç´¢
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« query å’Œ config
            
        Returns:
            List[SearchResultBatch]: æœç´¢ç»“æœåˆ—è¡¨
        """
        tasks = [
            self.search(
                query=q['query'],
                user_config=UserSearchConfig.from_json(q.get('config', {})),
                task_id=q.get('task_id')
            )
            for q in queries
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        batches = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æœç´¢ç¬¬{i}ä¸ªæŸ¥è¯¢å¤±è´¥: {result}")
                # åˆ›å»ºå¤±è´¥çš„æ‰¹æ¬¡
                batch = SearchResultBatch(
                    task_id=queries[i].get('task_id', ''),
                    query=queries[i]['query'],
                    search_config=queries[i].get('config', {})
                )
                batch.set_error(str(result))
                batches.append(batch)
            else:
                batches.append(result)
        
        return batches