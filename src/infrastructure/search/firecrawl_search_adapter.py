"""
Firecrawl æœç´¢æœåŠ¡é€‚é…å™¨
"""

import asyncio
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.core.domain.entities.search_config import SearchConfigManager, UserSearchConfig
from src.config import settings
from src.utils.logger import get_logger

logger = get_logger(__name__)


class FirecrawlSearchAdapter:
    """
Firecrawl æœç´¢APIé€‚é…å™¨
    """
    
    def __init__(self):
        self.api_key = settings.FIRECRAWL_API_KEY
        self.base_url = settings.FIRECRAWL_BASE_URL.rstrip('/')
        self.config_manager = SearchConfigManager()
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        # ä¼˜å…ˆä»settingsè¯»å–TEST_MODEï¼Œfallbackåˆ°ç¯å¢ƒå˜é‡
        self.is_test_mode = getattr(settings, 'TEST_MODE',
                                     os.getenv("TEST_MODE", "false").lower() == "true")

        if self.is_test_mode:
            logger.info("ğŸ§ª Firecrawlé€‚é…å™¨è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ - å°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
        else:
            logger.info(f"ğŸŒ Firecrawlé€‚é…å™¨è¿è¡Œåœ¨ç”Ÿäº§æ¨¡å¼ - API Base URL: {self.base_url}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def search(self, 
                    query: str, 
                    user_config: Optional[UserSearchConfig] = None,
                    task_id: Optional[str] = None) -> SearchResultBatch:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            user_config: ç”¨æˆ·æœç´¢é…ç½®
            task_id: ä»»åŠ¡ID
            
        Returns:
            SearchResultBatch: æœç´¢ç»“æœæ‰¹æ¬¡
        """
        start_time = datetime.utcnow()
        
        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        if self.is_test_mode:
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœ - æŸ¥è¯¢: '{query}' (ä»»åŠ¡ID: {task_id})")
            return self._generate_test_results(query, task_id)
        
        # è·å–æœ€ç»ˆé…ç½®
        if user_config is None:
            user_config = UserSearchConfig()
        
        config = self.config_manager.get_effective_config(user_config)
        
        # æ„å»ºè¯·æ±‚ä½“
        request_body = self._build_request_body(query, config)
        
        # åˆ›å»ºç»“æœæ‰¹æ¬¡
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config=config,
            is_test_mode=False
        )
        
        try:
            # é…ç½®httpxå®¢æˆ·ç«¯ - ä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†ä»¥é¿å…SOCKSé—®é¢˜
            # Firecrawl APIä¸éœ€è¦ä»£ç†ï¼Œç›´æ¥è¿æ¥
            client_config = {
                "proxies": None,  # ç¦ç”¨ä»£ç†
                "trust_env": False  # ä¸ä¿¡ä»»ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®
            }

            logger.info(f"ğŸ” æ­£åœ¨è°ƒç”¨ Firecrawl API: {self.base_url}/v2/search")
            logger.info(f"ğŸ“ è¯·æ±‚å‚æ•°: {request_body}")

            # å‘é€è¯·æ±‚
            async with httpx.AsyncClient(**client_config) as client:
                response = await client.post(
                    f"{self.base_url}/v2/search",
                    headers=self.headers,
                    json=request_body,
                    timeout=config.get('timeout', 30)
                )

                logger.info(f"ğŸ“¡ API å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()

                # è§£æå“åº”
                data = response.json()
                logger.info(f"ğŸ“¦ å“åº”æ•°æ®ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

                # å¤„ç†ç»“æœ
                results = self._parse_search_results(data, task_id)
                logger.info(f"âœ… è§£æå¾—åˆ° {len(results)} æ¡æœç´¢ç»“æœ")

                # æ·»åŠ åˆ°æ‰¹æ¬¡
                for result in results:
                    batch.add_result(result)

                batch.total_count = data.get('total', len(results))
                # v2ä½¿ç”¨creditsUsed, v0ä½¿ç”¨credits_used
                batch.credits_used = data.get('creditsUsed', data.get('credits_used', 1))

        except httpx.HTTPStatusError as e:
            error_detail = f"HTTP {e.response.status_code}"
            try:
                error_body = e.response.json()
                error_detail += f": {error_body}"
            except:
                error_detail += f": {e.response.text[:200]}"

            logger.error(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {error_detail}")
            batch.set_error(error_detail)

        except httpx.TimeoutException as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            batch.set_error(error_msg)

        except Exception as e:
            import traceback
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(f"âŒ æœç´¢å‘ç”Ÿæ„å¤–é”™è¯¯: {error_msg}")
            logger.error(f"å †æ ˆä¿¡æ¯:\n{traceback.format_exc()}")
            batch.set_error(error_msg)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.utcnow()
        batch.execution_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        return batch
    
    def _build_request_body(self, query: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚ä½“"""
        body = {
            "query": query,
            "limit": config.get('limit', 20),
            "lang": config.get('language', 'zh')
        }

        # æ·»åŠ scrapeOptionsä»¥è·å–å®Œæ•´ç½‘é¡µå†…å®¹
        # Firecrawl API v2: é»˜è®¤searchåªè¿”å›å…ƒæ•°æ®(title, url, description)
        # éœ€è¦scrapeOptionsæ‰èƒ½è·å–å®Œæ•´çš„markdown/htmlå†…å®¹
        scrape_formats = config.get('scrape_formats', ['markdown', 'html', 'links'])
        if scrape_formats:
            body['scrapeOptions'] = {
                "formats": scrape_formats
            }
            # å¯ä»¥æ·»åŠ æ›´å¤šscrapeé€‰é¡¹
            if config.get('only_main_content', True):
                body['scrapeOptions']['onlyMainContent'] = True

        # æ·»åŠ å¯é€‰å‚æ•°
        if config.get('include_domains'):
            body['includeDomains'] = config['include_domains']

        if config.get('exclude_domains'):
            body['excludeDomains'] = config['exclude_domains']

        if config.get('time_range'):
            body['tbs'] = self._convert_time_range(config['time_range'])

        return body
    
    def _convert_time_range(self, time_range: str) -> str:
        """è½¬æ¢æ—¶é—´èŒƒå›´ä¸ºFirecrawlæ ¼å¼"""
        mapping = {
            "day": "qdr:d",
            "week": "qdr:w",
            "month": "qdr:m",
            "year": "qdr:y"
        }
        return mapping.get(time_range, "")
    
    def _parse_search_results(self, data: Dict[str, Any], task_id: Optional[str]) -> List[SearchResult]:
        """è§£ææœç´¢ç»“æœ - æ”¯æŒFirecrawl API v2æ ¼å¼"""
        results = []

        # Firecrawl API v2 å“åº”æ ¼å¼: data æ˜¯ä¸€ä¸ªå­—å…¸,åŒ…å« 'web' é”®
        # ä¾‹å¦‚: {"success": true, "data": {"web": [...]}, "creditsUsed": 1}
        data_content = data.get('data', {})

        # å¤„ç†v2æ ¼å¼: data.web æ˜¯ç»“æœåˆ—è¡¨
        if isinstance(data_content, dict) and 'web' in data_content:
            items = data_content.get('web', [])
        # å…¼å®¹v0æ ¼å¼: data ç›´æ¥æ˜¯ç»“æœåˆ—è¡¨
        elif isinstance(data_content, list):
            items = data_content
        else:
            logger.warning(f"æœªçŸ¥çš„å“åº”æ ¼å¼: dataç±»å‹ä¸º {type(data_content)}")
            items = []

        for item in items:
            # v2 API with scrapeOptions: markdownå’Œhtmlå­—æ®µåŒ…å«å®Œæ•´å†…å®¹
            markdown = item.get('markdown', '')
            html = item.get('html', '')

            # contentå­—æ®µä½¿ç”¨markdownå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨htmlï¼Œæœ€åfallbackåˆ°ç©º
            content = markdown if markdown else (html if html else item.get('content', ''))

            # ä»åŸå§‹æ•°æ®çš„metadataä¸­æå–articleå­—æ®µ
            item_metadata = item.get('metadata', {})

            # å¤„ç†article_tagï¼šå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
            article_tag_raw = item_metadata.get('article:tag')
            if isinstance(article_tag_raw, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç”¨é€—å·è¿æ¥æˆå­—ç¬¦ä¸²
                article_tag = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
            else:
                article_tag = article_tag_raw

            article_published_time = item_metadata.get('article:published_time')

            # æ„å»ºmetadataï¼ŒåŒ…å«links
            metadata = item_metadata.copy() if item_metadata else {}
            if 'links' in item:
                metadata['extracted_links'] = item.get('links', [])

            result = SearchResult(
                task_id=task_id if task_id else "",
                title=item.get('title', ''),
                url=item.get('url', ''),
                content=content,  # ä¼˜å…ˆä½¿ç”¨markdown/htmlå†…å®¹
                snippet=item.get('description', item.get('snippet', '')),  # v2ä½¿ç”¨description
                source=item.get('source', 'web'),
                published_date=self._parse_date(item.get('publishedDate')),
                author=item.get('author'),
                language=item.get('language'),
                raw_data=item,
                markdown_content=markdown,  # ä¿å­˜markdownæ ¼å¼
                html_content=html,  # ä¿å­˜htmlæ ¼å¼ä¸ºé¡¶å±‚å­—æ®µ
                article_tag=article_tag,  # æ–‡ç« æ ‡ç­¾
                article_published_time=article_published_time,  # æ–‡ç« å‘å¸ƒæ—¶é—´
                metadata=metadata,  # åŒ…å«linksçš„metadata
                relevance_score=item.get('score', 0.0),
                status=ResultStatus.PENDING
            )
            results.append(result)

        return results
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        try:
            return datetime.fromisoformat(date_str)
        except:
            return None
    
    def _generate_test_results(self, query: str, task_id: Optional[str]) -> SearchResultBatch:
        """ç”Ÿæˆæµ‹è¯•æ¨¡å¼çš„æ¨¡æ‹Ÿç»“æœ"""
        batch = SearchResultBatch(
            task_id=task_id if task_id else "",
            query=query,
            search_config={'limit': 10},
            is_test_mode=True,
            test_limit_applied=True
        )
        
        # ç”Ÿæˆ10æ¡æ¨¡æ‹Ÿç»“æœ
        for i in range(10):
            result = SearchResult(
                task_id=task_id if task_id else "",
                title=f"æµ‹è¯•ç»“æœ {i+1}: {query}",
                url=f"https://example.com/test/{i+1}",
                content=f"è¿™æ˜¯å…³äº'{query}'çš„æµ‹è¯•å†…å®¹ {i+1}ã€‚" * 10,
                snippet=f"æµ‹è¯•æ‘˜è¦: {query} - ç»“æœ {i+1}",
                source="test",
                published_date=datetime.utcnow(),
                relevance_score=0.9 - (i * 0.05),
                is_test_data=True,
                status=ResultStatus.PROCESSED
            )
            batch.add_result(result)
        
        batch.total_count = 10
        batch.credits_used = 0  # æµ‹è¯•æ¨¡å¼ä¸æ¶ˆè€—ç§¯åˆ†
        batch.execution_time_ms = 100  # æ¨¡æ‹Ÿ100mså“åº”æ—¶é—´
        
        return batch
    
    async def batch_search(self, 
                          queries: List[Dict[str, Any]]) -> List[SearchResultBatch]:
        """
        æ‰¹é‡æœç´¢
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« query å’Œ config
            
        Returns:
            List[SearchResultBatch]: æœç´¢ç»“æœåˆ—è¡¨
        """
        tasks = [
            self.search(
                query=q['query'],
                user_config=UserSearchConfig.from_json(q.get('config', {})),
                task_id=q.get('task_id')
            )
            for q in queries
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        batches = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æœç´¢ç¬¬{i}ä¸ªæŸ¥è¯¢å¤±è´¥: {result}")
                # åˆ›å»ºå¤±è´¥çš„æ‰¹æ¬¡
                batch = SearchResultBatch(
                    task_id=queries[i].get('task_id', ''),
                    query=queries[i]['query'],
                    search_config=queries[i].get('config', {})
                )
                batch.set_error(str(result))
                batches.append(batch)
            else:
                batches.append(result)
        
        return batches