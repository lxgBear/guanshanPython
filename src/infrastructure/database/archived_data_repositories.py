"""æ•°æ®æºå­˜æ¡£æ•°æ®ä»“å‚¨å±‚

æä¾›å­˜æ¡£æ•°æ®çš„æŒä¹…åŒ–æ“ä½œï¼ŒåŒ…æ‹¬ï¼š
- åˆ›å»ºå­˜æ¡£è®°å½•ï¼ˆconfirmæ—¶è‡ªåŠ¨è§¦å‘ï¼‰
- æŸ¥è¯¢å­˜æ¡£æ•°æ®ï¼ˆåˆ†é¡µæ”¯æŒï¼‰
- ç»Ÿè®¡å­˜æ¡£æ•°é‡
- åˆ é™¤å­˜æ¡£æ•°æ®ï¼ˆçº§è”åˆ é™¤ï¼‰
- MongoDBäº‹åŠ¡æ”¯æŒ
"""

from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from motor.motor_asyncio import AsyncIOMotorDatabase, AsyncIOMotorClientSession

from src.core.domain.entities.archived_data import ArchivedData
from src.utils.logger import get_logger

logger = get_logger(__name__)


class ArchivedDataRepository:
    """å­˜æ¡£æ•°æ®ä»“å‚¨"""

    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.collection = db.data_source_archived_data

    async def create(
        self,
        archived_data: ArchivedData,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> ArchivedData:
        """åˆ›å»ºå­˜æ¡£è®°å½•

        é€šå¸¸åœ¨æ•°æ®æºconfirmæ“ä½œæ—¶ï¼Œç”±Serviceå±‚äº‹åŠ¡è°ƒç”¨

        Args:
            archived_data: å­˜æ¡£æ•°æ®å®ä½“
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆå¿…é¡»æä¾›ä»¥ä¿è¯äº‹åŠ¡ä¸€è‡´æ€§ï¼‰

        Returns:
            åˆ›å»ºçš„å­˜æ¡£æ•°æ®å®ä½“
        """
        doc = self._to_document(archived_data)
        await self.collection.insert_one(doc, session=session)
        logger.info(f"âœ… åˆ›å»ºå­˜æ¡£è®°å½•: {archived_data.id} (æ•°æ®æº: {archived_data.data_source_id}, ç±»å‹: {archived_data.data_type})")
        return archived_data

    async def find_by_data_source(
        self,
        data_source_id: str,
        limit: int = 100,
        skip: int = 0
    ) -> List[ArchivedData]:
        """æŒ‰æ•°æ®æºæŸ¥è¯¢å­˜æ¡£æ•°æ®ï¼ˆåˆ†é¡µï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            limit: æ¯é¡µæ•°é‡
            skip: è·³è¿‡æ•°é‡

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“åˆ—è¡¨
        """
        cursor = self.collection.find(
            {"data_source_id": data_source_id}
        ).sort("created_at", -1).skip(skip).limit(limit)

        docs = await cursor.to_list(length=limit)
        return [self._from_document(doc) for doc in docs]

    async def count_by_data_source(
        self,
        data_source_id: str
    ) -> int:
        """ç»Ÿè®¡æ•°æ®æºçš„å­˜æ¡£æ•°é‡

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            å­˜æ¡£è®°å½•æ•°é‡
        """
        return await self.collection.count_documents(
            {"data_source_id": data_source_id}
        )

    async def find_by_id(
        self,
        archived_data_id: str,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> Optional[ArchivedData]:
        """æ ¹æ®IDæŸ¥è¯¢å­˜æ¡£æ•°æ®

        Args:
            archived_data_id: å­˜æ¡£æ•°æ®ID
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        doc = await self.collection.find_one(
            {"id": archived_data_id},
            session=session
        )
        return self._from_document(doc) if doc else None

    async def find_by_original_data_id(
        self,
        original_data_id: str,
        data_type: str
    ) -> Optional[ArchivedData]:
        """æ ¹æ®åŸå§‹æ•°æ®IDæŸ¥è¯¢å­˜æ¡£ï¼ˆé˜²é‡å¤å­˜æ¡£ï¼‰

        Args:
            original_data_id: åŸå§‹æ•°æ®ID
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        doc = await self.collection.find_one({
            "original_data_id": original_data_id,
            "data_type": data_type
        })
        return self._from_document(doc) if doc else None

    async def delete_by_data_source(
        self,
        data_source_id: str,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> int:
        """åˆ é™¤æ•°æ®æºçš„æ‰€æœ‰å­˜æ¡£è®°å½•ï¼ˆçº§è”åˆ é™¤ï¼‰

        é€šå¸¸åœ¨åˆ é™¤æ•°æ®æºæ—¶ç”±Serviceå±‚äº‹åŠ¡è°ƒç”¨

        Args:
            data_source_id: æ•°æ®æºID
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ é™¤çš„è®°å½•æ•°é‡
        """
        result = await self.collection.delete_many(
            {"data_source_id": data_source_id},
            session=session
        )
        deleted_count = result.deleted_count
        logger.info(f"ğŸ—‘ï¸  åˆ é™¤æ•°æ®æºå­˜æ¡£: {data_source_id}ï¼Œå…±{deleted_count}æ¡è®°å½•")
        return deleted_count

    async def get_statistics(
        self,
        data_source_id: str
    ) -> Dict[str, Any]:
        """è·å–æ•°æ®æºå­˜æ¡£ç»Ÿè®¡ä¿¡æ¯

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        pipeline = [
            {"$match": {"data_source_id": data_source_id}},
            {
                "$group": {
                    "_id": "$data_type",
                    "count": {"$sum": 1},
                    "total_content_size": {"$sum": {"$strLenCP": "$content"}}
                }
            }
        ]

        results = await self.collection.aggregate(pipeline).to_list(length=None)

        stats = {
            "data_source_id": data_source_id,
            "total_count": 0,
            "scheduled_count": 0,
            "instant_count": 0,
            "total_content_size": 0,
            "by_type": {}
        }

        for result in results:
            data_type = result["_id"]
            count = result["count"]
            content_size = result["total_content_size"]

            stats["total_count"] += count
            stats["total_content_size"] += content_size
            stats["by_type"][data_type] = {
                "count": count,
                "content_size": content_size
            }

            if data_type == "scheduled":
                stats["scheduled_count"] = count
            elif data_type == "instant":
                stats["instant_count"] = count

        return stats

    async def find_with_pagination(
        self,
        data_source_id: str,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[ArchivedData], int]:
        """åˆ†é¡µæŸ¥è¯¢å­˜æ¡£æ•°æ®ï¼ˆå«æ€»æ•°ï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡

        Returns:
            (å­˜æ¡£æ•°æ®åˆ—è¡¨, æ€»è®°å½•æ•°) å…ƒç»„
        """
        skip = (page - 1) * page_size

        # å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢å’Œè®¡æ•°
        cursor = self.collection.find(
            {"data_source_id": data_source_id}
        ).sort("created_at", -1).skip(skip).limit(page_size)

        docs, total = await asyncio.gather(
            cursor.to_list(length=page_size),
            self.count_by_data_source(data_source_id)
        )

        archived_list = [self._from_document(doc) for doc in docs]
        return archived_list, total

    # ==========================================
    # æ–‡æ¡£è½¬æ¢æ–¹æ³•
    # ==========================================

    def _to_document(self, archived_data: ArchivedData) -> Dict[str, Any]:
        """å®ä½“è½¬MongoDBæ–‡æ¡£

        Args:
            archived_data: å­˜æ¡£æ•°æ®å®ä½“

        Returns:
            MongoDBæ–‡æ¡£å­—å…¸
        """
        return {
            "id": archived_data.id,
            "data_source_id": archived_data.data_source_id,
            "original_data_id": archived_data.original_data_id,
            "data_type": archived_data.data_type,
            # æ ¸å¿ƒå†…å®¹
            "title": archived_data.title,
            "url": archived_data.url,
            "content": archived_data.content,  # å®Œæ•´å†…å®¹
            "snippet": archived_data.snippet,
            "published_date": archived_data.published_date,
            # Firecrawlå­—æ®µ
            "markdown_content": archived_data.markdown_content,
            "html_content": archived_data.html_content,
            # ç±»å‹ç‰¹å®šå­—æ®µå’Œå…ƒæ•°æ®
            "type_specific_fields": archived_data.type_specific_fields,
            "metadata": archived_data.metadata,
            # å­˜æ¡£å…ƒä¿¡æ¯
            "archived_at": archived_data.archived_at,
            "archived_by": archived_data.archived_by,
            "archived_reason": archived_data.archived_reason,
            # åŸå§‹æ•°æ®è¿½æº¯
            "original_created_at": archived_data.original_created_at,
            "original_status": archived_data.original_status,
            # ç³»ç»Ÿå­—æ®µ
            "created_at": archived_data.created_at,
            "updated_at": archived_data.updated_at,
        }

    def _from_document(self, doc: Dict[str, Any]) -> ArchivedData:
        """MongoDBæ–‡æ¡£è½¬å®ä½“

        Args:
            doc: MongoDBæ–‡æ¡£å­—å…¸

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“
        """
        return ArchivedData(
            id=doc["id"],
            data_source_id=doc.get("data_source_id", ""),
            original_data_id=doc.get("original_data_id", ""),
            data_type=doc.get("data_type", ""),
            # æ ¸å¿ƒå†…å®¹
            title=doc.get("title", ""),
            url=doc.get("url", ""),
            content=doc.get("content", ""),
            snippet=doc.get("snippet"),
            published_date=doc.get("published_date"),
            # Firecrawlå­—æ®µ
            markdown_content=doc.get("markdown_content"),
            html_content=doc.get("html_content"),
            # ç±»å‹ç‰¹å®šå­—æ®µå’Œå…ƒæ•°æ®
            type_specific_fields=doc.get("type_specific_fields", {}),
            metadata=doc.get("metadata", {}),
            # å­˜æ¡£å…ƒä¿¡æ¯
            archived_at=doc.get("archived_at", datetime.utcnow()),
            archived_by=doc.get("archived_by", ""),
            archived_reason=doc.get("archived_reason", "confirm"),
            # åŸå§‹æ•°æ®è¿½æº¯
            original_created_at=doc.get("original_created_at"),
            original_status=doc.get("original_status", ""),
            # ç³»ç»Ÿå­—æ®µ
            created_at=doc.get("created_at", datetime.utcnow()),
            updated_at=doc.get("updated_at", datetime.utcnow()),
        )


# éœ€è¦å¯¼å…¥asyncioç”¨äºå¹¶è¡ŒæŸ¥è¯¢
import asyncio
