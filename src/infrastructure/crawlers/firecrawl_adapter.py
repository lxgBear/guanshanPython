"""
Firecrawlçˆ¬è™«é€‚é…å™¨å®ç°
å®ç°é¢†åŸŸå±‚å®šä¹‰çš„CrawlerInterfaceæ¥å£
"""
import asyncio
import hashlib
from typing import Optional, Dict, Any, List
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from firecrawl import Firecrawl
from firecrawl.v2.types import ScrapeOptions

from src.core.domain.interfaces.crawler_interface import (
    CrawlerInterface,
    CrawlResult,
    CrawlException
)
from src.config import settings
from src.utils.logger import get_logger

logger = get_logger(__name__)


class FirecrawlAdapter(CrawlerInterface):
    """
    Firecrawlçˆ¬è™«é€‚é…å™¨
    å°†Firecrawl APIé€‚é…ä¸ºç³»ç»Ÿçš„CrawlerInterface
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–Firecrawlé€‚é…å™¨
        
        Args:
            api_key: Firecrawl APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®ä¸­è¯»å–
        """
        self.api_key = api_key or settings.FIRECRAWL_API_KEY
        if not self.api_key:
            raise ValueError("Firecrawl APIå¯†é’¥æœªé…ç½®")

        # v4.6.0: ä½¿ç”¨ Firecrawl (v2 API)
        self.client = Firecrawl(api_key=self.api_key)
        self.timeout = settings.FIRECRAWL_TIMEOUT
        self.max_retries = settings.FIRECRAWL_MAX_RETRIES

        logger.info("Firecrawl v2 é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((ConnectionError, TimeoutError))
    )
    async def scrape(self, url: str, **options) -> CrawlResult:
        """
        çˆ¬å–å•ä¸ªé¡µé¢

        Args:
            url: ç›®æ ‡URL
            **options: çˆ¬å–é€‰é¡¹
                - wait_for: ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 1000
                - include_tags: è¦åŒ…å«çš„ HTML æ ‡ç­¾åˆ—è¡¨
                - exclude_tags: è¦æ’é™¤çš„ HTML æ ‡ç­¾åˆ—è¡¨ï¼Œé»˜è®¤ Noneï¼ˆä¸æ’é™¤ï¼‰
                - only_main_content: åªæå–ä¸»è¦å†…å®¹ï¼Œé»˜è®¤ Falseï¼ˆè·å–å®Œæ•´ HTMLï¼‰
                - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            CrawlResult: çˆ¬å–ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹çˆ¬å–URL: {url}")

            # Firecrawl v2 API: ä½¿ç”¨å‘½åå‚æ•°
            formats = options.get('formats', ['markdown', 'html'])
            only_main_content = options.get('only_main_content', False)  # é»˜è®¤ False è·å–å®Œæ•´ HTML
            wait_for = options.get('wait_for', 1000)
            include_tags = options.get('include_tags')
            exclude_tags = options.get('exclude_tags')
            timeout = options.get('timeout', self.timeout)

            logger.info(f"çˆ¬å–å‚æ•°: formats={formats}, onlyMainContent={only_main_content}, waitFor={wait_for}ms")

            # v4.6.0: ä½¿ç”¨ v2 API çš„ scrape() æ–¹æ³•ï¼ˆåŒæ­¥ï¼‰
            result = await asyncio.to_thread(
                self.client.scrape,
                url,
                formats=formats,
                only_main_content=only_main_content,
                wait_for=wait_for,
                include_tags=include_tags,
                exclude_tags=exclude_tags,
                timeout=timeout
            )

            # å¤„ç†ç»“æœï¼ˆv2 è¿”å› Document å¯¹è±¡ï¼‰
            crawl_result = CrawlResult(
                url=url,
                content=getattr(result, 'content', '') or '',
                markdown=getattr(result, 'markdown', None),
                html=getattr(result, 'html', None),
                metadata=getattr(result, 'metadata', {}),
                screenshot=getattr(result, 'screenshot', None)
            )

            logger.info(f"æˆåŠŸçˆ¬å–URL: {url}")
            return crawl_result

        except asyncio.TimeoutError:
            logger.error(f"çˆ¬å–è¶…æ—¶: {url}")
            raise CrawlException(f"çˆ¬å–è¶…æ—¶ ({self.timeout}ç§’)", url=url)
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            raise CrawlException(f"çˆ¬å–å¤±è´¥: {str(e)}", url=url)
    
    async def crawl(self, url: str, limit: int = 10, **options) -> List[CrawlResult]:
        """
        çˆ¬å–æ•´ä¸ªç½‘ç«™

        Args:
            url: èµ·å§‹URL
            limit: æœ€å¤§é¡µé¢æ•°
            **options: çˆ¬å–é€‰é¡¹
                - prompt: è‡ªç„¶è¯­è¨€æè¿°çˆ¬å–æ„å›¾ï¼ˆv2 APIæ–°å¢ï¼‰
                - max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
                - include_paths: åŒ…å«çš„URLè·¯å¾„æ¨¡å¼
                - exclude_paths: æ’é™¤çš„URLè·¯å¾„æ¨¡å¼
                - only_main_content: åªæå–ä¸»è¦å†…å®¹ï¼Œé»˜è®¤ Falseï¼ˆè·å–å®Œæ•´ HTMLï¼‰
                - wait_for: ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                - exclude_tags: æ’é™¤çš„HTMLæ ‡ç­¾ï¼Œé»˜è®¤ Noneï¼ˆä¸æ’é™¤ï¼‰

        Returns:
            List[CrawlResult]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹çˆ¬å–ç½‘ç«™: {url}, é™åˆ¶: {limit}é¡µ")

            # Firecrawl v2 API: ä½¿ç”¨å‘½åå‚æ•°ï¼ˆä¸å†ä½¿ç”¨ params å­—å…¸ï¼‰
            max_depth = options.get('max_depth', 3)
            include_paths = options.get('include_paths', [])
            exclude_paths = options.get('exclude_paths', [])
            prompt = options.get('prompt')  # v2 API æ–°å¢: è‡ªç„¶è¯­è¨€æè¿°

            # v2 API: æ„å»º scrape_options
            scrape_options = ScrapeOptions(
                formats=['markdown', 'html'],  # æ ¼å¼åˆ—è¡¨
                only_main_content=options.get('only_main_content', False),  # é»˜è®¤ False è·å–å®Œæ•´ HTML
                wait_for=options.get('wait_for', 1000),
                exclude_tags=options.get('exclude_tags')  # é»˜è®¤ Noneï¼Œä¸æ’é™¤ä»»ä½•æ ‡ç­¾
            )

            if prompt:
                logger.info(f"ğŸ¤– ä½¿ç”¨ prompt å‚æ•°: {prompt}")
            logger.info(f"Firecrawl v2 çˆ¬å–å‚æ•°: limit={limit}, max_discovery_depth={max_depth}")

            # v4.6.0: ä½¿ç”¨ v2 API çš„ crawl() æ–¹æ³•ï¼ˆåŒæ­¥ï¼Œè¿”å› CrawlJobï¼‰
            # timeout=None è¡¨ç¤ºæ°¸ä¸è¶…æ—¶,è®©çˆ¬å–ä»»åŠ¡å®Œæ•´æ‰§è¡Œ
            crawl_params = {
                "url": url,
                "limit": limit,
                "max_discovery_depth": max_depth,
                "include_paths": include_paths,
                "exclude_paths": exclude_paths,
                "scrape_options": scrape_options,
                "poll_interval": 2,
                "timeout": None  # æ°¸ä¸è¶…æ—¶
            }

            # å¦‚æœæœ‰ promptï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if prompt:
                crawl_params["prompt"] = prompt

            job = await asyncio.to_thread(
                self.client.crawl,
                **crawl_params
            )

            logger.info(f"Firecrawl v2 crawl å®Œæˆï¼Œjob ç±»å‹: {type(job)}")

            # å¤„ç† CrawlJob ç»“æœ
            results = []
            if hasattr(job, 'data') and job.data:
                for document in job.data:
                    result = CrawlResult(
                        url=getattr(document, 'url', '') or '',
                        content=getattr(document, 'content', '') or '',
                        markdown=getattr(document, 'markdown', None),
                        html=getattr(document, 'html', None),
                        metadata=getattr(document, 'metadata', {})
                    )
                    results.append(result)

            logger.info(f"æˆåŠŸçˆ¬å–ç½‘ç«™: {url}, è·å¾— {len(results)} é¡µ")
            return results
            
        except Exception as e:
            logger.error(f"ç½‘ç«™çˆ¬å–å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            raise CrawlException(f"ç½‘ç«™çˆ¬å–å¤±è´¥: {str(e)}", url=url)
    
    async def map(self, url: str, limit: int = 100) -> List[str]:
        """
        ç”Ÿæˆç«™ç‚¹åœ°å›¾
        
        Args:
            url: ç›®æ ‡ç½‘ç«™URL
            limit: æœ€å¤§URLæ•°é‡
        
        Returns:
            List[str]: URLåˆ—è¡¨
        """
        try:
            logger.info(f"ç”Ÿæˆç«™ç‚¹åœ°å›¾: {url}, é™åˆ¶: {limit}")
            
            result = await self.client.map(url, limit=limit)
            urls = result.get('urls', [])
            
            logger.info(f"æˆåŠŸç”Ÿæˆç«™ç‚¹åœ°å›¾: {url}, å‘ç° {len(urls)} ä¸ªURL")
            return urls
            
        except Exception as e:
            logger.error(f"ç«™ç‚¹åœ°å›¾ç”Ÿæˆå¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            raise CrawlException(f"ç«™ç‚¹åœ°å›¾ç”Ÿæˆå¤±è´¥: {str(e)}", url=url)
    
    async def extract(self, url: str, schema: Dict) -> Dict:
        """
        æå–ç»“æ„åŒ–æ•°æ®
        
        Args:
            url: ç›®æ ‡URL
            schema: æå–æ¨¡å¼
        
        Returns:
            Dict: æå–çš„ç»“æ„åŒ–æ•°æ®
        """
        try:
            logger.info(f"æå–ç»“æ„åŒ–æ•°æ®: {url}")
            
            # Firecrawlçš„extractç«¯ç‚¹æ”¯æŒè‡ªç„¶è¯­è¨€æè¿°
            result = await self.client.extract(
                url=url,
                schema=schema,
                formats=['markdown']
            )
            
            extracted_data = result.get('data', {})
            logger.info(f"æˆåŠŸæå–æ•°æ®: {url}")
            return extracted_data
            
        except Exception as e:
            logger.error(f"æ•°æ®æå–å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            raise CrawlException(f"æ•°æ®æå–å¤±è´¥: {str(e)}", url=url)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=4, max=20),
        retry=retry_if_exception_type((ConnectionError, TimeoutError))
    )
    async def search(self, query: str, limit: int = 10) -> List[CrawlResult]:
        """
        æœç´¢å¹¶çˆ¬å–ç»“æœ

        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœæ•°é‡é™åˆ¶ï¼ˆæ³¨æ„ï¼šFirecrawl v2å¯èƒ½ä¸æ”¯æŒæ­¤å‚æ•°ï¼‰

        Returns:
            List[CrawlResult]: æœç´¢ç»“æœ
        """
        try:
            logger.info(f"æœç´¢æŸ¥è¯¢: {query}, æœŸæœ›é™åˆ¶: {limit}")

            # v2 API: æ„å»º scrape_options
            scrape_options = ScrapeOptions(
                formats=['markdown', 'html']
            )

            logger.info(f"Firecrawl v2 æœç´¢å‚æ•°: limit={limit}")

            # v4.6.0: ä½¿ç”¨ v2 API çš„ search() æ–¹æ³•ï¼ˆè¿”å› SearchDataï¼‰
            search_data = await asyncio.to_thread(
                self.client.search,
                query,
                limit=limit,
                scrape_options=scrape_options,
                timeout=self.timeout
            )

            # å¤„ç† SearchData ç»“æœ
            results = []
            if hasattr(search_data, 'data') and search_data.data:
                for document in search_data.data[:limit]:
                    crawl_result = CrawlResult(
                        url=getattr(document, 'url', '') or '',
                        content=getattr(document, 'content', '') or getattr(document, 'markdown', '') or '',
                        markdown=getattr(document, 'markdown', None),
                        html=getattr(document, 'html', None),
                        metadata=getattr(document, 'metadata', {})
                    )
                    results.append(crawl_result)

            logger.info(f"æœç´¢å®Œæˆ: {query}, è·å¾— {len(results)} ä¸ªç»“æœ")
            return results

        except asyncio.TimeoutError:
            error_msg = f"æœç´¢è¶…æ—¶ (è¶…è¿‡{self.timeout}ç§’): {query}"
            logger.error(error_msg)
            raise CrawlException(error_msg)
        except Exception as e:
            error_msg = f"æœç´¢å¤±è´¥: {query}, é”™è¯¯ç±»å‹: {type(e).__name__}, è¯¦æƒ…: {str(e) or 'æ— è¯¦ç»†ä¿¡æ¯'}"
            logger.error(error_msg)
            raise CrawlException(error_msg)
    
    def _build_scrape_options(self, options: Dict) -> Dict:
        """
        æ„å»ºFirecrawlçˆ¬å–é€‰é¡¹
        
        Args:
            options: ç”¨æˆ·é€‰é¡¹
        
        Returns:
            Dict: Firecrawlé€‰é¡¹
        """
        scrape_options = {
            'formats': ['markdown', 'html'],
            'waitFor': options.get('wait_for', 1000)
        }
        
        # æ·»åŠ åŒ…å«/æ’é™¤æ ‡ç­¾
        if 'include_tags' in options:
            scrape_options['includeTags'] = options['include_tags']
        if 'exclude_tags' in options:
            scrape_options['excludeTags'] = options.get('exclude_tags', ['nav', 'footer', 'header'])
        
        # æ·»åŠ é¡µé¢äº¤äº’åŠ¨ä½œ
        if 'actions' in options:
            scrape_options['actions'] = options['actions']
        
        return scrape_options
    
    def _process_scrape_result(self, url: str, result: Dict) -> CrawlResult:
        """
        å¤„ç†çˆ¬å–ç»“æœ
        
        Args:
            url: åŸå§‹URL
            result: Firecrawlè¿”å›çš„ç»“æœ
        
        Returns:
            CrawlResult: æ ‡å‡†åŒ–çš„çˆ¬å–ç»“æœ
        """
        return CrawlResult(
            url=url,
            content=result.get('content', ''),
            markdown=result.get('markdown'),
            html=result.get('html'),
            metadata=result.get('metadata', {}),
            screenshot=result.get('screenshot')
        )


class FirecrawlRateLimiter:
    """
    Firecrawlé€Ÿç‡é™åˆ¶å™¨
    ç®¡ç†APIè°ƒç”¨é€Ÿç‡ï¼Œé¿å…è§¦å‘é™åˆ¶
    """
    
    def __init__(self, max_requests_per_minute: int = 60):
        self.max_requests_per_minute = max_requests_per_minute
        self.request_times: List[float] = []
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        """è·å–æ‰§è¡Œæƒé™ï¼Œå¿…è¦æ—¶ç­‰å¾…"""
        async with self.lock:
            import time
            now = time.time()
            
            # æ¸…ç†ä¸€åˆ†é’Ÿå‰çš„è®°å½•
            self.request_times = [
                t for t in self.request_times
                if now - t < 60
            ]
            
            # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…
            if len(self.request_times) >= self.max_requests_per_minute:
                wait_time = 60 - (now - self.request_times[0])
                if wait_time > 0:
                    logger.warning(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                    await asyncio.sleep(wait_time)
            
            # è®°å½•æ–°è¯·æ±‚
            self.request_times.append(now)