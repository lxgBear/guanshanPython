"""
MongoDB AIå¤„ç†ç»“æœ Repository å®ç°

åŸºäº MongoDB çš„ AI å¤„ç†ç»“æœæ•°æ®è®¿é—®å®ç°ï¼Œå®ç° IProcessedResultRepository æ¥å£ã€‚

Version: v3.0.0 (æ¨¡å—åŒ–æ¶æ„)
"""

from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple

from src.core.domain.entities.processed_result import ProcessedResult, ProcessedStatus
from src.infrastructure.persistence.interfaces import IProcessedResultRepository
from src.infrastructure.persistence.interfaces.i_repository import (
    RepositoryException,
    EntityNotFoundException
)
from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


class MongoProcessedResultRepository(IProcessedResultRepository):
    """
    MongoDB AIå¤„ç†ç»“æœ Repository å®ç°

    ä½¿ç”¨ MongoDB ä½œä¸ºæ•°æ®å­˜å‚¨ï¼Œå®ç° IProcessedResultRepository æ¥å£ã€‚
    é›†åˆå: news_results
    """

    def __init__(self):
        self.collection_name = "news_results"

    async def _get_collection(self):
        """è·å– MongoDB é›†åˆ"""
        db = await get_mongodb_database()
        return db[self.collection_name]

    def _result_to_dict(self, result: ProcessedResult) -> Dict[str, Any]:
        """å°† ProcessedResult å®ä½“è½¬æ¢ä¸º MongoDB æ–‡æ¡£ï¼ˆv2.0.1 æ‰©å±•ï¼‰"""
        return {
            "_id": str(result.id),
            "raw_result_id": str(result.raw_result_id),
            "task_id": str(result.task_id),
            # åŸå§‹å­—æ®µï¼ˆv2.0.1 æ–°å¢ï¼‰
            "title": result.title,
            "url": result.url,
            "source_url": result.source_url,
            "content": result.content,
            "snippet": result.snippet,
            "markdown_content": result.markdown_content,
            "html_content": result.html_content,
            "author": result.author,
            "published_date": result.published_date,
            "language": result.language,
            "source": result.source,
            "metadata": result.metadata,
            "quality_score": result.quality_score,
            "relevance_score": result.relevance_score,
            "search_position": result.search_position,
            # AI å¤„ç†æ•°æ®
            "content_zh": result.content_zh,
            "title_generated": result.title_generated,
            "translated_title": result.translated_title,
            "translated_content": result.translated_content,
            "summary": result.summary,
            "key_points": result.key_points,
            "cls_results": result.cls_results,
            "sentiment": result.sentiment,
            "categories": result.categories,
            "html_ctx_llm": result.html_ctx_llm,
            "html_ctx_regex": result.html_ctx_regex,
            "article_published_time": result.article_published_time,
            "article_tag": result.article_tag,
            # AI å…ƒæ•°æ®
            "ai_model": result.ai_model,
            "ai_processing_time_ms": result.ai_processing_time_ms,
            "ai_confidence_score": result.ai_confidence_score,
            "ai_metadata": result.ai_metadata,
            "processing_status": result.processing_status,
            "http_status_code": result.http_status_code,
            "is_test_data": result.is_test_data,
            # ç”¨æˆ·æ“ä½œ
            "status": result.status.value,
            "user_rating": result.user_rating,
            "user_notes": result.user_notes,
            # æ—¶é—´æˆ³
            "created_at": result.created_at,
            "processed_at": result.processed_at,
            "updated_at": result.updated_at,
            # é”™è¯¯å¤„ç†
            "processing_error": result.processing_error,
            "retry_count": result.retry_count,
            # news_results åµŒå¥—å­—æ®µï¼ˆv2.0.2ï¼‰
            "news_results": result.news_results,
            # å†…å®¹æ¸…ç†å­—æ®µï¼ˆv2.0.2ï¼‰
            "content_cleaned": result.content_cleaned
        }

    def _dict_to_result(self, data: Dict[str, Any]) -> ProcessedResult:
        """å°† MongoDB æ–‡æ¡£è½¬æ¢ä¸º ProcessedResult å®ä½“ï¼ˆv2.0.1 æ‰©å±•ï¼‰"""
        # å®‰å…¨åœ°è§£æ status å­—æ®µï¼Œé˜²æ­¢æ•°æ®æŸåå¯¼è‡´çš„é”™è¯¯
        status_value = data.get("status", "pending")
        try:
            status = ProcessedStatus(status_value)
        except ValueError:
            # å¦‚æœ status å€¼æ— æ•ˆï¼Œè®°å½•é”™è¯¯å¹¶ä½¿ç”¨é»˜è®¤å€¼
            logger.error(
                f"âš ï¸  æ•°æ®æŸåï¼šæ— æ•ˆçš„ status å€¼ '{status_value}' (æ–‡æ¡£ID: {data.get('_id')}), "
                f"å·²é‡ç½®ä¸º 'pending'"
            )
            status = ProcessedStatus.PENDING

        return ProcessedResult(
            id=str(data.get("_id", "")),
            raw_result_id=str(data.get("raw_result_id", "")),
            task_id=str(data.get("task_id", "")),
            # åŸå§‹å­—æ®µï¼ˆv2.0.1 æ–°å¢ï¼‰
            title=data.get("title", ""),
            url=data.get("url", ""),
            source_url=data.get("source_url", ""),
            content=data.get("content", ""),
            snippet=data.get("snippet"),
            markdown_content=data.get("markdown_content"),
            html_content=data.get("html_content"),
            author=data.get("author"),
            published_date=data.get("published_date"),
            language=data.get("language"),
            source=data.get("source", "web"),
            metadata=data.get("metadata", {}),
            quality_score=data.get("quality_score", 0.0),
            relevance_score=data.get("relevance_score", 0.0),
            search_position=data.get("search_position", 0),
            # AI å¤„ç†æ•°æ®
            content_zh=data.get("content_zh"),
            title_generated=data.get("title_generated"),
            translated_title=data.get("translated_title"),
            translated_content=data.get("translated_content"),
            summary=data.get("summary"),
            key_points=data.get("key_points", []),
            cls_results=data.get("cls_results"),
            sentiment=data.get("sentiment"),
            categories=data.get("categories", []),
            html_ctx_llm=data.get("html_ctx_llm"),
            html_ctx_regex=data.get("html_ctx_regex"),
            article_published_time=data.get("article_published_time"),
            article_tag=data.get("article_tag"),
            # AI å…ƒæ•°æ®
            ai_model=data.get("ai_model"),
            ai_processing_time_ms=data.get("ai_processing_time_ms", 0),
            ai_confidence_score=data.get("ai_confidence_score", 0.0),
            ai_metadata=data.get("ai_metadata", {}),
            processing_status=data.get("processing_status", "pending"),
            http_status_code=data.get("http_status_code"),
            is_test_data=data.get("is_test_data", False),
            # ç”¨æˆ·æ“ä½œ
            status=status,
            user_rating=data.get("user_rating"),
            user_notes=data.get("user_notes"),
            # æ—¶é—´æˆ³
            created_at=data.get("created_at", datetime.utcnow()),
            processed_at=data.get("processed_at"),
            updated_at=data.get("updated_at", datetime.utcnow()),
            # é”™è¯¯å¤„ç†
            processing_error=data.get("processing_error"),
            retry_count=data.get("retry_count", 0),
            # news_results åµŒå¥—å­—æ®µï¼ˆv2.0.2ï¼‰
            news_results=data.get("news_results"),
            # å†…å®¹æ¸…ç†å­—æ®µï¼ˆv2.0.2ï¼‰
            content_cleaned=data.get("content_cleaned")
        )

    # ==================== IBasicRepository å®ç° ====================

    async def create(self, entity: ProcessedResult) -> str:
        """åˆ›å»º AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            result_dict = self._result_to_dict(entity)

            await collection.insert_one(result_dict)
            logger.info(f"âœ… åˆ›å»º AI å¤„ç†ç»“æœ: {entity.title[:50]} (ID: {entity.id})")

            return str(entity.id)

        except Exception as e:
            logger.error(f"âŒ åˆ›å»º AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ›å»º AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def get_by_id(self, id: str) -> Optional[ProcessedResult]:
        """æ ¹æ® ID è·å– AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"_id": id})

            if data:
                return self._dict_to_result(data)
            return None

        except Exception as e:
            logger.error(f"âŒ è·å– AI å¤„ç†ç»“æœå¤±è´¥ (ID: {id}): {e}")
            raise RepositoryException(f"è·å– AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def update(self, entity: ProcessedResult) -> bool:
        """æ›´æ–° AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            result_dict = self._result_to_dict(entity)
            result_dict.pop("_id")  # ç§»é™¤ ID å­—æ®µ

            result = await collection.update_one(
                {"_id": str(entity.id)},
                {"$set": result_dict}
            )

            if result.matched_count == 0:
                raise EntityNotFoundException(f"AI å¤„ç†ç»“æœä¸å­˜åœ¨: {entity.id}")

            logger.info(f"âœ… æ›´æ–° AI å¤„ç†ç»“æœ: {entity.title[:50]} (ID: {entity.id})")
            return result.modified_count > 0

        except EntityNotFoundException:
            raise
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–° AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def delete(self, id: str) -> bool:
        """åˆ é™¤ AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            result = await collection.delete_one({"_id": id})

            if result.deleted_count > 0:
                logger.info(f"âœ… åˆ é™¤ AI å¤„ç†ç»“æœ: {id}")
                return True

            return False

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ é™¤ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def exists(self, id: str) -> bool:
        """æ£€æŸ¥ AI å¤„ç†ç»“æœæ˜¯å¦å­˜åœ¨"""
        try:
            collection = await self._get_collection()
            count = await collection.count_documents({"_id": id}, limit=1)
            return count > 0

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ AI å¤„ç†ç»“æœå­˜åœ¨æ€§å¤±è´¥: {e}")
            raise RepositoryException(f"æ£€æŸ¥ AI å¤„ç†ç»“æœå­˜åœ¨æ€§å¤±è´¥: {e}", e)

    # ==================== IQueryableRepository å®ç° ====================

    async def find_all(self, limit: Optional[int] = None) -> List[ProcessedResult]:
        """è·å–æ‰€æœ‰ AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            cursor = collection.find({}).sort("created_at", -1)

            if limit:
                cursor = cursor.limit(limit)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results

        except Exception as e:
            logger.error(f"âŒ è·å–æ‰€æœ‰ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"è·å–æ‰€æœ‰ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def find_by_criteria(self, criteria: Dict[str, Any]) -> List[ProcessedResult]:
        """æ ¹æ®æ¡ä»¶æŸ¥è¯¢ AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            cursor = collection.find(criteria).sort("created_at", -1)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results

        except Exception as e:
            logger.error(f"âŒ æ¡ä»¶æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ¡ä»¶æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def count(self, criteria: Optional[Dict[str, Any]] = None) -> int:
        """ç»Ÿè®¡ AI å¤„ç†ç»“æœæ•°é‡"""
        try:
            collection = await self._get_collection()
            query = criteria if criteria else {}
            return await collection.count_documents(query)

        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡ AI å¤„ç†ç»“æœæ•°é‡å¤±è´¥: {e}")
            raise RepositoryException(f"ç»Ÿè®¡ AI å¤„ç†ç»“æœæ•°é‡å¤±è´¥: {e}", e)

    # ==================== IPaginatableRepository å®ç° ====================

    async def find_with_pagination(
        self,
        page: int,
        page_size: int,
        criteria: Optional[Dict[str, Any]] = None,
        sort_by: Optional[str] = None,
        sort_order: str = "desc"
    ) -> Tuple[List[ProcessedResult], int]:
        """åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœ"""
        if page < 1:
            raise ValueError("é¡µç å¿…é¡»å¤§äºç­‰äº1")
        if page_size < 1:
            raise ValueError("é¡µå¤§å°å¿…é¡»å¤§äºç­‰äº1")

        try:
            collection = await self._get_collection()
            query = criteria if criteria else {}

            # ç»Ÿè®¡æ€»æ•°
            total = await collection.count_documents(query)

            # åˆ†é¡µæŸ¥è¯¢
            skip = (page - 1) * page_size
            sort_field = sort_by if sort_by else "created_at"
            sort_direction = -1 if sort_order == "desc" else 1

            cursor = collection.find(query).sort(sort_field, sort_direction).skip(skip).limit(page_size)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results, total

        except ValueError:
            raise
        except Exception as e:
            logger.error(f"âŒ åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    # ==================== IBulkOperationRepository å®ç° ====================

    async def bulk_create(self, entities: List[ProcessedResult]) -> List[str]:
        """æ‰¹é‡åˆ›å»º AI å¤„ç†ç»“æœ"""
        if not entities:
            return []

        try:
            collection = await self._get_collection()
            result_dicts = [self._result_to_dict(entity) for entity in entities]

            result = await collection.insert_many(result_dicts)
            logger.info(f"âœ… æ‰¹é‡åˆ›å»º AI å¤„ç†ç»“æœ: {len(result.inserted_ids)} æ¡")

            return [str(id) for id in result.inserted_ids]

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ›å»º AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡åˆ›å»º AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def bulk_update(self, entities: List[ProcessedResult]) -> int:
        """æ‰¹é‡æ›´æ–° AI å¤„ç†ç»“æœ"""
        if not entities:
            return 0

        try:
            collection = await self._get_collection()
            updated_count = 0

            for entity in entities:
                result_dict = self._result_to_dict(entity)
                result_dict.pop("_id")

                result = await collection.update_one(
                    {"_id": str(entity.id)},
                    {"$set": result_dict}
                )
                updated_count += result.modified_count

            logger.info(f"âœ… æ‰¹é‡æ›´æ–° AI å¤„ç†ç»“æœ: {updated_count} æ¡")
            return updated_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ›´æ–° AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡æ›´æ–° AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def bulk_delete(self, ids: List[str]) -> int:
        """æ‰¹é‡åˆ é™¤ AI å¤„ç†ç»“æœ"""
        if not ids:
            return 0

        try:
            collection = await self._get_collection()
            result = await collection.delete_many({"_id": {"$in": ids}})

            logger.info(f"âœ… æ‰¹é‡åˆ é™¤ AI å¤„ç†ç»“æœ: {result.deleted_count} æ¡")
            return result.deleted_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ é™¤ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡åˆ é™¤ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    # ==================== IProcessedResultRepository ç‰¹å®šæ–¹æ³• ====================

    async def find_by_task_id(
        self,
        task_id: str,
        status: Optional[ProcessedStatus] = None,
        limit: Optional[int] = None
    ) -> List[ProcessedResult]:
        """æ ¹æ®ä»»åŠ¡ ID æŸ¥è¯¢ AI å¤„ç†ç»“æœ"""
        try:
            criteria = {"task_id": task_id}
            if status is not None:
                criteria["status"] = status.value

            collection = await self._get_collection()
            cursor = collection.find(criteria).sort("created_at", -1)

            if limit:
                cursor = cursor.limit(limit)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results

        except Exception as e:
            logger.error(f"âŒ æŒ‰ä»»åŠ¡ ID æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŒ‰ä»»åŠ¡ ID æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def find_by_raw_result_id(
        self,
        raw_result_id: str
    ) -> Optional[ProcessedResult]:
        """æ ¹æ®åŸå§‹ç»“æœ ID æŸ¥è¯¢ AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"raw_result_id": raw_result_id})

            if data:
                return self._dict_to_result(data)
            return None

        except Exception as e:
            logger.error(f"âŒ æ ¹æ®åŸå§‹ç»“æœ ID è·å– AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ ¹æ®åŸå§‹ç»“æœ ID è·å– AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def find_by_status(
        self,
        status: ProcessedStatus,
        limit: Optional[int] = None
    ) -> List[ProcessedResult]:
        """æ ¹æ®çŠ¶æ€æŸ¥è¯¢ AI å¤„ç†ç»“æœ"""
        try:
            criteria = {"status": status.value}
            collection = await self._get_collection()
            cursor = collection.find(criteria).sort("created_at", -1)

            if limit:
                cursor = cursor.limit(limit)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results

        except Exception as e:
            logger.error(f"âŒ æŒ‰çŠ¶æ€æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŒ‰çŠ¶æ€æŸ¥è¯¢ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def find_pending_results(
        self,
        limit: Optional[int] = None
    ) -> List[ProcessedResult]:
        """æŸ¥è¯¢å¾…å¤„ç†çš„ç»“æœ"""
        try:
            return await self.find_by_status(ProcessedStatus.PENDING, limit)

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¾…å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŸ¥è¯¢å¾…å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def update_processing_status(
        self,
        result_id: str,
        status: ProcessedStatus,
        **kwargs
    ) -> bool:
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        try:
            collection = await self._get_collection()

            update_data = {
                "status": status.value,
                "updated_at": datetime.utcnow()
            }

            # åˆå¹¶é¢å¤–å­—æ®µ
            update_data.update(kwargs)

            # ç‰¹æ®Šå¤„ç†ï¼šCOMPLETED çŠ¶æ€è®¾ç½®å¤„ç†å®Œæˆæ—¶é—´
            if status == ProcessedStatus.COMPLETED and "processed_at" not in kwargs:
                update_data["processed_at"] = datetime.utcnow()

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            if result.matched_count == 0:
                logger.warning(f"âš ï¸  å¤„ç†çŠ¶æ€æœªæ›´æ–°ï¼ˆè®°å½•ä¸å­˜åœ¨ï¼‰: {result_id}")
                return False

            logger.info(f"âœ… æ›´æ–°å¤„ç†çŠ¶æ€: {result_id} -> {status.value}")
            return result.modified_count > 0

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {e}", e)

    async def save_ai_result(
        self,
        result_id: str,
        ai_data: Dict[str, Any]
    ) -> bool:
        """ä¿å­˜ AI å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()

            update_data = {
                "status": ProcessedStatus.COMPLETED.value,
                "processed_at": datetime.utcnow(),
                "updated_at": datetime.utcnow()
            }

            # åˆå¹¶ AI æ•°æ®
            update_data.update(ai_data)

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            if result.matched_count == 0:
                logger.warning(f"âš ï¸  AI ç»“æœæœªä¿å­˜ï¼ˆè®°å½•ä¸å­˜åœ¨ï¼‰: {result_id}")
                return False

            logger.info(f"âœ… ä¿å­˜ AI å¤„ç†ç»“æœ: {result_id}")
            return result.modified_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ AI å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"ä¿å­˜ AI å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def update_user_action(
        self,
        result_id: str,
        status: Optional[ProcessedStatus] = None,
        user_rating: Optional[int] = None,
        user_notes: Optional[str] = None
    ) -> bool:
        """æ›´æ–°ç”¨æˆ·æ“ä½œ"""
        try:
            collection = await self._get_collection()

            update_data = {"updated_at": datetime.utcnow()}

            if status is not None:
                update_data["status"] = status.value
            if user_rating is not None:
                if not 1 <= user_rating <= 5:
                    raise ValueError("ç”¨æˆ·è¯„åˆ†å¿…é¡»åœ¨ 1-5 ä¹‹é—´")
                update_data["user_rating"] = user_rating
            if user_notes is not None:
                update_data["user_notes"] = user_notes

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            if result.matched_count == 0:
                logger.warning(f"âš ï¸  ç”¨æˆ·æ“ä½œæœªæ›´æ–°ï¼ˆè®°å½•ä¸å­˜åœ¨ï¼‰: {result_id}")
                return False

            logger.info(f"âœ… æ›´æ–°ç”¨æˆ·æ“ä½œ: {result_id}")
            return result.modified_count > 0

        except ValueError as e:
            logger.error(f"âŒ ç”¨æˆ·æ“ä½œå‚æ•°é”™è¯¯: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç”¨æˆ·æ“ä½œå¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–°ç”¨æˆ·æ“ä½œå¤±è´¥: {e}", e)

    async def get_processing_statistics(
        self,
        task_id: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection = await self._get_collection()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            match_criteria = {}
            if task_id:
                match_criteria["task_id"] = task_id
            if start_date:
                match_criteria["created_at"] = {"$gte": start_date}
            if end_date:
                if "created_at" in match_criteria:
                    match_criteria["created_at"]["$lte"] = end_date
                else:
                    match_criteria["created_at"] = {"$lte": end_date}

            # èšåˆç»Ÿè®¡
            pipeline = [
                {"$match": match_criteria},
                {"$group": {
                    "_id": "$status",
                    "count": {"$sum": 1},
                    "avg_processing_time": {"$avg": "$ai_processing_time_ms"}
                }}
            ]

            status_stats = {status.value: 0 for status in ProcessedStatus}
            total = 0
            total_processing_time = 0

            async for doc in collection.aggregate(pipeline):
                status_stats[doc["_id"]] = doc["count"]
                total += doc["count"]
                if doc.get("avg_processing_time"):
                    total_processing_time += doc["avg_processing_time"]

            # è®¡ç®—æˆåŠŸç‡
            completed = status_stats.get(ProcessedStatus.COMPLETED.value, 0)
            failed = status_stats.get(ProcessedStatus.FAILED.value, 0)
            success_rate = (completed / (completed + failed)) * 100 if (completed + failed) > 0 else 0.0

            return {
                "total": total,
                "pending": status_stats.get(ProcessedStatus.PENDING.value, 0),
                "processing": status_stats.get(ProcessedStatus.PROCESSING.value, 0),
                "completed": completed,
                "failed": failed,
                "archived": status_stats.get(ProcessedStatus.ARCHIVED.value, 0),
                "deleted": status_stats.get(ProcessedStatus.DELETED.value, 0),
                "avg_processing_time_ms": total_processing_time / len([s for s in status_stats.values() if s > 0]) if total > 0 else 0,
                "success_rate": round(success_rate, 2)
            }

        except Exception as e:
            logger.error(f"âŒ è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            raise RepositoryException(f"è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}", e)

    async def find_with_pagination_and_filters(
        self,
        task_id: Optional[str] = None,
        status: Optional[ProcessedStatus] = None,
        categories: Optional[List[str]] = None,
        min_rating: Optional[int] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        page: int = 1,
        page_size: int = 20,
        sort_by: str = "created_at",
        sort_order: str = "desc"
    ) -> Tuple[List[ProcessedResult], int]:
        """åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœï¼ˆå¸¦å¤šç»´åº¦è¿‡æ»¤ï¼‰"""
        try:
            collection = await self._get_collection()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            criteria = {}

            if task_id:
                criteria["task_id"] = task_id
            if status:
                criteria["status"] = status.value
            if categories:
                criteria["categories"] = {"$in": categories}
            if min_rating:
                criteria["user_rating"] = {"$gte": min_rating}
            if start_date:
                criteria["created_at"] = {"$gte": start_date}
            if end_date:
                if "created_at" in criteria:
                    criteria["created_at"]["$lte"] = end_date
                else:
                    criteria["created_at"] = {"$lte": end_date}

            # ä½¿ç”¨åˆ†é¡µæŸ¥è¯¢æ–¹æ³•
            return await self.find_with_pagination(
                page=page,
                page_size=page_size,
                criteria=criteria,
                sort_by=sort_by,
                sort_order=sort_order
            )

        except Exception as e:
            logger.error(f"âŒ åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœï¼ˆå¸¦è¿‡æ»¤ï¼‰å¤±è´¥: {e}")
            raise RepositoryException(f"åˆ†é¡µæŸ¥è¯¢ AI å¤„ç†ç»“æœï¼ˆå¸¦è¿‡æ»¤ï¼‰å¤±è´¥: {e}", e)

    # ==================== é¢å¤–çš„ä¸šåŠ¡æ–¹æ³• (å‘åå…¼å®¹) ====================

    async def create_pending_result(self, raw_result_id: str, task_id: str) -> ProcessedResult:
        """
        åˆ›å»ºå¾…å¤„ç†çš„ç»“æœè®°å½•ï¼ˆv2.0.1 å¤åˆ¶åŸå§‹å­—æ®µï¼‰

        ä» search_results è·å–åŸå§‹æ•°æ®å¹¶å¤åˆ¶åˆ° processed_resultsï¼Œ
        é¿å…å‰ç«¯æŸ¥è¯¢æ—¶éœ€è¦ JOIN æ“ä½œã€‚
        """
        try:
            collection = await self._get_collection()

            # 1. ä» search_results æŸ¥è¯¢åŸå§‹æ•°æ®
            db = await get_mongodb_database()
            search_results_collection = db['search_results']
            raw_data = await search_results_collection.find_one({"_id": raw_result_id})

            # 2. åˆ›å»ºå¾…å¤„ç†ç»“æœå®ä½“ï¼Œå¤åˆ¶åŸå§‹å­—æ®µ
            if raw_data:
                result = ProcessedResult(
                    raw_result_id=raw_result_id,
                    task_id=task_id,
                    status=ProcessedStatus.PENDING,
                    # å¤åˆ¶åŸå§‹å­—æ®µï¼ˆv2.0.1ï¼‰
                    title=raw_data.get("title", ""),
                    url=raw_data.get("url", ""),
                    source_url=raw_data.get("source_url", ""),
                    content=raw_data.get("content", ""),
                    snippet=raw_data.get("snippet"),
                    markdown_content=raw_data.get("markdown_content"),
                    html_content=raw_data.get("html_content"),
                    author=raw_data.get("author"),
                    published_date=raw_data.get("published_date"),
                    language=raw_data.get("language"),
                    source=raw_data.get("source", "web"),
                    metadata=raw_data.get("metadata", {}),
                    quality_score=raw_data.get("quality_score", 0.0),
                    relevance_score=raw_data.get("relevance_score", 0.0),
                    search_position=raw_data.get("search_position", 0)
                )
            else:
                # å¦‚æœæ‰¾ä¸åˆ°åŸå§‹æ•°æ®ï¼Œåˆ›å»ºæœ€å°è®°å½•
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŸå§‹ç»“æœæ•°æ®: {raw_result_id}")
                result = ProcessedResult(
                    raw_result_id=raw_result_id,
                    task_id=task_id,
                    status=ProcessedStatus.PENDING
                )

            # 3. ä¿å­˜åˆ°æ•°æ®åº“
            result_dict = self._result_to_dict(result)
            await collection.insert_one(result_dict)

            logger.info(f"âœ… åˆ›å»ºå¾…å¤„ç†ç»“æœ: raw_result_id={raw_result_id}, processed_id={result.id}")
            return result

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¾…å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ›å»ºå¾…å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def bulk_create_pending_results(
        self,
        raw_result_ids: List[str],
        task_id: str
    ) -> List[ProcessedResult]:
        """
        æ‰¹é‡åˆ›å»ºå¾…å¤„ç†ç»“æœï¼ˆv2.0.1 å¤åˆ¶åŸå§‹å­—æ®µï¼‰
        """
        try:
            collection = await self._get_collection()

            # 1. ä» search_results æŸ¥è¯¢åŸå§‹æ•°æ®
            db = await get_mongodb_database()
            search_results_collection = db['search_results']

            # æ‰¹é‡æŸ¥è¯¢åŸå§‹ç»“æœ
            search_results_cursor = search_results_collection.find({
                "_id": {"$in": raw_result_ids}
            })

            # æ„å»º ID -> åŸå§‹ç»“æœçš„æ˜ å°„
            search_results_map = {}
            async for raw_data in search_results_cursor:
                search_results_map[str(raw_data["_id"])] = raw_data

            # 2. åˆ›å»ºæ‰€æœ‰å¾…å¤„ç†ç»“æœå®ä½“ï¼Œå¤åˆ¶åŸå§‹å­—æ®µ
            results = []
            for raw_id in raw_result_ids:
                raw_data = search_results_map.get(raw_id)

                if raw_data:
                    # å¤åˆ¶åŸå§‹å­—æ®µåˆ° ProcessedResult
                    result = ProcessedResult(
                        raw_result_id=raw_id,
                        task_id=task_id,
                        status=ProcessedStatus.PENDING,
                        # å¤åˆ¶åŸå§‹å­—æ®µï¼ˆv2.0.1ï¼‰
                        title=raw_data.get("title", ""),
                        url=raw_data.get("url", ""),
                        source_url=raw_data.get("source_url", ""),
                        content=raw_data.get("content", ""),
                        snippet=raw_data.get("snippet"),
                        markdown_content=raw_data.get("markdown_content"),
                        html_content=raw_data.get("html_content"),
                        author=raw_data.get("author"),
                        published_date=raw_data.get("published_date"),
                        language=raw_data.get("language"),
                        source=raw_data.get("source", "web"),
                        metadata=raw_data.get("metadata", {}),
                        quality_score=raw_data.get("quality_score", 0.0),
                        relevance_score=raw_data.get("relevance_score", 0.0),
                        search_position=raw_data.get("search_position", 0)
                    )
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°åŸå§‹æ•°æ®ï¼Œåˆ›å»ºæœ€å°è®°å½•
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŸå§‹ç»“æœæ•°æ®: {raw_id}")
                    result = ProcessedResult(
                        raw_result_id=raw_id,
                        task_id=task_id,
                        status=ProcessedStatus.PENDING
                    )

                results.append(result)

            # 3. æ‰¹é‡æ’å…¥
            if results:
                result_dicts = [self._result_to_dict(r) for r in results]
                await collection.insert_many(result_dicts)
                logger.info(f"âœ… æ‰¹é‡åˆ›å»ºå¾…å¤„ç†ç»“æœ: {len(results)}æ¡ï¼ˆå·²å¤åˆ¶åŸå§‹å­—æ®µï¼‰")

            return results

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ›å»ºå¾…å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡åˆ›å»ºå¾…å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def get_by_task(
        self,
        task_id: str,
        status: Optional[ProcessedStatus] = None,
        processing_status: Optional[str] = None,
        page: int = 1,
        page_size: int = 20
    ) -> Tuple[List[ProcessedResult], int]:
        """è·å–ä»»åŠ¡çš„å¤„ç†ç»“æœï¼ˆæ”¯æŒçŠ¶æ€ç­›é€‰å’Œåˆ†é¡µï¼‰"""
        try:
            collection = await self._get_collection()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {"task_id": task_id}
            if status is not None:
                query["status"] = status.value
            if processing_status is not None:
                query["processing_status"] = processing_status

            # æ€»æ•°
            total = await collection.count_documents(query)

            # åˆ†é¡µæŸ¥è¯¢
            skip = (page - 1) * page_size
            cursor = collection.find(query).sort("created_at", -1).skip(skip).limit(page_size)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results, total

        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"è·å–ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}", e)

    async def get_status_statistics(self, task_id: str) -> Dict[str, int]:
        """è·å–ä»»åŠ¡çš„çŠ¶æ€ç»Ÿè®¡"""
        try:
            collection = await self._get_collection()

            pipeline = [
                {"$match": {"task_id": task_id}},
                {"$group": {
                    "_id": "$status",
                    "count": {"$sum": 1}
                }}
            ]

            status_counts = {status.value: 0 for status in ProcessedStatus}

            async for doc in collection.aggregate(pipeline):
                status_counts[doc["_id"]] = doc["count"]

            return status_counts

        except Exception as e:
            logger.error(f"âŒ è·å–çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}")
            raise RepositoryException(f"è·å–çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}", e)

    async def get_failed_results(self, max_retry: int = 3) -> List[ProcessedResult]:
        """è·å–å¤±è´¥çš„ç»“æœï¼ˆç”¨äºé‡è¯•ï¼‰"""
        try:
            collection = await self._get_collection()

            cursor = collection.find({
                "status": ProcessedStatus.FAILED.value,
                "retry_count": {"$lt": max_retry}
            }).sort("updated_at", 1)  # æŒ‰æ›´æ–°æ—¶é—´å‡åºï¼ˆä¼˜å…ˆé‡è¯•æ—§çš„ï¼‰

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            if results:
                logger.info(f"ğŸ“‹ æ‰¾åˆ°{len(results)}ä¸ªå¯é‡è¯•çš„å¤±è´¥ç»“æœ")

            return results

        except Exception as e:
            logger.error(f"âŒ è·å–å¤±è´¥ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"è·å–å¤±è´¥ç»“æœå¤±è´¥: {e}", e)

    async def delete_by_task(self, task_id: str) -> int:
        """åˆ é™¤ä»»åŠ¡çš„æ‰€æœ‰å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            result = await collection.delete_many({"task_id": task_id})

            logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡å¤„ç†ç»“æœ: {task_id}, åˆ é™¤æ•°é‡: {result.deleted_count}")
            return result.deleted_count

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ é™¤ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}", e)
