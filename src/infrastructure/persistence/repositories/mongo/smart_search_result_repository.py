"""æ™ºèƒ½æœç´¢ç»“æœä»“å‚¨ MongoDB å®ç°

Version: v3.0.0 (æ¨¡å—åŒ–æ¶æ„)

å®ç° ISmartSearchResultRepository æ¥å£ï¼Œæä¾›ï¼š
- æ™ºèƒ½æœç´¢ç»“æœçš„å­˜å‚¨å’ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨ç‹¬ç«‹é›†åˆ smart_search_resultsï¼‰
- æŒ‰å­æŸ¥è¯¢ç´¢å¼•åˆ†ç»„æŸ¥è¯¢
- èšåˆä¼˜å…ˆçº§ç®¡ç†
- ç»“æœçŠ¶æ€ç®¡ç†ï¼ˆv2.1.0ï¼‰
- å¤šç»´åº¦ç»Ÿè®¡åˆ†æ

èŒè´£ï¼š
- æ•°æ®åº“æ“ä½œï¼šMongoDB é›†åˆ smart_search_results
- æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µç®¡ç†
- èšåˆæŸ¥è¯¢ä¼˜åŒ–
- ç»Ÿè®¡åˆ†æ

æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µï¼š
- original_query: åŸå§‹æŸ¥è¯¢
- decomposed_query: åˆ†è§£åçš„å­æŸ¥è¯¢
- decomposition_reasoning: åˆ†è§£ç†ç”±
- query_focus: æŸ¥è¯¢ç„¦ç‚¹
- sub_query_index: å­æŸ¥è¯¢ç´¢å¼•
- aggregation_priority: èšåˆä¼˜å…ˆçº§
- relevance_to_original: å¯¹åŸå§‹æŸ¥è¯¢çš„ç›¸å…³æ€§
"""

from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple

from motor.motor_asyncio import AsyncIOMotorDatabase

from src.core.domain.entities.search_result import SearchResult, ResultStatus
from src.core.domain.entities.smart_search_task import SmartSearchTask
from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.persistence.interfaces.i_smart_search_repository import (
    ISmartSearchResultRepository
)
from src.infrastructure.persistence.interfaces.i_repository import RepositoryException
from src.utils.logger import get_logger

logger = get_logger(__name__)


class MongoSmartSearchResultRepository(ISmartSearchResultRepository):
    """æ™ºèƒ½æœç´¢ç»“æœä»“å‚¨ MongoDB å®ç°

    é›†åˆ: smart_search_results

    ç´¢å¼•å»ºè®®:
    - _id (é»˜è®¤)
    - task_id (æŸ¥è¯¢ä¼˜åŒ–)
    - (task_id, sub_query_index) å¤åˆç´¢å¼•
    - (task_id, status) å¤åˆç´¢å¼•
    - original_query (è·¨ä»»åŠ¡æŸ¥è¯¢)
    - aggregation_priority (æ’åºä¼˜åŒ–)
    - relevance_score (æ’åºä¼˜åŒ–)
    - created_at (æ’åºä¼˜åŒ–)

    v1.5.0 IDç³»ç»Ÿç»Ÿä¸€ï¼š
    - æ‰€æœ‰IDä½¿ç”¨é›ªèŠ±ç®—æ³•å­—ç¬¦ä¸²æ ¼å¼
    - ç§»é™¤UUIDä¾èµ–
    """

    def __init__(self, db: Optional[AsyncIOMotorDatabase] = None):
        """åˆå§‹åŒ–ä»“å‚¨

        Args:
            db: MongoDBæ•°æ®åº“å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è·å–
        """
        self._db = db
        self.collection_name = "smart_search_results"

    async def _get_collection(self):
        """è·å–MongoDBé›†åˆ"""
        if self._db is None:
            self._db = await get_mongodb_database()
        return self._db[self.collection_name]

    def _result_to_dict(
        self,
        result: SearchResult,
        task: Optional[SmartSearchTask] = None,
        sub_query_index: int = 0
    ) -> Dict[str, Any]:
        """å°†SearchResultå®ä½“è½¬æ¢ä¸ºMongoDBæ–‡æ¡£

        Args:
            result: æœç´¢ç»“æœå®ä½“
            task: æ™ºèƒ½æœç´¢ä»»åŠ¡ (ç”¨äºå¡«å……æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µ)
            sub_query_index: å­æŸ¥è¯¢ç´¢å¼•

        Returns:
            MongoDBæ–‡æ¡£å­—å…¸
        """
        doc = {
            "_id": str(result.id),
            "task_id": str(result.task_id),

            # æœç´¢ç»“æœæ ¸å¿ƒæ•°æ®
            "title": result.title,
            "url": result.url,
            "content": result.content,
            "snippet": result.snippet,

            # å…ƒæ•°æ®
            "source": result.source,
            "published_date": result.published_date,
            "author": result.author,
            "language": result.language,

            # Firecrawl ç‰¹å®šå­—æ®µ
            "markdown_content": result.markdown_content,
            "html_content": result.html_content,
            "article_tag": result.article_tag,
            "article_published_time": result.article_published_time,

            # ç²¾ç®€çš„å…ƒæ•°æ®
            "source_url": result.source_url,
            "http_status_code": result.http_status_code,
            "search_position": result.search_position,
            "metadata": result.metadata,

            # è´¨é‡æŒ‡æ ‡
            "relevance_score": result.relevance_score,
            "quality_score": result.quality_score,

            # çŠ¶æ€ä¸æ—¶é—´
            "status": result.status.value,
            "created_at": result.created_at,
            "processed_at": result.processed_at,

            # æµ‹è¯•æ¨¡å¼æ ‡è®°
            "is_test_data": result.is_test_data,
        }

        # æ·»åŠ æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µ
        doc["sub_query_index"] = sub_query_index
        doc["original_query"] = ""
        doc["decomposed_query"] = ""
        doc["decomposition_reasoning"] = ""
        doc["query_focus"] = ""
        doc["relevance_to_original"] = 0.0
        doc["aggregation_priority"] = 0
        doc["sub_search_task_id"] = ""

        # å¦‚æœæä¾›äº†ä»»åŠ¡ä¿¡æ¯ï¼Œå¡«å……æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µ
        if task:
            doc["original_query"] = task.original_query

            # è·å–å¯¹åº”çš„å­æŸ¥è¯¢ä¿¡æ¯
            if sub_query_index < len(task.decomposed_queries):
                sub_query = task.decomposed_queries[sub_query_index]
                doc["decomposed_query"] = sub_query.query
                doc["decomposition_reasoning"] = sub_query.reasoning
                doc["query_focus"] = sub_query.focus

            # è®¾ç½®èšåˆä¼˜å…ˆçº§ (åŸºäºç›¸å…³æ€§åˆ†æ•°)
            doc["aggregation_priority"] = int(result.relevance_score * 100)

        return doc

    def _dict_to_result(self, doc: Dict[str, Any]) -> SearchResult:
        """å°†MongoDBæ–‡æ¡£è½¬æ¢ä¸ºSearchResultå®ä½“

        Args:
            doc: MongoDBæ–‡æ¡£

        Returns:
            æœç´¢ç»“æœå®ä½“
        """
        # v1.5.0: ä¼˜å…ˆä½¿ç”¨idå­—æ®µï¼ˆé›ªèŠ±IDï¼‰ï¼Œfallbackåˆ°_idï¼ˆå‘åå…¼å®¹ï¼‰
        result_id = str(doc.get("id") or doc.get("_id", ""))
        task_id = str(doc.get("task_id", ""))

        return SearchResult(
            id=result_id,
            task_id=task_id,

            # æœç´¢ç»“æœæ ¸å¿ƒæ•°æ®
            title=doc.get("title", ""),
            url=doc.get("url", ""),
            content=doc.get("content", ""),
            snippet=doc.get("snippet"),

            # å…ƒæ•°æ®
            source=doc.get("source", "web"),
            published_date=doc.get("published_date"),
            author=doc.get("author"),
            language=doc.get("language"),

            # Firecrawl ç‰¹å®šå­—æ®µ
            markdown_content=doc.get("markdown_content"),
            html_content=doc.get("html_content"),
            article_tag=doc.get("article_tag"),
            article_published_time=doc.get("article_published_time"),

            # ç²¾ç®€çš„å…ƒæ•°æ®
            source_url=doc.get("source_url"),
            http_status_code=doc.get("http_status_code"),
            search_position=doc.get("search_position"),
            metadata=doc.get("metadata", {}),

            # è´¨é‡æŒ‡æ ‡
            relevance_score=doc.get("relevance_score", 0.0),
            quality_score=doc.get("quality_score", 0.0),

            # çŠ¶æ€ä¸æ—¶é—´
            status=ResultStatus(doc.get("status", "pending")),
            created_at=doc.get("created_at", datetime.utcnow()),
            processed_at=doc.get("processed_at"),

            # æµ‹è¯•æ¨¡å¼æ ‡è®°
            is_test_data=doc.get("is_test_data", False),
        )

    async def save_results(
        self,
        results: List[SearchResult],
        task: SmartSearchTask,
        sub_query_index: int = 0
    ) -> None:
        """æ‰¹é‡ä¿å­˜æœç´¢ç»“æœï¼ˆæ·»åŠ æ™ºèƒ½æœç´¢ç‰¹å®šå­—æ®µï¼‰

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            task: æ™ºèƒ½æœç´¢ä»»åŠ¡
            sub_query_index: å­æŸ¥è¯¢ç´¢å¼•

        Raises:
            RepositoryException: ä¿å­˜å¤±è´¥æ—¶æŠ›å‡º
        """
        if not results:
            return

        try:
            documents = []
            for result in results:
                doc = self._result_to_dict(result, task, sub_query_index)
                documents.append(doc)

            await (await self._get_collection()).insert_many(documents)
            logger.info(
                f"âœ… æ‰¹é‡ä¿å­˜æ™ºèƒ½æœç´¢ç»“æœ: task_id={task.id}, "
                f"sub_query_index={sub_query_index}, count={len(results)}"
            )

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜æ™ºèƒ½æœç´¢ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡ä¿å­˜æ™ºèƒ½æœç´¢ç»“æœå¤±è´¥: {e}", e)

    async def get_results_by_task(
        self,
        task_id: str,
        skip: int = 0,
        limit: int = 50,
        sort_by: str = "aggregation_priority"
    ) -> Tuple[List[SearchResult], int]:
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰æœç´¢ç»“æœ

        Args:
            task_id: ä»»åŠ¡ID
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°
            sort_by: æ’åºå­—æ®µ (aggregation_priority, relevance_score, created_at)

        Returns:
            (ç»“æœåˆ—è¡¨, æ€»æ•°)

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            query = {"task_id": task_id}

            # æ€»æ•°
            total = await (await self._get_collection()).count_documents(query)

            # æ’åºè§„åˆ™
            sort_fields = []
            if sort_by == "aggregation_priority":
                sort_fields = [
                    ("aggregation_priority", -1),
                    ("relevance_score", -1),
                    ("created_at", -1)
                ]
            elif sort_by == "relevance_score":
                sort_fields = [("relevance_score", -1), ("created_at", -1)]
            elif sort_by == "created_at":
                sort_fields = [("created_at", -1)]
            else:
                sort_fields = [("aggregation_priority", -1)]

            # æŸ¥è¯¢
            cursor = (await self._get_collection()).find(query).sort(sort_fields).skip(skip).limit(limit)

            results = []
            async for doc in cursor:
                results.append(self._dict_to_result(doc))

            logger.debug(
                f"ğŸ“‹ æŸ¥è¯¢ä»»åŠ¡ç»“æœ: task_id={task_id}, sort_by={sort_by}, total={total}"
            )
            return results, total

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŸ¥è¯¢ä»»åŠ¡ç»“æœå¤±è´¥: {e}", e)

    async def get_results_by_sub_query(
        self,
        task_id: str,
        sub_query_index: int,
        skip: int = 0,
        limit: int = 50
    ) -> Tuple[List[SearchResult], int]:
        """è·å–ç‰¹å®šå­æŸ¥è¯¢çš„ç»“æœ

        Args:
            task_id: ä»»åŠ¡ID
            sub_query_index: å­æŸ¥è¯¢ç´¢å¼•
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°

        Returns:
            (ç»“æœåˆ—è¡¨, æ€»æ•°)

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            query = {
                "task_id": task_id,
                "sub_query_index": sub_query_index
            }

            # æ€»æ•°
            total = await (await self._get_collection()).count_documents(query)

            # æŸ¥è¯¢
            cursor = (await self._get_collection()).find(query).sort([
                ("relevance_score", -1),
                ("created_at", -1)
            ]).skip(skip).limit(limit)

            results = []
            async for doc in cursor:
                results.append(self._dict_to_result(doc))

            logger.debug(
                f"ğŸ“‹ æŸ¥è¯¢å­æŸ¥è¯¢ç»“æœ: task_id={task_id}, "
                f"sub_query_index={sub_query_index}, total={total}"
            )
            return results, total

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å­æŸ¥è¯¢ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŸ¥è¯¢å­æŸ¥è¯¢ç»“æœå¤±è´¥: {e}", e)

    async def get_top_results(
        self,
        task_id: str,
        limit: int = 10,
        min_relevance_score: float = 0.0
    ) -> List[SearchResult]:
        """è·å–ä»»åŠ¡çš„topç»“æœï¼ˆæŒ‰èšåˆä¼˜å…ˆçº§å’Œç›¸å…³æ€§ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°
            min_relevance_score: æœ€å°ç›¸å…³æ€§åˆ†æ•°é˜ˆå€¼

        Returns:
            ç»“æœåˆ—è¡¨

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            query = {
                "task_id": task_id,
                "relevance_score": {"$gte": min_relevance_score}
            }

            cursor = (await self._get_collection()).find(query).sort([
                ("aggregation_priority", -1),
                ("relevance_score", -1),
                ("quality_score", -1)
            ]).limit(limit)

            results = []
            async for doc in cursor:
                results.append(self._dict_to_result(doc))

            logger.debug(
                f"ğŸ¯ æŸ¥è¯¢topç»“æœ: task_id={task_id}, "
                f"limit={limit}, min_score={min_relevance_score}, count={len(results)}"
            )
            return results

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢topç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"æŸ¥è¯¢topç»“æœå¤±è´¥: {e}", e)

    async def get_results_by_original_query(
        self,
        original_query: str,
        skip: int = 0,
        limit: int = 50
    ) -> Tuple[List[SearchResult], int]:
        """æ ¹æ®åŸå§‹æŸ¥è¯¢è·å–ç»“æœï¼ˆè·¨ä»»åŠ¡æŸ¥è¯¢ï¼‰

        Args:
            original_query: åŸå§‹æŸ¥è¯¢
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°

        Returns:
            (ç»“æœåˆ—è¡¨, æ€»æ•°)

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            query = {"original_query": original_query}

            # æ€»æ•°
            total = await (await self._get_collection()).count_documents(query)

            # æŸ¥è¯¢
            cursor = (await self._get_collection()).find(query).sort([
                ("created_at", -1),
                ("relevance_score", -1)
            ]).skip(skip).limit(limit)

            results = []
            async for doc in cursor:
                results.append(self._dict_to_result(doc))

            logger.debug(
                f"ğŸ“‹ è·¨ä»»åŠ¡æŸ¥è¯¢: original_query={original_query}, total={total}"
            )
            return results, total

        except Exception as e:
            logger.error(f"âŒ è·¨ä»»åŠ¡æŸ¥è¯¢å¤±è´¥: {e}")
            raise RepositoryException(f"è·¨ä»»åŠ¡æŸ¥è¯¢å¤±è´¥: {e}", e)

    async def update_aggregation_priority(
        self,
        result_id: str,
        priority: int
    ) -> bool:
        """æ›´æ–°ç»“æœçš„èšåˆä¼˜å…ˆçº§

        Args:
            result_id: ç»“æœID
            priority: æ–°çš„ä¼˜å…ˆçº§

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ

        Raises:
            RepositoryException: æ›´æ–°å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            result = await (await self._get_collection()).update_one(
                {"_id": result_id},
                {"$set": {"aggregation_priority": priority}}
            )

            logger.debug(f"ğŸ“Š æ›´æ–°èšåˆä¼˜å…ˆçº§: result_id={result_id}, priority={priority}")
            return result.modified_count > 0

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°èšåˆä¼˜å…ˆçº§å¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–°èšåˆä¼˜å…ˆçº§å¤±è´¥: {e}", e)

    async def update_relevance_to_original(
        self,
        result_id: str,
        relevance: float
    ) -> bool:
        """æ›´æ–°ç»“æœå¯¹åŸå§‹æŸ¥è¯¢çš„ç›¸å…³æ€§

        Args:
            result_id: ç»“æœID
            relevance: ç›¸å…³æ€§åˆ†æ•° (0.0-1.0)

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ

        Raises:
            RepositoryException: æ›´æ–°å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            result = await (await self._get_collection()).update_one(
                {"_id": result_id},
                {"$set": {"relevance_to_original": relevance}}
            )

            logger.debug(f"ğŸ“Š æ›´æ–°åŸå§‹ç›¸å…³æ€§: result_id={result_id}, relevance={relevance}")
            return result.modified_count > 0

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°åŸå§‹ç›¸å…³æ€§å¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–°åŸå§‹ç›¸å…³æ€§å¤±è´¥: {e}", e)

    async def delete_results_by_task(self, task_id: str) -> int:
        """åˆ é™¤ä»»åŠ¡çš„æ‰€æœ‰ç»“æœ

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            åˆ é™¤çš„è®°å½•æ•°

        Raises:
            RepositoryException: åˆ é™¤å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            result = await (await self._get_collection()).delete_many({"task_id": task_id})
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡ç»“æœ: task_id={task_id}, count={result.deleted_count}")
            return result.deleted_count

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"åˆ é™¤ä»»åŠ¡ç»“æœå¤±è´¥: {e}", e)

    async def count_results_by_task(self, task_id: str) -> int:
        """ç»Ÿè®¡ä»»åŠ¡çš„ç»“æœæ•°é‡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ç»“æœæ•°é‡

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            count = await (await self._get_collection()).count_documents({"task_id": task_id})
            return count

        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
            raise RepositoryException(f"ç»Ÿè®¡ä»»åŠ¡ç»“æœå¤±è´¥: {e}", e)

    async def get_statistics_by_task(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çš„ç»“æœç»Ÿè®¡ä¿¡æ¯

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Raises:
            RepositoryException: ç»Ÿè®¡å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            pipeline = [
                {"$match": {"task_id": task_id}},
                {"$group": {
                    "_id": "$sub_query_index",
                    "count": {"$sum": 1},
                    "avg_relevance": {"$avg": "$relevance_score"},
                    "avg_quality": {"$avg": "$quality_score"},
                    "max_relevance": {"$max": "$relevance_score"},
                    "min_relevance": {"$min": "$relevance_score"}
                }},
                {"$sort": {"_id": 1}}
            ]

            sub_query_stats = []
            async for doc in (await self._get_collection()).aggregate(pipeline):
                sub_query_stats.append({
                    "sub_query_index": doc["_id"],
                    "count": doc["count"],
                    "avg_relevance_score": round(doc["avg_relevance"], 3),
                    "avg_quality_score": round(doc["avg_quality"], 3),
                    "max_relevance_score": round(doc["max_relevance"], 3),
                    "min_relevance_score": round(doc["min_relevance"], 3)
                })

            # æ€»ä½“ç»Ÿè®¡
            total_count = await self.count_results_by_task(task_id)

            logger.debug(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: task_id={task_id}, total={total_count}")
            return {
                "total_count": total_count,
                "sub_query_statistics": sub_query_stats
            }

        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
            raise RepositoryException(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}", e)

    # ==================== çŠ¶æ€ç®¡ç†æ–¹æ³• (v2.1.0æ–°å¢) ====================

    async def get_results_by_status(
        self,
        task_id: str,
        status: ResultStatus,
        skip: int = 0,
        limit: int = 50
    ) -> Tuple[List[SearchResult], int]:
        """æŒ‰çŠ¶æ€ç­›é€‰æœç´¢ç»“æœ

        Args:
            task_id: ä»»åŠ¡ID
            status: ç»“æœçŠ¶æ€
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°

        Returns:
            (ç»“æœåˆ—è¡¨, æ€»æ•°)

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            query = {
                "task_id": task_id,
                "status": status.value
            }

            # æ€»æ•°
            total = await (await self._get_collection()).count_documents(query)

            # æŸ¥è¯¢
            cursor = (await self._get_collection()).find(query).sort([
                ("created_at", -1),
                ("relevance_score", -1)
            ]).skip(skip).limit(limit)

            results = []
            async for doc in cursor:
                results.append(self._dict_to_result(doc))

            logger.debug(
                f"ğŸ“‹ æŒ‰çŠ¶æ€æŸ¥è¯¢: task_id={task_id}, "
                f"status={status.value}, total={total}"
            )
            return results, total

        except Exception as e:
            logger.error(f"âŒ æŒ‰çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}")
            raise RepositoryException(f"æŒ‰çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}", e)

    async def count_by_status(self, task_id: str) -> Dict[str, int]:
        """ç»Ÿè®¡å„çŠ¶æ€ç»“æœæ•°é‡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            çŠ¶æ€è®¡æ•°å­—å…¸

        Raises:
            RepositoryException: ç»Ÿè®¡å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            pipeline = [
                {"$match": {"task_id": task_id}},
                {"$group": {
                    "_id": "$status",
                    "count": {"$sum": 1}
                }}
            ]

            status_counts = {status.value: 0 for status in ResultStatus}

            async for doc in (await self._get_collection()).aggregate(pipeline):
                status_counts[doc["_id"]] = doc["count"]

            logger.debug(f"ğŸ“Š çŠ¶æ€ç»Ÿè®¡: task_id={task_id}, counts={status_counts}")
            return status_counts

        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}")
            raise RepositoryException(f"çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}", e)

    async def update_result_status(
        self,
        result_id: str,
        new_status: ResultStatus
    ) -> bool:
        """æ›´æ–°å•ä¸ªç»“æœçŠ¶æ€

        Args:
            result_id: ç»“æœID
            new_status: æ–°çŠ¶æ€

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ

        Raises:
            RepositoryException: æ›´æ–°å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            update_data = {
                "status": new_status.value,
                "processed_at": datetime.utcnow()
            }

            result = await (await self._get_collection()).update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            logger.debug(f"ğŸ“ æ›´æ–°ç»“æœçŠ¶æ€: result_id={result_id}, status={new_status.value}")
            return result.modified_count > 0

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç»“æœçŠ¶æ€å¤±è´¥: {e}")
            raise RepositoryException(f"æ›´æ–°ç»“æœçŠ¶æ€å¤±è´¥: {e}", e)

    async def bulk_update_status(
        self,
        result_ids: List[str],
        new_status: ResultStatus
    ) -> int:
        """æ‰¹é‡æ›´æ–°ç»“æœçŠ¶æ€

        Args:
            result_ids: ç»“æœIDåˆ—è¡¨
            new_status: æ–°çŠ¶æ€

        Returns:
            æ›´æ–°çš„è®°å½•æ•°

        Raises:
            RepositoryException: æ›´æ–°å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            update_data = {
                "status": new_status.value,
                "processed_at": datetime.utcnow()
            }

            result = await (await self._get_collection()).update_many(
                {"_id": {"$in": result_ids}},
                {"$set": update_data}
            )

            logger.info(
                f"ğŸ“ æ‰¹é‡æ›´æ–°çŠ¶æ€: count={result.modified_count}, "
                f"status={new_status.value}"
            )
            return result.modified_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
            raise RepositoryException(f"æ‰¹é‡æ›´æ–°çŠ¶æ€å¤±è´¥: {e}", e)

    async def get_status_distribution(self, task_id: str) -> Dict[str, Any]:
        """è·å–çŠ¶æ€åˆ†å¸ƒç»Ÿè®¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            çŠ¶æ€åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯

        Raises:
            RepositoryException: ç»Ÿè®¡å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # è·å–å„çŠ¶æ€è®¡æ•°
            status_counts = await self.count_by_status(task_id)
            total = sum(status_counts.values())

            # è®¡ç®—ç™¾åˆ†æ¯”
            distribution = {}
            for status, count in status_counts.items():
                percentage = (count / total * 100) if total > 0 else 0
                distribution[status] = {
                    "count": count,
                    "percentage": round(percentage, 2)
                }

            logger.debug(f"ğŸ“Š çŠ¶æ€åˆ†å¸ƒ: task_id={task_id}, total={total}")
            return {
                "total": total,
                "distribution": distribution
            }

        except Exception as e:
            logger.error(f"âŒ è·å–çŠ¶æ€åˆ†å¸ƒå¤±è´¥: {e}")
            raise RepositoryException(f"è·å–çŠ¶æ€åˆ†å¸ƒå¤±è´¥: {e}", e)
