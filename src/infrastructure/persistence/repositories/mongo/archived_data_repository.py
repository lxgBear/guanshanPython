"""ArchivedData MongoDB Repository å®ç°

Version: v3.0.0 (æ¨¡å—åŒ–æ¶æ„)

æä¾›å­˜æ¡£æ•°æ®çš„MongoDBæŒä¹…åŒ–å®ç°ã€‚
"""

import asyncio
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from motor.motor_asyncio import AsyncIOMotorDatabase, AsyncIOMotorClientSession

from src.core.domain.entities.archived_data import ArchivedData
from src.infrastructure.persistence.interfaces import IArchivedDataRepository
from src.infrastructure.persistence.exceptions import RepositoryException
from src.utils.logger import get_logger

logger = get_logger(__name__)


class MongoArchivedDataRepository(IArchivedDataRepository):
    """ArchivedData MongoDB Repository å®ç°

    é›†åˆåç§°: data_source_archived_data

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - åˆ›å»ºå­˜æ¡£è®°å½•ï¼ˆconfirmæ—¶è‡ªåŠ¨è§¦å‘ï¼‰
    - æŒ‰æ•°æ®æºæŸ¥è¯¢å’Œåˆ†é¡µ
    - é˜²é‡å¤å­˜æ¡£ï¼ˆåŸå§‹æ•°æ®IDæŸ¥è¯¢ï¼‰
    - ç»Ÿè®¡ä¿¡æ¯ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ã€å†…å®¹å¤§å°ï¼‰
    - çº§è”åˆ é™¤ï¼ˆåˆ é™¤æ•°æ®æºæ—¶æ¸…ç†å­˜æ¡£ï¼‰
    - äº‹åŠ¡æ”¯æŒï¼ˆè·¨é›†åˆåŒæ­¥ï¼‰
    """

    COLLECTION_NAME = "data_source_archived_data"

    def __init__(self, db: AsyncIOMotorDatabase):
        """åˆå§‹åŒ–Repository

        Args:
            db: MongoDBæ•°æ®åº“å®ä¾‹
        """
        self.db = db
        self.collection = db[self.COLLECTION_NAME]

    async def create(
        self,
        entity: ArchivedData,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> ArchivedData:
        """åˆ›å»ºå­˜æ¡£è®°å½•

        Args:
            entity: å­˜æ¡£æ•°æ®å®ä½“
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆé€šå¸¸å¿…é¡»æä¾›ä»¥ä¿è¯äº‹åŠ¡ä¸€è‡´æ€§ï¼‰

        Returns:
            åˆ›å»ºçš„å­˜æ¡£æ•°æ®å®ä½“

        Raises:
            RepositoryException: åˆ›å»ºå¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            doc = self._to_document(entity)
            await self.collection.insert_one(doc, session=session)
            logger.info(
                f"âœ… åˆ›å»ºå­˜æ¡£è®°å½•: {entity.id} "
                f"(æ•°æ®æº: {entity.data_source_id}, ç±»å‹: {entity.data_type})"
            )
            return entity
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå­˜æ¡£è®°å½•å¤±è´¥: {entity.id}, é”™è¯¯: {e}")
            raise RepositoryException(f"åˆ›å»ºå­˜æ¡£è®°å½•å¤±è´¥: {e}")

    async def find_by_id(
        self,
        archived_data_id: str,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> Optional[ArchivedData]:
        """æ ¹æ®IDæŸ¥è¯¢å­˜æ¡£æ•°æ®

        Args:
            archived_data_id: å­˜æ¡£æ•°æ®ID
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“æˆ–None

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            doc = await self.collection.find_one(
                {"id": archived_data_id},
                session=session
            )
            return self._from_document(doc) if doc else None
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å­˜æ¡£æ•°æ®å¤±è´¥: {archived_data_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"æŸ¥è¯¢å­˜æ¡£æ•°æ®å¤±è´¥: {e}")

    async def find_by_data_source(
        self,
        data_source_id: str,
        limit: int = 100,
        skip: int = 0
    ) -> List[ArchivedData]:
        """æŒ‰æ•°æ®æºæŸ¥è¯¢å­˜æ¡£æ•°æ®ï¼ˆåˆ†é¡µï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            limit: æ¯é¡µæ•°é‡
            skip: è·³è¿‡æ•°é‡

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“åˆ—è¡¨

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            cursor = self.collection.find(
                {"data_source_id": data_source_id}
            ).sort("created_at", -1).skip(skip).limit(limit)

            docs = await cursor.to_list(length=limit)
            return [self._from_document(doc) for doc in docs]
        except Exception as e:
            logger.error(f"âŒ æŒ‰æ•°æ®æºæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {data_source_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"æŒ‰æ•°æ®æºæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {e}")

    async def count_by_data_source(
        self,
        data_source_id: str
    ) -> int:
        """ç»Ÿè®¡æ•°æ®æºçš„å­˜æ¡£æ•°é‡

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            å­˜æ¡£è®°å½•æ•°é‡

        Raises:
            RepositoryException: ç»Ÿè®¡å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            return await self.collection.count_documents(
                {"data_source_id": data_source_id}
            )
        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡å­˜æ¡£æ•°é‡å¤±è´¥: {data_source_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"ç»Ÿè®¡å­˜æ¡£æ•°é‡å¤±è´¥: {e}")

    async def find_by_original_data_id(
        self,
        original_data_id: str,
        data_type: str
    ) -> Optional[ArchivedData]:
        """æ ¹æ®åŸå§‹æ•°æ®IDæŸ¥è¯¢å­˜æ¡£ï¼ˆé˜²é‡å¤å­˜æ¡£ï¼‰

        Args:
            original_data_id: åŸå§‹æ•°æ®ID
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“æˆ–None

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            doc = await self.collection.find_one({
                "original_data_id": original_data_id,
                "data_type": data_type
            })
            return self._from_document(doc) if doc else None
        except Exception as e:
            logger.error(
                f"âŒ æ ¹æ®åŸå§‹æ•°æ®IDæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {original_data_id}, é”™è¯¯: {e}"
            )
            raise RepositoryException(f"æ ¹æ®åŸå§‹æ•°æ®IDæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {e}")

    async def delete_by_data_source(
        self,
        data_source_id: str,
        session: Optional[AsyncIOMotorClientSession] = None
    ) -> int:
        """åˆ é™¤æ•°æ®æºçš„æ‰€æœ‰å­˜æ¡£è®°å½•ï¼ˆçº§è”åˆ é™¤ï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            session: MongoDBäº‹åŠ¡ä¼šè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ é™¤çš„è®°å½•æ•°é‡

        Raises:
            RepositoryException: åˆ é™¤å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            result = await self.collection.delete_many(
                {"data_source_id": data_source_id},
                session=session
            )
            deleted_count = result.deleted_count
            logger.info(
                f"ğŸ—‘ï¸  åˆ é™¤æ•°æ®æºå­˜æ¡£: {data_source_id}ï¼Œå…±{deleted_count}æ¡è®°å½•"
            )
            return deleted_count
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ•°æ®æºå­˜æ¡£å¤±è´¥: {data_source_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"åˆ é™¤æ•°æ®æºå­˜æ¡£å¤±è´¥: {e}")

    async def get_statistics(
        self,
        data_source_id: str
    ) -> Dict[str, Any]:
        """è·å–æ•°æ®æºå­˜æ¡£ç»Ÿè®¡ä¿¡æ¯

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Raises:
            RepositoryException: ç»Ÿè®¡å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            pipeline = [
                {"$match": {"data_source_id": data_source_id}},
                {
                    "$group": {
                        "_id": "$data_type",
                        "count": {"$sum": 1},
                        "total_content_size": {"$sum": {"$strLenCP": "$content"}}
                    }
                }
            ]

            results = await self.collection.aggregate(pipeline).to_list(length=None)

            stats = {
                "data_source_id": data_source_id,
                "total_count": 0,
                "scheduled_count": 0,
                "instant_count": 0,
                "total_content_size": 0,
                "by_type": {}
            }

            for result in results:
                data_type = result["_id"]
                count = result["count"]
                content_size = result["total_content_size"]

                stats["total_count"] += count
                stats["total_content_size"] += content_size
                stats["by_type"][data_type] = {
                    "count": count,
                    "content_size": content_size
                }

                if data_type == "scheduled":
                    stats["scheduled_count"] = count
                elif data_type == "instant":
                    stats["instant_count"] = count

            return stats
        except Exception as e:
            logger.error(f"âŒ è·å–å­˜æ¡£ç»Ÿè®¡å¤±è´¥: {data_source_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"è·å–å­˜æ¡£ç»Ÿè®¡å¤±è´¥: {e}")

    async def find_with_pagination(
        self,
        data_source_id: str,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[ArchivedData], int]:
        """åˆ†é¡µæŸ¥è¯¢å­˜æ¡£æ•°æ®ï¼ˆå«æ€»æ•°ï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡

        Returns:
            (å­˜æ¡£æ•°æ®åˆ—è¡¨, æ€»è®°å½•æ•°) å…ƒç»„

        Raises:
            RepositoryException: æŸ¥è¯¢å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            skip = (page - 1) * page_size

            # å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢å’Œè®¡æ•°
            cursor = self.collection.find(
                {"data_source_id": data_source_id}
            ).sort("created_at", -1).skip(skip).limit(page_size)

            docs, total = await asyncio.gather(
                cursor.to_list(length=page_size),
                self.count_by_data_source(data_source_id)
            )

            archived_list = [self._from_document(doc) for doc in docs]
            return archived_list, total
        except Exception as e:
            logger.error(f"âŒ åˆ†é¡µæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {data_source_id}, é”™è¯¯: {e}")
            raise RepositoryException(f"åˆ†é¡µæŸ¥è¯¢å­˜æ¡£å¤±è´¥: {e}")

    # ==========================================
    # æ–‡æ¡£è½¬æ¢æ–¹æ³•
    # ==========================================

    def _to_document(self, archived_data: ArchivedData) -> Dict[str, Any]:
        """å®ä½“è½¬MongoDBæ–‡æ¡£

        Args:
            archived_data: å­˜æ¡£æ•°æ®å®ä½“

        Returns:
            MongoDBæ–‡æ¡£å­—å…¸
        """
        return {
            "id": archived_data.id,
            "data_source_id": archived_data.data_source_id,
            "original_data_id": archived_data.original_data_id,
            "data_type": archived_data.data_type,
            # æ ¸å¿ƒå†…å®¹
            "title": archived_data.title,
            "url": archived_data.url,
            "content": archived_data.content,  # å®Œæ•´å†…å®¹
            "snippet": archived_data.snippet,
            "published_date": archived_data.published_date,
            # Firecrawlå­—æ®µ
            "markdown_content": archived_data.markdown_content,
            "html_content": archived_data.html_content,
            # ç±»å‹ç‰¹å®šå­—æ®µå’Œå…ƒæ•°æ®
            "type_specific_fields": archived_data.type_specific_fields,
            "metadata": archived_data.metadata,
            # å­˜æ¡£å…ƒä¿¡æ¯
            "archived_at": archived_data.archived_at,
            "archived_by": archived_data.archived_by,
            "archived_reason": archived_data.archived_reason,
            # åŸå§‹æ•°æ®è¿½æº¯
            "original_created_at": archived_data.original_created_at,
            "original_status": archived_data.original_status,
            # ç³»ç»Ÿå­—æ®µ
            "created_at": archived_data.created_at,
            "updated_at": archived_data.updated_at,
        }

    def _from_document(self, doc: Dict[str, Any]) -> ArchivedData:
        """MongoDBæ–‡æ¡£è½¬å®ä½“

        Args:
            doc: MongoDBæ–‡æ¡£å­—å…¸

        Returns:
            å­˜æ¡£æ•°æ®å®ä½“
        """
        return ArchivedData(
            id=doc["id"],
            data_source_id=doc.get("data_source_id", ""),
            original_data_id=doc.get("original_data_id", ""),
            data_type=doc.get("data_type", ""),
            # æ ¸å¿ƒå†…å®¹
            title=doc.get("title", ""),
            url=doc.get("url", ""),
            content=doc.get("content", ""),
            snippet=doc.get("snippet"),
            published_date=doc.get("published_date"),
            # Firecrawlå­—æ®µ
            markdown_content=doc.get("markdown_content"),
            html_content=doc.get("html_content"),
            # ç±»å‹ç‰¹å®šå­—æ®µå’Œå…ƒæ•°æ®
            type_specific_fields=doc.get("type_specific_fields", {}),
            metadata=doc.get("metadata", {}),
            # å­˜æ¡£å…ƒä¿¡æ¯
            archived_at=doc.get("archived_at", datetime.utcnow()),
            archived_by=doc.get("archived_by", ""),
            archived_reason=doc.get("archived_reason", "confirm"),
            # åŸå§‹æ•°æ®è¿½æº¯
            original_created_at=doc.get("original_created_at"),
            original_status=doc.get("original_status", ""),
            # ç³»ç»Ÿå­—æ®µ
            created_at=doc.get("created_at", datetime.utcnow()),
            updated_at=doc.get("updated_at", datetime.utcnow()),
        )
