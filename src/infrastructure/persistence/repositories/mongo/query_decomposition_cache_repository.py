"""æŸ¥è¯¢åˆ†è§£ç¼“å­˜ä»“å‚¨ MongoDB å®ç°

Version: v3.0.0 (æ¨¡å—åŒ–æ¶æ„)

å®ç° IQueryDecompositionCacheRepository æ¥å£ï¼Œæä¾›ï¼š
- LLMåˆ†è§£ç»“æœç¼“å­˜ç®¡ç†ï¼ˆé™ä½APIæˆæœ¬ï¼‰
- ç¼“å­˜TTLç®¡ç†ï¼ˆé»˜è®¤24å°æ—¶ï¼‰
- ç¼“å­˜ç»Ÿè®¡å’Œæ¸…ç†åŠŸèƒ½

èŒè´£ï¼š
- æ•°æ®åº“æ“ä½œï¼šMongoDB é›†åˆ query_decomposition_cache
- ç¼“å­˜é”®è®¡ç®—ï¼šMD5(query + search_context)
- å‘½ä¸­ç»Ÿè®¡ç»´æŠ¤
- è¿‡æœŸç¼“å­˜æ¸…ç†

ç¼“å­˜ç­–ç•¥ï¼š
- ç¼“å­˜é”®ï¼šMD5(query + search_context)
- TTLï¼š24å°æ—¶è‡ªåŠ¨è¿‡æœŸ
- å‘½ä¸­ç»Ÿè®¡ï¼šè®°å½•ç¼“å­˜ä½¿ç”¨æ¬¡æ•°å’Œæœ€åä½¿ç”¨æ—¶é—´
- Upsertæ“ä½œï¼šå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥

æ³¨æ„ï¼š
- ç¼“å­˜å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹
- æ‰€æœ‰æ–¹æ³•åº”æ•è·å¼‚å¸¸å¹¶è¿”å›é»˜è®¤å€¼
"""

import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any

from src.core.domain.entities.query_decomposition import QueryDecomposition
from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.persistence.interfaces.i_smart_search_repository import (
    IQueryDecompositionCacheRepository
)
from src.utils.logger import get_logger

logger = get_logger(__name__)


class MongoQueryDecompositionCacheRepository(IQueryDecompositionCacheRepository):
    """æŸ¥è¯¢åˆ†è§£ç¼“å­˜ä»“å‚¨ MongoDB å®ç°

    é›†åˆ: query_decomposition_cache

    ç´¢å¼•å»ºè®®:
    - query_hash (å”¯ä¸€ç´¢å¼•)
    - expires_at (TTLç´¢å¼•ï¼Œè‡ªåŠ¨åˆ é™¤è¿‡æœŸæ–‡æ¡£)
    - last_used_at (æŸ¥è¯¢ä¼˜åŒ–)
    - hit_count (ç»Ÿè®¡ä¼˜åŒ–)

    ç¼“å­˜æ–‡æ¡£ç»“æ„:
    {
        "query_hash": "md5_hash_string",
        "original_query": "åŸå§‹æŸ¥è¯¢",
        "search_context": {...},
        "decomposition_result": {...},
        "llm_model": "gpt-4",
        "tokens_used": 1500,
        "hit_count": 5,
        "first_created_at": datetime,
        "last_used_at": datetime,
        "expires_at": datetime,
        "created_at": datetime,
        "updated_at": datetime
    }
    """

    def __init__(self):
        self.collection_name = "query_decomposition_cache"

    async def _get_collection(self):
        """è·å–MongoDBé›†åˆ"""
        db = await get_mongodb_database()
        return db[self.collection_name]

    def _calculate_cache_key(self, query: str, context: Dict[str, Any]) -> str:
        """è®¡ç®—ç¼“å­˜é”®

        Args:
            query: åŸå§‹æŸ¥è¯¢
            context: æœç´¢ä¸Šä¸‹æ–‡

        Returns:
            MD5å“ˆå¸Œå­—ç¬¦ä¸²
        """
        # æ„å»ºç¼“å­˜é”®å†…å®¹
        cache_content = f"{query}|{context.get('target_domains', '')}|{context.get('language', '')}|{context.get('time_range', '')}"

        # MD5å“ˆå¸Œ
        return hashlib.md5(cache_content.encode()).hexdigest()

    async def get_cached_decomposition(
        self,
        query: str,
        context: Dict[str, Any]
    ) -> Optional[QueryDecomposition]:
        """è·å–ç¼“å­˜çš„åˆ†è§£ç»“æœ

        Args:
            query: åŸå§‹æŸ¥è¯¢
            context: æœç´¢ä¸Šä¸‹æ–‡

        Returns:
            QueryDecompositionæˆ–None

        ä¸šåŠ¡é€»è¾‘ï¼š
        - è®¡ç®—ç¼“å­˜é”®ï¼šMD5(query + context)
        - æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆexpires_at > nowï¼‰
        - å‘½ä¸­åˆ™æ›´æ–°hit_countå’Œlast_used_at
        - æœªå‘½ä¸­è¿”å›None

        æ³¨æ„ï¼š
        - ç¼“å­˜å¤±è´¥ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›None
        """
        try:
            collection = await self._get_collection()
            query_hash = self._calculate_cache_key(query, context)

            # æŸ¥è¯¢ç¼“å­˜
            data = await collection.find_one({
                "query_hash": query_hash,
                "expires_at": {"$gt": datetime.utcnow()}  # æœªè¿‡æœŸ
            })

            if data:
                # æ›´æ–°å‘½ä¸­æ¬¡æ•°å’Œæœ€åä½¿ç”¨æ—¶é—´
                await collection.update_one(
                    {"_id": data["_id"]},
                    {
                        "$inc": {"hit_count": 1},
                        "$set": {"last_used_at": datetime.utcnow()}
                    }
                )

                logger.info(
                    f"âœ… ç¼“å­˜å‘½ä¸­: query_hash={query_hash}, "
                    f"hit_count={data['hit_count'] + 1}"
                )

                # æ„å»ºQueryDecompositionå¯¹è±¡
                return QueryDecomposition.from_dict(data["decomposition_result"])

            logger.debug(f"ğŸ” ç¼“å­˜æœªå‘½ä¸­: query_hash={query_hash}")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜åˆ†è§£ç»“æœå¤±è´¥: {e}")
            # ç¼“å­˜å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹ï¼Œè¿”å›None
            return None

    async def save_decomposition(
        self,
        query: str,
        context: Dict[str, Any],
        decomposition: QueryDecomposition,
        ttl_hours: int = 24
    ) -> bool:
        """ä¿å­˜åˆ†è§£ç»“æœåˆ°ç¼“å­˜

        Args:
            query: åŸå§‹æŸ¥è¯¢
            context: æœç´¢ä¸Šä¸‹æ–‡
            decomposition: åˆ†è§£ç»“æœ
            ttl_hours: è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ

        ä¸šåŠ¡é€»è¾‘ï¼š
        - è®¡ç®—ç¼“å­˜é”®å’Œè¿‡æœŸæ—¶é—´
        - Upsertæ“ä½œï¼ˆå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰
        - å¤±è´¥ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›False

        æ³¨æ„ï¼š
        - ç¼“å­˜å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹
        """
        try:
            collection = await self._get_collection()
            query_hash = self._calculate_cache_key(query, context)

            now = datetime.utcnow()
            expires_at = now + timedelta(hours=ttl_hours)

            # æ„å»ºç¼“å­˜æ–‡æ¡£
            cache_doc = {
                "query_hash": query_hash,
                "original_query": query,
                "search_context": context,
                "decomposition_result": decomposition.to_dict(),
                "llm_model": decomposition.model,
                "tokens_used": decomposition.tokens_used,
                "hit_count": 0,
                "first_created_at": now,
                "last_used_at": now,
                "expires_at": expires_at,
                "created_at": now,
                "updated_at": now
            }

            # Upsertæ“ä½œï¼ˆå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰
            await collection.update_one(
                {"query_hash": query_hash},
                {"$set": cache_doc},
                upsert=True
            )

            logger.info(
                f"âœ… ä¿å­˜åˆ†è§£ç»“æœåˆ°ç¼“å­˜: query_hash={query_hash}, "
                f"expires_at={expires_at}"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†è§£ç»“æœåˆ°ç¼“å­˜å¤±è´¥: {e}")
            # ç¼“å­˜å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹
            return False

    async def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
            {
                "total_cached": 100,
                "valid_cached": 80,
                "expired_cached": 20,
                "total_hits": 500,
                "avg_hits_per_cache": 5.0,
                "estimated_tokens_saved": 50000,
                "cache_hit_rate": 0.8
            }

        ç”¨é€”ï¼š
        - ç›‘æ§ç¼“å­˜æ•ˆæœ
        - æˆæœ¬èŠ‚çº¦åˆ†æ

        æ³¨æ„ï¼š
        - ç»Ÿè®¡å¤±è´¥è¿”å›ç©ºå­—å…¸
        """
        try:
            collection = await self._get_collection()

            # æ€»ç¼“å­˜æ•°
            total_cached = await collection.count_documents({})

            # æœ‰æ•ˆç¼“å­˜æ•°ï¼ˆæœªè¿‡æœŸï¼‰
            valid_cached = await collection.count_documents({
                "expires_at": {"$gt": datetime.utcnow()}
            })

            # æ€»å‘½ä¸­æ¬¡æ•°å’ŒtokenèŠ‚çº¦
            pipeline = [
                {"$group": {
                    "_id": None,
                    "total_hits": {"$sum": "$hit_count"},
                    "avg_hits": {"$avg": "$hit_count"},
                    "total_tokens_saved": {"$sum": "$tokens_used"}
                }}
            ]

            cursor = collection.aggregate(pipeline)
            stats_result = await cursor.to_list(length=1)

            if stats_result:
                stats = stats_result[0]
            else:
                stats = {
                    "total_hits": 0,
                    "avg_hits": 0.0,
                    "total_tokens_saved": 0
                }

            cache_hit_rate = round(stats["total_hits"] / total_cached, 2) if total_cached > 0 else 0.0

            result = {
                "total_cached": total_cached,
                "valid_cached": valid_cached,
                "expired_cached": total_cached - valid_cached,
                "total_hits": stats["total_hits"],
                "avg_hits_per_cache": round(stats["avg_hits"], 2),
                "estimated_tokens_saved": stats["total_tokens_saved"] * stats["total_hits"],
                "cache_hit_rate": cache_hit_rate
            }

            logger.info(
                f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: total={total_cached}, valid={valid_cached}, "
                f"hit_rate={cache_hit_rate}"
            )

            return result

        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    async def clear_expired_cache(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜

        Returns:
            åˆ é™¤çš„ç¼“å­˜æ•°é‡

        ç”¨é€”ï¼š
        - å®šæœŸæ¸…ç†ï¼ˆå¦‚æ¯æ—¥å‡Œæ™¨ï¼‰
        - é‡Šæ”¾å­˜å‚¨ç©ºé—´

        æ³¨æ„ï¼š
        - æ¸…ç†å¤±è´¥è¿”å›0
        """
        try:
            collection = await self._get_collection()

            result = await collection.delete_many({
                "expires_at": {"$lt": datetime.utcnow()}
            })

            deleted_count = result.deleted_count
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜: åˆ é™¤ {deleted_count} æ¡")

            return deleted_count

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")
            return 0
