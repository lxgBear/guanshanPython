"""æ•°æ®æ•´ç¼–æœåŠ¡

æä¾›æ•°æ®æºç®¡ç†çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®æºCRUDæ“ä½œ
- çŠ¶æ€åŒæ­¥ç®¡ç†ï¼ˆMongoDBäº‹åŠ¡ï¼‰
- åŸå§‹æ•°æ®å¼•ç”¨ç®¡ç†
- æ‰¹é‡æ“ä½œæ”¯æŒ

v1.5.1: äº‹åŠ¡å…¼å®¹æ€§æ”¹è¿›
- æ”¯æŒstandalone MongoDBï¼ˆå¼€å‘ç¯å¢ƒï¼‰
- æ”¯æŒreplica set MongoDBï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from contextlib import asynccontextmanager
from motor.motor_asyncio import AsyncIOMotorDatabase

from src.core.domain.entities.data_source import (
    DataSource,
    DataSourceStatus,
    RawDataReference
)
from src.core.domain.entities.search_result import SearchResult, ResultStatus
from src.core.domain.entities.instant_search_result import InstantSearchResult, InstantSearchResultStatus
from src.core.domain.entities.archived_data import ArchivedData
from src.infrastructure.database.data_source_repositories import DataSourceRepository
from src.infrastructure.database.archived_data_repositories import ArchivedDataRepository
from src.infrastructure.database.connection import is_mongodb_replica_set
from src.utils.logger import get_logger

logger = get_logger(__name__)


class DataCurationService:
    """æ•°æ®æ•´ç¼–æœåŠ¡

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ•°æ®æºç”Ÿå‘½å‘¨æœŸç®¡ç†
    - çŠ¶æ€åŒæ­¥ï¼ˆä½¿ç”¨MongoDBäº‹åŠ¡ï¼‰
    - åŸå§‹æ•°æ®å¼•ç”¨ç®¡ç†
    """

    def __init__(self, db: AsyncIOMotorDatabase):
        """åˆå§‹åŒ–æœåŠ¡

        Args:
            db: MongoDBæ•°æ®åº“å®ä¾‹
        """
        self.db = db
        self.data_source_repo = DataSourceRepository(db)
        self.archived_data_repo = ArchivedDataRepository(db)  # ã€æ–°å¢ã€‘å­˜æ¡£æ•°æ®ä»“å‚¨
        self.search_results_collection = db.search_results
        self.instant_search_results_collection = db.instant_search_results
        self._supports_transactions = None  # ç¼“å­˜äº‹åŠ¡æ”¯æŒæ£€æµ‹ç»“æœ

    @asynccontextmanager
    async def _transaction_context(self):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆå…¼å®¹standaloneå’Œreplica setï¼‰

        v1.5.1: è‡ªåŠ¨æ£€æµ‹MongoDBæ˜¯å¦æ”¯æŒäº‹åŠ¡
        - Replica set/mongos: ä½¿ç”¨äº‹åŠ¡ä¿è¯ACID
        - Standalone: ç›´æ¥æ“ä½œï¼ˆæ— äº‹åŠ¡ä½†ä»ä¿è¯å•æ–‡æ¡£åŸå­æ€§ï¼‰

        Yields:
            session: MongoDB sessionå¯¹è±¡ï¼ˆå¦‚æœæ”¯æŒäº‹åŠ¡ï¼‰ï¼Œå¦åˆ™ä¸ºNone
        """
        # ç¼“å­˜æ£€æµ‹ç»“æœï¼Œé¿å…é‡å¤æ£€æµ‹
        if self._supports_transactions is None:
            self._supports_transactions = await is_mongodb_replica_set()

        if self._supports_transactions:
            # ä½¿ç”¨äº‹åŠ¡
            async with await self.db.client.start_session() as session:
                async with session.start_transaction():
                    yield session
        else:
            # Standaloneæ¨¡å¼ï¼šä¸ä½¿ç”¨äº‹åŠ¡
            # æ³¨æ„ï¼šè¿™é‡Œyield Noneï¼Œrepositoryæ–¹æ³•éœ€è¦èƒ½å¤Ÿå¤„ç†session=None
            logger.debug("âš ï¸  MongoDB standaloneæ¨¡å¼ï¼Œæ“ä½œä¸ä½¿ç”¨äº‹åŠ¡")
            yield None

    # ==========================================
    # æ•°æ®æºåŸºç¡€æ“ä½œ
    # ==========================================

    async def create_data_source(
        self,
        title: str,
        description: str,
        created_by: str,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        primary_category: Optional[str] = None,
        secondary_category: Optional[str] = None,
        tertiary_category: Optional[str] = None,
        custom_tags: Optional[List[str]] = None
    ) -> DataSource:
        """åˆ›å»ºæ•°æ®æºï¼ˆè‰ç¨¿çŠ¶æ€ï¼‰

        Args:
            title: æ•°æ®æºæ ‡é¢˜
            description: æ•°æ®æºæè¿°
            created_by: åˆ›å»ºè€…
            tags: æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            metadata: æ‰©å±•å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            primary_category: ç¬¬ä¸€çº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            secondary_category: ç¬¬äºŒçº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            tertiary_category: ç¬¬ä¸‰çº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            custom_tags: è‡ªå®šä¹‰æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ›å»ºçš„æ•°æ®æºå®ä½“
        """
        data_source = DataSource(
            title=title,
            description=description,
            created_by=created_by,
            updated_by=created_by,
            tags=tags or [],
            metadata=metadata or {},
            primary_category=primary_category,
            secondary_category=secondary_category,
            tertiary_category=tertiary_category,
            custom_tags=custom_tags or []
        )

        await self.data_source_repo.create(data_source)
        logger.info(f"âœ… åˆ›å»ºæ•°æ®æº: {data_source.id} - {title}")

        return data_source

    async def get_data_source(self, data_source_id: str) -> Optional[DataSource]:
        """è·å–æ•°æ®æºè¯¦æƒ…

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            æ•°æ®æºå®ä½“ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return await self.data_source_repo.find_by_id(data_source_id)

    async def list_data_sources(
        self,
        created_by: Optional[str] = None,
        status: Optional[str] = None,
        source_type: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        primary_category: Optional[str] = None,
        secondary_category: Optional[str] = None,
        tertiary_category: Optional[str] = None,
        limit: int = 50,
        skip: int = 0
    ) -> tuple[List[DataSource], int]:
        """åˆ—å‡ºæ•°æ®æºï¼ˆæ”¯æŒè¿‡æ»¤å’Œåˆ†é¡µï¼‰

        Args:
            created_by: åˆ›å»ºè€…è¿‡æ»¤
            status: çŠ¶æ€è¿‡æ»¤
            source_type: æ•°æ®æºç±»å‹è¿‡æ»¤
            start_date: å¼€å§‹æ—¥æœŸè¿‡æ»¤
            end_date: ç»“æŸæ—¥æœŸè¿‡æ»¤
            primary_category: ç¬¬ä¸€çº§åˆ†ç±»è¿‡æ»¤
            secondary_category: ç¬¬äºŒçº§åˆ†ç±»è¿‡æ»¤
            tertiary_category: ç¬¬ä¸‰çº§åˆ†ç±»è¿‡æ»¤
            limit: æ¯é¡µæ•°é‡
            skip: è·³è¿‡æ•°é‡

        Returns:
            (æ•°æ®æºåˆ—è¡¨, æ€»æ•°)
        """
        data_sources = await self.data_source_repo.find_all(
            created_by=created_by,
            status=status,
            source_type=source_type,
            start_date=start_date,
            end_date=end_date,
            primary_category=primary_category,
            secondary_category=secondary_category,
            tertiary_category=tertiary_category,
            limit=limit,
            skip=skip
        )

        total = await self.data_source_repo.count(
            created_by=created_by,
            status=status,
            source_type=source_type,
            start_date=start_date,
            end_date=end_date,
            primary_category=primary_category,
            secondary_category=secondary_category,
            tertiary_category=tertiary_category
        )

        return data_sources, total

    async def update_data_source_info(
        self,
        data_source_id: str,
        title: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
        primary_category: Optional[str] = None,
        secondary_category: Optional[str] = None,
        tertiary_category: Optional[str] = None,
        custom_tags: Optional[List[str]] = None,
        updated_by: str = ""
    ) -> bool:
        """æ›´æ–°æ•°æ®æºåŸºç¡€ä¿¡æ¯ï¼ˆä»…è‰ç¨¿çŠ¶æ€å¯ç¼–è¾‘ï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            title: æ–°æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            description: æ–°æè¿°ï¼ˆå¯é€‰ï¼‰
            tags: æ–°æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            primary_category: ç¬¬ä¸€çº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            secondary_category: ç¬¬äºŒçº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            tertiary_category: ç¬¬ä¸‰çº§åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            custom_tags: è‡ªå®šä¹‰æ ‡ç­¾æ•°ç»„ï¼ˆå¯é€‰ï¼‰
            updated_by: æ›´æ–°è€…

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨æˆ–ä¸å¯ç¼–è¾‘
        """
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_edit():
            raise ValueError(
                f"Cannot edit data source in status '{data_source.status.value}'"
            )

        update_data = {"updated_by": updated_by}

        if title is not None:
            update_data["title"] = title

        if description is not None:
            update_data["description"] = description

        if tags is not None:
            update_data["tags"] = tags

        if primary_category is not None:
            update_data["primary_category"] = primary_category

        if secondary_category is not None:
            update_data["secondary_category"] = secondary_category

        if tertiary_category is not None:
            update_data["tertiary_category"] = tertiary_category

        if custom_tags is not None:
            update_data["custom_tags"] = custom_tags

        return await self.data_source_repo.update(data_source_id, update_data)

    async def update_data_source_content(
        self,
        data_source_id: str,
        edited_content: str,
        updated_by: str
    ) -> bool:
        """æ›´æ–°æ•°æ®æºç¼–è¾‘å†…å®¹ï¼ˆä»…è‰ç¨¿çŠ¶æ€å¯ç¼–è¾‘ï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            edited_content: ç¼–è¾‘å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
            updated_by: æ›´æ–°è€…

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨æˆ–ä¸å¯ç¼–è¾‘
        """
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_edit():
            raise ValueError(
                f"Cannot edit data source in status '{data_source.status.value}'"
            )

        update_data = {
            "edited_content": edited_content,
            "content_version": data_source.content_version + 1,
            "updated_by": updated_by
        }

        return await self.data_source_repo.update(data_source_id, update_data)

    # ==========================================
    # åŸå§‹æ•°æ®ç®¡ç†ï¼ˆå¸¦çŠ¶æ€åŒæ­¥ï¼‰
    # ==========================================

    async def add_raw_data_to_source(
        self,
        data_source_id: str,
        data_id: str,
        data_type: str,
        added_by: str
    ) -> bool:
        """æ·»åŠ åŸå§‹æ•°æ®åˆ°æ•°æ®æº

        v1.5.2: ä¸å†è‡ªåŠ¨ä¿®æ”¹æ•°æ®çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

        Args:
            data_source_id: æ•°æ®æºID
            data_id: åŸå§‹æ•°æ®ID
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰
            added_by: æ·»åŠ è€…

        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨ã€ä¸å¯ç¼–è¾‘æˆ–åŸå§‹æ•°æ®ä¸å­˜åœ¨
        """
        # éªŒè¯æ•°æ®æº
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_edit():
            raise ValueError(
                f"Cannot add data to data source in status '{data_source.status.value}'"
            )

        # v1.5.0: ç»Ÿä¸€é›ªèŠ±IDåï¼Œdata_typeç›´æ¥å†³å®šé›†åˆ
        # v2.0.2: æ”¯æŒä» news_results æŸ¥æ‰¾AIå¤„ç†åçš„æ•°æ®
        if data_type == "scheduled":
            collection = self.search_results_collection
            # é¦–å…ˆåœ¨åŸå§‹æ•°æ®é›†åˆä¸­æŸ¥æ‰¾
            raw_data_doc = await collection.find_one({"_id": data_id})
            # å¦‚æœæœªæ‰¾åˆ°ï¼Œå°è¯•åœ¨AIå¤„ç†ç»“æœé›†åˆä¸­æŸ¥æ‰¾
            if not raw_data_doc:
                news_collection = self.db.news_results
                raw_data_doc = await news_collection.find_one({"_id": data_id})
                if raw_data_doc:
                    logger.info(f"ğŸ“° åœ¨news_resultsä¸­æ‰¾åˆ°AIå¤„ç†åçš„æ•°æ®: {data_id}")
        elif data_type == "instant":
            collection = self.instant_search_results_collection
            raw_data_doc = await collection.find_one({"_id": data_id})
        else:
            raise ValueError(f"Invalid data type: {data_type}")

        if not raw_data_doc:
            # v1.5.0+: æ™ºèƒ½é”™è¯¯æç¤º - æ£€æµ‹UUIDæ ¼å¼å¹¶æä¾›å¸®åŠ©ä¿¡æ¯
            is_uuid_format = "-" in data_id
            if is_uuid_format:
                # UUIDæ ¼å¼çš„IDå¯èƒ½æ˜¯æ—§æ•°æ®æˆ–å‰ç«¯ç¼“å­˜
                logger.warning(
                    f"âš ï¸  æ£€æµ‹åˆ°UUIDæ ¼å¼ID: {data_id}, "
                    f"v1.5.0åç³»ç»Ÿå·²ç»Ÿä¸€ä½¿ç”¨é›ªèŠ±IDæ ¼å¼ã€‚"
                    f"è¯·åˆ·æ–°é¡µé¢è·å–æœ€æ–°æ•°æ®ã€‚"
                )
                raise ValueError(
                    f"æ•°æ® '{data_id}' ä¸å­˜åœ¨ã€‚"
                    f"æ£€æµ‹åˆ°æ—§çš„UUIDæ ¼å¼IDï¼Œç³»ç»Ÿå·²äºv1.5.0ç»Ÿä¸€ä¸ºé›ªèŠ±IDæ ¼å¼ã€‚"
                    f"å¯èƒ½åŸå› ï¼šâ‘ å‰ç«¯ç¼“å­˜çš„æ—§æ•°æ® â‘¡æ•°æ®å·²è¢«åˆ é™¤ã€‚å»ºè®®åˆ·æ–°é¡µé¢é‡æ–°åŠ è½½æ•°æ®ã€‚"
                )
            else:
                raise ValueError(f"Raw data '{data_id}' not found in {data_type} collection")

        # v1.5.2: ç§»é™¤çŠ¶æ€é™åˆ¶ï¼Œå…è®¸ä»»ä½•çŠ¶æ€çš„æ•°æ®æ·»åŠ åˆ°æ•°æ®æº
        current_status = raw_data_doc.get("status", "pending")

        # v1.5.1: ä½¿ç”¨å…¼å®¹çš„äº‹åŠ¡ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒstandaloneå’Œreplica setï¼‰
        async with self._transaction_context() as session:
            try:
                # v1.5.2: ä¸å†ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€
                # 1. æ·»åŠ åˆ°æ•°æ®æºå¼•ç”¨åˆ—è¡¨
                ref = RawDataReference(
                    data_id=data_id,
                    data_type=data_type,
                    title=raw_data_doc.get("title", ""),
                    url=raw_data_doc.get("url", ""),
                    snippet=raw_data_doc.get("snippet", "") or raw_data_doc.get("content", "")[:200],
                    added_by=added_by
                )

                await self.data_source_repo.add_raw_data_ref(
                    data_source_id,
                    ref,
                    session=session
                )

                # 3. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                data_source.add_raw_data(
                    data_id=data_id,
                    data_type=data_type,
                    title=ref.title,
                    url=ref.url,
                    snippet=ref.snippet,
                    added_by=added_by
                )

                await self.data_source_repo.update_statistics(
                    data_source_id,
                    data_source.total_raw_data_count,
                    data_source.scheduled_data_count,
                    data_source.instant_data_count,
                    session=session
                )

                logger.info(
                    f"âœ… æ·»åŠ åŸå§‹æ•°æ®åˆ°æ•°æ®æº: {data_id} ({data_type}) â†’ {data_source_id} "
                    f"(å½“å‰çŠ¶æ€: {current_status})"
                )

                return True

            except Exception as e:
                logger.error(f"âŒ æ·»åŠ åŸå§‹æ•°æ®å¤±è´¥ï¼ˆäº‹åŠ¡å›æ»šï¼‰: {str(e)}")
                raise

    async def remove_raw_data_from_source(
        self,
        data_source_id: str,
        data_id: str,
        data_type: str,
        removed_by: str
    ) -> bool:
        """ä»æ•°æ®æºç§»é™¤åŸå§‹æ•°æ®

        v1.5.2: ä¸å†è‡ªåŠ¨ä¿®æ”¹æ•°æ®çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

        Args:
            data_source_id: æ•°æ®æºID
            data_id: åŸå§‹æ•°æ®ID
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰
            removed_by: ç§»é™¤è€…

        Returns:
            æ˜¯å¦ç§»é™¤æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨æˆ–ä¸å¯ç¼–è¾‘
        """
        # éªŒè¯æ•°æ®æº
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_edit():
            raise ValueError(
                f"Cannot remove data from data source in status '{data_source.status.value}'"
            )

        # v1.5.0: ç»Ÿä¸€é›ªèŠ±IDåï¼Œdata_typeç›´æ¥å†³å®šé›†åˆ
        if data_type == "scheduled":
            collection = self.search_results_collection
        elif data_type == "instant":
            collection = self.instant_search_results_collection
        else:
            raise ValueError(f"Invalid data type: {data_type}")

        # è·å–å½“å‰çŠ¶æ€ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        raw_data_doc = await collection.find_one({"_id": data_id})
        current_status = raw_data_doc.get("status", "pending") if raw_data_doc else "unknown"

        # v1.5.1: ä½¿ç”¨å…¼å®¹çš„äº‹åŠ¡ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒstandaloneå’Œreplica setï¼‰
        async with self._transaction_context() as session:
            try:
                # v1.5.2: ä¸å†ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€
                # 1. ä»æ•°æ®æºå¼•ç”¨åˆ—è¡¨ç§»é™¤
                await self.data_source_repo.remove_raw_data_ref(
                    data_source_id,
                    data_id,
                    session=session
                )

                # 2. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                data_source.remove_raw_data(data_id, removed_by)

                await self.data_source_repo.update_statistics(
                    data_source_id,
                    data_source.total_raw_data_count,
                    data_source.scheduled_data_count,
                    data_source.instant_data_count,
                    session=session
                )

                logger.info(
                    f"âœ… ä»æ•°æ®æºç§»é™¤åŸå§‹æ•°æ®: {data_id} ({data_type}) â† {data_source_id} "
                    f"(å½“å‰çŠ¶æ€: {current_status})"
                )

                return True

            except Exception as e:
                logger.error(f"âŒ ç§»é™¤åŸå§‹æ•°æ®å¤±è´¥ï¼ˆäº‹åŠ¡å›æ»šï¼‰: {str(e)}")
                raise

    # ==========================================
    # æ•°æ®æºçŠ¶æ€ç®¡ç†ï¼ˆå¸¦äº‹åŠ¡ï¼‰
    # ==========================================

    async def confirm_data_source(
        self,
        data_source_id: str,
        confirmed_by: str
    ) -> bool:
        """ç¡®å®šæ•°æ®æº

        çŠ¶æ€è½¬æ¢ï¼šDRAFT â†’ CONFIRMED
        v1.5.2: ä¸å†è‡ªåŠ¨ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

        Args:
            data_source_id: æ•°æ®æºID
            confirmed_by: ç¡®å®šè€…

        Returns:
            æ˜¯å¦ç¡®å®šæˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨æˆ–ä¸å¯ç¡®å®š
        """
        # éªŒè¯æ•°æ®æº
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_confirm():
            raise ValueError(
                f"Cannot confirm data source in status '{data_source.status.value}' "
                f"or with no raw data (count: {data_source.total_raw_data_count})"
            )

        # v1.5.1: ä½¿ç”¨å…¼å®¹çš„äº‹åŠ¡ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒstandaloneå’Œreplica setï¼‰
        async with self._transaction_context() as session:
            try:
                # 1. æ›´æ–°æ•°æ®æºçŠ¶æ€ â†’ CONFIRMED
                data_source.confirm(confirmed_by)

                await self.data_source_repo.update(
                    data_source_id,
                    {
                        "status": data_source.status.value,
                        "confirmed_by": data_source.confirmed_by,
                        "confirmed_at": data_source.confirmed_at,
                        "updated_by": data_source.updated_by
                    },
                    session=session
                )

                # v1.5.2: ä¸å†ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€
                # 2. è·å–åŸå§‹æ•°æ®IDåˆ—è¡¨ï¼ˆç”¨äºå­˜æ¡£ï¼‰
                scheduled_ids = data_source.get_raw_data_ids_by_type("scheduled")
                instant_ids = data_source.get_raw_data_ids_by_type("instant")

                # 3. ã€å­˜æ¡£ã€‘å­˜æ¡£åŸå§‹æ•°æ®åˆ°ç‹¬ç«‹è¡¨
                archived_count = 0

                # 3.1 å­˜æ¡£scheduledç±»å‹æ•°æ®
                for data_id in scheduled_ids:
                    try:
                        raw_doc = await self.search_results_collection.find_one(
                            {"_id": data_id},
                            session=session
                        )

                        if raw_doc:
                            search_result = self._doc_to_search_result(raw_doc)
                            archived_data = ArchivedData.from_search_result(
                                search_result=search_result,
                                data_source_id=data_source_id,
                                archived_by=confirmed_by,
                                archived_reason="confirm"
                            )
                            await self.archived_data_repo.create(archived_data, session=session)
                            archived_count += 1
                        else:
                            logger.warning(f"âš ï¸  åŸå§‹æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡å­˜æ¡£: {data_id} (scheduled)")

                    except Exception as e:
                        logger.error(f"âŒ å­˜æ¡£scheduledæ•°æ®å¤±è´¥: {data_id}, é”™è¯¯: {str(e)}")
                        # ç»§ç»­å¤„ç†å…¶ä»–æ•°æ®ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹

                # 3.2 å­˜æ¡£instantç±»å‹æ•°æ®
                for data_id in instant_ids:
                    try:
                        raw_doc = await self.instant_search_results_collection.find_one(
                            {"_id": data_id},
                            session=session
                        )

                        if raw_doc:
                            instant_result = self._doc_to_instant_search_result(raw_doc)
                            archived_data = ArchivedData.from_instant_search_result(
                                instant_result=instant_result,
                                data_source_id=data_source_id,
                                archived_by=confirmed_by,
                                archived_reason="confirm"
                            )
                            await self.archived_data_repo.create(archived_data, session=session)
                            archived_count += 1
                        else:
                            logger.warning(f"âš ï¸  åŸå§‹æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡å­˜æ¡£: {data_id} (instant)")

                    except Exception as e:
                        logger.error(f"âŒ å­˜æ¡£instantæ•°æ®å¤±è´¥: {data_id}, é”™è¯¯: {str(e)}")
                        # ç»§ç»­å¤„ç†å…¶ä»–æ•°æ®ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹

                logger.info(
                    f"âœ… ç¡®å®šæ•°æ®æº: {data_source_id} "
                    f"(å­˜æ¡£äº† {archived_count} æ¡æ•°æ®: "
                    f"{len(scheduled_ids)} æ¡scheduled + {len(instant_ids)} æ¡instant)"
                )

                return True

            except Exception as e:
                logger.error(f"âŒ ç¡®å®šæ•°æ®æºå¤±è´¥ï¼ˆäº‹åŠ¡å›æ»šï¼‰: {str(e)}")
                raise

    async def revert_data_source_to_draft(
        self,
        data_source_id: str,
        reverted_by: str
    ) -> bool:
        """æ¢å¤æ•°æ®æºä¸ºè‰ç¨¿

        çŠ¶æ€è½¬æ¢ï¼šCONFIRMED â†’ DRAFT
        v1.5.2: ä¸å†è‡ªåŠ¨ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

        Args:
            data_source_id: æ•°æ®æºID
            reverted_by: æ“ä½œè€…

        Returns:
            æ˜¯å¦æ¢å¤æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨æˆ–ä¸å¯æ¢å¤
        """
        # éªŒè¯æ•°æ®æº
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        if not data_source.can_revert_to_draft():
            raise ValueError(
                f"Cannot revert data source in status '{data_source.status.value}' to draft"
            )

        # v1.5.1: ä½¿ç”¨å…¼å®¹çš„äº‹åŠ¡ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒstandaloneå’Œreplica setï¼‰
        async with self._transaction_context() as session:
            try:
                # 1. æ›´æ–°æ•°æ®æºçŠ¶æ€ â†’ DRAFT
                data_source.revert_to_draft(reverted_by)

                await self.data_source_repo.update(
                    data_source_id,
                    {
                        "status": data_source.status.value,
                        "confirmed_by": None,
                        "confirmed_at": None,
                        "updated_by": data_source.updated_by
                    },
                    session=session
                )

                # v1.5.2: ä¸å†ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€
                # 2. è·å–æ•°æ®ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                scheduled_ids = data_source.get_raw_data_ids_by_type("scheduled")
                instant_ids = data_source.get_raw_data_ids_by_type("instant")

                logger.info(
                    f"âœ… æ¢å¤æ•°æ®æºä¸ºè‰ç¨¿: {data_source_id} "
                    f"(åŒ…å« {len(scheduled_ids)} æ¡scheduledæ•°æ®, "
                    f"{len(instant_ids)} æ¡instantæ•°æ®)"
                )

                return True

            except Exception as e:
                logger.error(f"âŒ æ¢å¤æ•°æ®æºå¤±è´¥ï¼ˆäº‹åŠ¡å›æ»šï¼‰: {str(e)}")
                raise

    async def delete_data_source(
        self,
        data_source_id: str,
        deleted_by: str
    ) -> bool:
        """åˆ é™¤æ•°æ®æº

        v1.5.2: ä¸å†è‡ªåŠ¨ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€ï¼ŒçŠ¶æ€ç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

        Args:
            data_source_id: æ•°æ®æºID
            deleted_by: åˆ é™¤è€…

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ

        Raises:
            ValueError: å¦‚æœæ•°æ®æºä¸å­˜åœ¨
        """
        # éªŒè¯æ•°æ®æº
        data_source = await self.get_data_source(data_source_id)
        if not data_source:
            raise ValueError(f"Data source '{data_source_id}' not found")

        # v1.5.1: ä½¿ç”¨å…¼å®¹çš„äº‹åŠ¡ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒstandaloneå’Œreplica setï¼‰
        async with self._transaction_context() as session:
            try:
                # v1.5.2: ä¸å†ä¿®æ”¹åŸå§‹æ•°æ®çŠ¶æ€
                # è·å–æ•°æ®ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                scheduled_ids = data_source.get_raw_data_ids_by_type("scheduled")
                instant_ids = data_source.get_raw_data_ids_by_type("instant")

                # åˆ é™¤æ•°æ®æº
                await self.data_source_repo.delete(data_source_id, session=session)

                logger.info(
                    f"âœ… åˆ é™¤æ•°æ®æº: {data_source_id} "
                    f"(çŠ¶æ€: {data_source.status.value}, "
                    f"åŒ…å« {len(scheduled_ids)} æ¡scheduled + {len(instant_ids)} æ¡instantæ•°æ®)"
                )

                return True

            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æ•°æ®æºå¤±è´¥ï¼ˆäº‹åŠ¡å›æ»šï¼‰: {str(e)}")
                raise

    # ==========================================
    # æ‰¹é‡æ“ä½œ
    # ==========================================

    async def batch_archive_raw_data(
        self,
        data_ids: List[str],
        data_type: str,
        updated_by: str
    ) -> Dict[str, Any]:
        """æ‰¹é‡ç•™å­˜åŸå§‹æ•°æ®

        çŠ¶æ€æ›´æ–°ï¼šä»»æ„çŠ¶æ€ â†’ archived

        Args:
            data_ids: åŸå§‹æ•°æ®IDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰
            updated_by: æ›´æ–°è€…

        Returns:
            æ“ä½œç»“æœç»Ÿè®¡
        """
        if data_type == "scheduled":
            collection = self.search_results_collection
        elif data_type == "instant":
            collection = self.instant_search_results_collection
        else:
            raise ValueError(f"Invalid data type: {data_type}")

        result = await collection.update_many(
            {"_id": {"$in": data_ids}},
            {
                "$set": {
                    "status": "archived",
                    "updated_at": datetime.utcnow()
                }
            }
        )

        logger.info(
            f"âœ… æ‰¹é‡ç•™å­˜åŸå§‹æ•°æ®: {result.modified_count}/{len(data_ids)} æ¡ ({data_type})"
        )

        return {
            "total": len(data_ids),
            "updated": result.modified_count,
            "status": "archived"
        }

    async def batch_delete_raw_data(
        self,
        data_ids: List[str],
        data_type: str,
        deleted_by: str
    ) -> Dict[str, Any]:
        """æ‰¹é‡è½¯åˆ é™¤åŸå§‹æ•°æ®

        çŠ¶æ€æ›´æ–°ï¼šä»»æ„çŠ¶æ€ â†’ deleted

        Args:
            data_ids: åŸå§‹æ•°æ®IDåˆ—è¡¨
            data_type: æ•°æ®ç±»å‹ï¼ˆscheduledæˆ–instantï¼‰
            deleted_by: åˆ é™¤è€…

        Returns:
            æ“ä½œç»“æœç»Ÿè®¡
        """
        if data_type == "scheduled":
            collection = self.search_results_collection
        elif data_type == "instant":
            collection = self.instant_search_results_collection
        else:
            raise ValueError(f"Invalid data type: {data_type}")

        result = await collection.update_many(
            {"_id": {"$in": data_ids}},
            {
                "$set": {
                    "status": "deleted",
                    "updated_at": datetime.utcnow()
                }
            }
        )

        logger.info(
            f"âœ… æ‰¹é‡åˆ é™¤åŸå§‹æ•°æ®: {result.modified_count}/{len(data_ids)} æ¡ ({data_type})"
        )

        return {
            "total": len(data_ids),
            "updated": result.modified_count,
            "status": "deleted"
        }

    # ==========================================
    # å­˜æ¡£æ•°æ®æŸ¥è¯¢
    # ==========================================

    async def get_archived_data(
        self,
        data_source_id: str,
        page: int = 1,
        page_size: int = 50
    ) -> tuple[List[ArchivedData], int]:
        """è·å–æ•°æ®æºçš„å­˜æ¡£æ•°æ®ï¼ˆåˆ†é¡µï¼‰

        Args:
            data_source_id: æ•°æ®æºID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡

        Returns:
            (å­˜æ¡£æ•°æ®åˆ—è¡¨, æ€»æ•°) å…ƒç»„
        """
        archived_list, total = await self.archived_data_repo.find_with_pagination(
            data_source_id=data_source_id,
            page=page,
            page_size=page_size
        )

        logger.info(
            f"ğŸ“Š æŸ¥è¯¢å­˜æ¡£æ•°æ®: æ•°æ®æº={data_source_id}, "
            f"é¡µç ={page}/{((total + page_size - 1) // page_size)}, "
            f"è¿”å›={len(archived_list)}æ¡, æ€»æ•°={total}æ¡"
        )

        return archived_list, total

    async def get_archived_data_statistics(
        self,
        data_source_id: str
    ) -> Dict[str, Any]:
        """è·å–æ•°æ®æºçš„å­˜æ¡£ç»Ÿè®¡ä¿¡æ¯

        Args:
            data_source_id: æ•°æ®æºID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = await self.archived_data_repo.get_statistics(data_source_id)

        logger.info(
            f"ğŸ“Š æŸ¥è¯¢å­˜æ¡£ç»Ÿè®¡: æ•°æ®æº={data_source_id}, "
            f"æ€»æ•°={stats.get('total_count', 0)}æ¡, "
            f"scheduled={stats.get('scheduled_count', 0)}æ¡, "
            f"instant={stats.get('instant_count', 0)}æ¡"
        )

        return stats

    # ==========================================
    # å†…éƒ¨è¾…åŠ©æ–¹æ³•
    # ==========================================

    def _doc_to_search_result(self, doc: Dict[str, Any]) -> SearchResult:
        """å°†MongoDBæ–‡æ¡£è½¬æ¢ä¸ºSearchResultå®ä½“

        v1.5.0: ç»Ÿä¸€é›ªèŠ±IDå¤„ç†

        Args:
            doc: MongoDBæ–‡æ¡£ï¼ˆæ¥è‡ªsearch_resultsé›†åˆï¼‰

        Returns:
            SearchResultå®ä½“
        """
        # v1.5.0: æ‰€æœ‰IDéƒ½æ˜¯é›ªèŠ±æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
        doc_id = doc.get("id", "")
        task_id = doc.get("task_id", "")

        # è½¬æ¢æ—¥æœŸå­—æ®µ
        published_date = doc.get("published_date")
        if isinstance(published_date, str):
            published_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))

        created_at = doc.get("created_at", datetime.utcnow())
        if isinstance(created_at, str):
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))

        processed_at = doc.get("processed_at")
        if isinstance(processed_at, str):
            processed_at = datetime.fromisoformat(processed_at.replace('Z', '+00:00'))

        # è½¬æ¢çŠ¶æ€æšä¸¾
        status_value = doc.get("status", "pending")
        try:
            status = ResultStatus(status_value)
        except ValueError:
            status = ResultStatus.PENDING

        return SearchResult(
            id=doc_id,  # v1.5.0: ç›´æ¥ä½¿ç”¨é›ªèŠ±IDï¼ˆå­—ç¬¦ä¸²ï¼‰
            task_id=task_id,  # v1.5.0: ç›´æ¥ä½¿ç”¨é›ªèŠ±IDï¼ˆå­—ç¬¦ä¸²ï¼‰
            # æ ¸å¿ƒå­—æ®µ
            title=doc.get("title", ""),
            url=doc.get("url", ""),
            content=doc.get("content", ""),
            snippet=doc.get("snippet"),
            # å…ƒæ•°æ®
            source=doc.get("source", "web"),
            published_date=published_date,
            author=doc.get("author"),
            language=doc.get("language"),
            # Firecrawlå­—æ®µ
            markdown_content=doc.get("markdown_content"),
            html_content=doc.get("html_content"),
            article_tag=doc.get("article_tag"),
            article_published_time=doc.get("article_published_time"),
            source_url=doc.get("source_url"),
            http_status_code=doc.get("http_status_code"),
            search_position=doc.get("search_position"),
            metadata=doc.get("metadata", {}),
            # è´¨é‡æŒ‡æ ‡
            relevance_score=doc.get("relevance_score", 0.0),
            quality_score=doc.get("quality_score", 0.0),
            # çŠ¶æ€
            status=status,
            created_at=created_at,
            processed_at=processed_at,
            # æµ‹è¯•æ ‡è®°
            is_test_data=doc.get("is_test_data", False)
        )

    def _doc_to_instant_search_result(self, doc: Dict[str, Any]) -> InstantSearchResult:
        """å°†MongoDBæ–‡æ¡£è½¬æ¢ä¸ºInstantSearchResultå®ä½“

        Args:
            doc: MongoDBæ–‡æ¡£ï¼ˆæ¥è‡ªinstant_search_resultsé›†åˆï¼‰

        Returns:
            InstantSearchResultå®ä½“
        """
        # è½¬æ¢æ—¥æœŸå­—æ®µ
        published_date = doc.get("published_date")
        if isinstance(published_date, str):
            published_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))

        first_found_at = doc.get("first_found_at", datetime.utcnow())
        if isinstance(first_found_at, str):
            first_found_at = datetime.fromisoformat(first_found_at.replace('Z', '+00:00'))

        last_found_at = doc.get("last_found_at", datetime.utcnow())
        if isinstance(last_found_at, str):
            last_found_at = datetime.fromisoformat(last_found_at.replace('Z', '+00:00'))

        created_at = doc.get("created_at", datetime.utcnow())
        if isinstance(created_at, str):
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))

        updated_at = doc.get("updated_at", datetime.utcnow())
        if isinstance(updated_at, str):
            updated_at = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))

        # è½¬æ¢çŠ¶æ€æšä¸¾
        status_value = doc.get("status", "pending")
        try:
            status = InstantSearchResultStatus(status_value)
        except ValueError:
            status = InstantSearchResultStatus.PENDING

        return InstantSearchResult(
            id=doc.get("id", ""),
            task_id=doc.get("task_id", ""),
            # æ ¸å¿ƒå­—æ®µ
            title=doc.get("title", ""),
            url=doc.get("url", ""),
            content=doc.get("content", ""),
            snippet=doc.get("snippet"),
            # å»é‡å’Œè§„èŒƒåŒ–
            content_hash=doc.get("content_hash", ""),
            url_normalized=doc.get("url_normalized", ""),
            # Firecrawlå­—æ®µ
            markdown_content=doc.get("markdown_content"),
            html_content=doc.get("html_content"),
            # å…ƒæ•°æ®
            source=doc.get("source", "web"),
            published_date=published_date,
            author=doc.get("author"),
            language=doc.get("language"),
            metadata=doc.get("metadata", {}),
            # è´¨é‡æŒ‡æ ‡
            relevance_score=doc.get("relevance_score", 0.0),
            quality_score=doc.get("quality_score", 0.0),
            # çŠ¶æ€
            status=status,
            # å‘ç°ç»Ÿè®¡
            first_found_at=first_found_at,
            last_found_at=last_found_at,
            found_count=doc.get("found_count", 1),
            unique_searches=doc.get("unique_searches", 1),
            # æ—¶é—´æˆ³
            created_at=created_at,
            updated_at=updated_at
        )
