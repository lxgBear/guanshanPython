"""
å•é¡µé¢çˆ¬å–æ‰§è¡Œå™¨

ä½¿ç”¨ Firecrawl Scrape API çˆ¬å–å•ä¸ªé¡µé¢çš„å†…å®¹
"""

from datetime import datetime
from typing import Optional, Dict, Any

from src.core.domain.entities.search_task import SearchTask
from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter

from ..base import TaskExecutor, ConfigValidationError, ExecutionError
from ..config import ScrapeConfig, ConfigFactory
from ..credits_calculator import FirecrawlCreditsCalculator


class ScrapeExecutor(TaskExecutor):
    """å•é¡µé¢çˆ¬å–ä»»åŠ¡æ‰§è¡Œå™¨

    é€‚ç”¨äºå®šæœŸçˆ¬å–ç‰¹å®šé¡µé¢å†…å®¹çš„åœºæ™¯
    """

    def __init__(self):
        super().__init__()
        self.scrape_adapter = FirecrawlAdapter()

    def _extract_metadata_fields(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ä»çˆ¬å–ç»“æœçš„metadataä¸­æå–ç»“æ„åŒ–å­—æ®µ

        ä¸FirecrawlSearchAdapterä¿æŒä¸€è‡´çš„å­—æ®µæå–é€»è¾‘

        Args:
            metadata: CrawlResult.metadataå­—å…¸

        Returns:
            åŒ…å«æå–å­—æ®µçš„å­—å…¸
        """
        extracted = {}

        # 1. æå–ä½œè€…
        extracted['author'] = metadata.get('author')

        # 2. æå–è¯­è¨€
        extracted['language'] = metadata.get('language')

        # 3. æå–æ–‡ç« æ ‡ç­¾ï¼ˆå¤„ç†åˆ—è¡¨æ ¼å¼ï¼‰
        article_tag_raw = metadata.get('article:tag')
        if isinstance(article_tag_raw, list):
            extracted['article_tag'] = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
        else:
            extracted['article_tag'] = article_tag_raw

        # 4. æå–æ–‡ç« å‘å¸ƒæ—¶é—´
        extracted['article_published_time'] = metadata.get('article:published_time')

        # 5. æå–æºURLï¼ˆé‡å®šå‘åœºæ™¯ï¼‰
        extracted['source_url'] = metadata.get('sourceURL')

        # 6. æå–HTTPçŠ¶æ€ç 
        extracted['http_status_code'] = metadata.get('statusCode')

        # 7. è§£æå‘å¸ƒæ—¥æœŸï¼ˆä»metadataæˆ–é¡¶å±‚ï¼‰
        published_date = None
        published_date_str = metadata.get('publishedDate') or metadata.get('published_date')
        if published_date_str:
            try:
                published_date = datetime.fromisoformat(published_date_str)
            except:
                self.logger.debug(f"æ— æ³•è§£æå‘å¸ƒæ—¥æœŸ: {published_date_str}")
        extracted['published_date'] = published_date

        return extracted

    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if not task.crawl_url:
            self.logger.error("å•é¡µé¢çˆ¬å–ä»»åŠ¡å¿…é¡»æä¾› crawl_url å‚æ•°")
            return False

        if not task.crawl_url.startswith(('http://', 'https://')):
            self.logger.error(f"crawl_url æ ¼å¼æ— æ•ˆ: {task.crawl_url}")
            return False

        return True

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œå•é¡µé¢çˆ¬å–ä»»åŠ¡

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            SearchResultBatch: çˆ¬å–ç»“æœæ‰¹æ¬¡

        Raises:
            ConfigValidationError: é…ç½®éªŒè¯å¤±è´¥
            ExecutionError: æ‰§è¡Œè¿‡ç¨‹é”™è¯¯
        """
        start_time = datetime.utcnow()
        self._log_execution_start(task)

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_scrape_config(task.search_config)

        try:
            # 3. æ‰§è¡Œçˆ¬å–
            self.logger.info(f"ğŸŒ çˆ¬å–é¡µé¢: {task.crawl_url}")

            scrape_options = {
                "only_main_content": config.only_main_content,
                "wait_for": config.wait_for,
                "include_tags": config.include_tags,
                "exclude_tags": config.exclude_tags,
                "timeout": config.timeout
            }

            crawl_result = await self.scrape_adapter.scrape(
                task.crawl_url,
                **scrape_options
            )

            # 4. æå–å…ƒæ•°æ®å­—æ®µï¼ˆä¸SearchExecutorä¿æŒä¸€è‡´ï¼‰
            metadata_fields = self._extract_metadata_fields(crawl_result.metadata or {})

            # 5. è½¬æ¢ä¸º SearchResultï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«å®Œæ•´å…ƒæ•°æ®å­—æ®µï¼‰
            search_result = SearchResult(
                task_id=str(task.id),
                title=crawl_result.metadata.get("title", task.crawl_url),
                url=crawl_result.url,
                snippet=(crawl_result.content[:200] if crawl_result.content else ""),
                source="scrape",
                # æ–°å¢å­—æ®µï¼šä»metadataæå–
                published_date=metadata_fields.get('published_date'),
                author=metadata_fields.get('author'),
                language=metadata_fields.get('language'),
                article_tag=metadata_fields.get('article_tag'),
                article_published_time=metadata_fields.get('article_published_time'),
                source_url=metadata_fields.get('source_url'),
                http_status_code=metadata_fields.get('http_status_code'),
                search_position=1,  # URLçˆ¬å–å›ºå®šä¸ºä½ç½®1
                # å†…å®¹å­—æ®µ
                markdown_content=(
                    crawl_result.markdown if crawl_result.markdown
                    else crawl_result.content
                ),
                html_content=crawl_result.html,
                metadata={},  # v2.1.0: ä¸å†ä¼ é€’metadataï¼Œæ‰€æœ‰å­—æ®µå·²æå–ä¸ºç‹¬ç«‹å­—æ®µ
                relevance_score=1.0,  # ç›´æ¥çˆ¬å–çš„é¡µé¢ç›¸å…³æ€§ä¸º100%
                status=ResultStatus.PENDING
            )
            self.logger.info(f"âœ… å·²æå–å…ƒæ•°æ®å­—æ®µ: author={metadata_fields.get('author')}, language={metadata_fields.get('language')}")

            # 6. åˆ›å»ºç»“æœæ‰¹æ¬¡
            batch = self._create_result_batch(
                task,
                query=f"é¡µé¢çˆ¬å–: {task.crawl_url}"
            )
            batch.add_result(search_result)
            batch.total_count = 1

            # è®¡ç®—å®é™…ç§¯åˆ†æ¶ˆè€—
            batch.credits_used = FirecrawlCreditsCalculator.calculate_actual_credits(
                operation="scrape",
                urls_scraped=1
            )
            self.logger.info(f"ğŸ’° ç§¯åˆ†æ¶ˆè€—: {batch.credits_used}")

            # 7. è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.utcnow()
            batch.execution_time_ms = int(
                (end_time - start_time).total_seconds() * 1000
            )

            self._log_execution_end(task, 1, batch.execution_time_ms)

            return batch

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œçˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            raise ExecutionError(f"çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
