"""
å…³é”®è¯æœç´¢æ‰§è¡Œå™¨

å®ç°ä¸¤é˜¶æ®µå¤„ç†æµç¨‹ï¼š
1. Search API è·å–æœç´¢ç»“æœï¼ˆæ ‡é¢˜ã€URLã€æ‘˜è¦ï¼‰
2. Scrape API æ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µå†…å®¹ï¼ˆå®Œæ•´æ­£æ–‡ï¼‰
"""

import asyncio
import re
from datetime import datetime
from typing import List, Optional

from src.core.domain.entities.search_task import SearchTask
from src.core.domain.entities.search_result import SearchResult, SearchResultBatch
from src.core.domain.entities.search_config import UserSearchConfig
from src.infrastructure.search.firecrawl_search_adapter import FirecrawlSearchAdapter
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter

from ..base import TaskExecutor, ConfigValidationError, ExecutionError
from ..config import SearchConfig, ConfigFactory
from ..credits_calculator import FirecrawlCreditsCalculator


class SearchExecutor(TaskExecutor):
    """å…³é”®è¯æœç´¢ä»»åŠ¡æ‰§è¡Œå™¨

    å·¥ä½œæµç¨‹ï¼š
    1. é˜¶æ®µ1ï¼šä½¿ç”¨ Search API è·å–æœç´¢ç»“æœ
    2. é˜¶æ®µ2ï¼šå¯¹æ¯ä¸ªç»“æœä½¿ç”¨ Scrape API çˆ¬å–è¯¦æƒ…é¡µ
    3. åˆå¹¶ç»“æœè¿”å›
    """

    def __init__(self):
        super().__init__()
        self.search_adapter = FirecrawlSearchAdapter()
        self.scrape_adapter = FirecrawlAdapter()

    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if not task.query:
            self.logger.error("å…³é”®è¯æœç´¢ä»»åŠ¡å¿…é¡»æä¾› query å‚æ•°")
            return False

        if not task.query.strip():
            self.logger.error("query å‚æ•°ä¸èƒ½ä¸ºç©º")
            return False

        return True

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œå…³é”®è¯æœç´¢ä»»åŠ¡

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            SearchResultBatch: åŒ…å«è¯¦æƒ…é¡µå†…å®¹çš„æœç´¢ç»“æœæ‰¹æ¬¡

        Raises:
            ConfigValidationError: é…ç½®éªŒè¯å¤±è´¥
            ExecutionError: æ‰§è¡Œè¿‡ç¨‹é”™è¯¯
        """
        start_time = datetime.utcnow()
        self._log_execution_start(task)

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_search_config(task.search_config)

        try:
            # 3. é˜¶æ®µ1ï¼šæ‰§è¡Œæœç´¢
            search_batch = await self._execute_search(task, config)

            if not search_batch.results:
                self.logger.warning(f"æœç´¢æ— ç»“æœ: {task.query}")
                return search_batch

            # 4. é˜¶æ®µ2ï¼šçˆ¬å–è¯¦æƒ…é¡µï¼ˆå¦‚æœå¯ç”¨ï¼‰
            scraped_count = 0
            if config.enable_detail_scrape:
                scraped_count = await self._enrich_with_details(
                    search_batch.results,
                    config,
                    task.query
                )
            else:
                self.logger.info("è¯¦æƒ…é¡µçˆ¬å–å·²ç¦ç”¨ï¼Œè·³è¿‡é˜¶æ®µ2")

            # 5. è®¡ç®—å®é™…ç§¯åˆ†æ¶ˆè€—
            # ä½¿ç”¨ FirecrawlCreditsCalculator è®¡ç®—å®é™…æ¶ˆè€—
            actual_credits = FirecrawlCreditsCalculator.calculate_actual_credits(
                operation="search",
                results_count=len(search_batch.results),
                scraped_count=scraped_count
            )
            search_batch.credits_used = actual_credits

            self.logger.info(
                f"ğŸ’° ç§¯åˆ†æ¶ˆè€—: æœç´¢={search_batch.credits_used - scraped_count}, "
                f"çˆ¬å–={scraped_count}, æ€»è®¡={actual_credits}"
            )

            # 6. è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.utcnow()
            search_batch.execution_time_ms = int(
                (end_time - start_time).total_seconds() * 1000
            )

            self._log_execution_end(
                task,
                len(search_batch.results),
                search_batch.execution_time_ms
            )

            return search_batch

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œæœç´¢ä»»åŠ¡å¤±è´¥: {e}")
            raise ExecutionError(f"æœç´¢ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")

    async def _execute_search(
        self,
        task: SearchTask,
        config: SearchConfig
    ) -> SearchResultBatch:
        """é˜¶æ®µ1ï¼šæ‰§è¡Œæœç´¢

        Args:
            task: æœç´¢ä»»åŠ¡
            config: æœç´¢é…ç½®

        Returns:
            SearchResultBatch: æœç´¢ç»“æœæ‰¹æ¬¡
        """
        self.logger.info(f"ğŸ” é˜¶æ®µ1ï¼šæœç´¢å…³é”®è¯ '{task.query}'")

        # æ„å»º UserSearchConfigï¼ˆä½¿ç”¨ overrides å­—å…¸ï¼‰
        user_config = UserSearchConfig(
            template_name="default",
            overrides={
                "limit": config.limit,
                "language": config.language,
                "include_domains": config.include_domains,
                "strict_language_filter": config.strict_language_filter
            }
        )

        # è°ƒç”¨ Search API
        search_batch = await self.search_adapter.search(
            query=task.query,
            user_config=user_config,
            task_id=str(task.id)
        )

        self.logger.info(
            f"âœ… é˜¶æ®µ1å®Œæˆï¼šè·å¾— {len(search_batch.results)} æ¡æœç´¢ç»“æœ"
        )

        return search_batch

    def _filter_homepage_urls(
        self,
        results: List[SearchResult],
        config: SearchConfig
    ) -> List[SearchResult]:
        """è¿‡æ»¤é¦–é¡µURLå’Œé»‘åå•åŸŸåï¼Œåªä¿ç•™è¯¦æƒ…é¡µURL

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            config: æœç´¢é…ç½®

        Returns:
            è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        filtered_results = []
        filter_stats = {
            'homepage': 0,
            'excluded_domain': 0,
            'total': len(results)
        }

        for result in results:
            url = result.url
            url_lower = url.lower()

            # 1. æ£€æŸ¥åŸŸåé»‘åå•ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            if config.exclude_domains:
                is_excluded_domain = False
                for excluded_domain in config.exclude_domains:
                    if excluded_domain.lower() in url_lower:
                        is_excluded_domain = True
                        filter_stats['excluded_domain'] += 1
                        self.logger.debug(
                            f"ğŸš« è¿‡æ»¤é»‘åå•åŸŸå: {result.url} (åŒ¹é…: {excluded_domain})"
                        )
                        break

                if is_excluded_domain:
                    continue  # è·³è¿‡é»‘åå•åŸŸå

            # 2. æ£€æŸ¥æ˜¯å¦è¿‡æ»¤é¦–é¡µï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.filter_homepage:
                # é¦–é¡µURLç‰¹å¾
                homepage_patterns = [
                    r'/$',  # ä»¥/ç»“å°¾
                    r'/index\.(html|php|htm|aspx|jsp)$',
                    r'/home$',
                    r'/default\.(html|aspx)$',
                    r'^https?://[^/]+/?$',  # åªæœ‰åŸŸåï¼Œæ²¡æœ‰è·¯å¾„
                ]

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…é¦–é¡µæ¨¡å¼
                is_homepage = False
                for pattern in homepage_patterns:
                    if re.search(pattern, url_lower):
                        is_homepage = True
                        filter_stats['homepage'] += 1
                        self.logger.debug(
                            f"ğŸš« è¿‡æ»¤é¦–é¡µURL: {result.url} (åŒ¹é…æ¨¡å¼: {pattern})"
                        )
                        break

                if is_homepage:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¯¦æƒ…é¡µç‰¹å¾ï¼ˆå¯ä»¥è¦†ç›–é¦–é¡µåˆ¤æ–­ï¼‰
                    detail_page_indicators = [
                        r'/\d{4}/\d{2}/',  # æ—¥æœŸè·¯å¾„ /2025/01/
                        r'/article/\d+',    # æ–‡ç« ID
                        r'/post/\d+',       # å¸–å­ID
                        r'/news/\d+',       # æ–°é—»ID
                        r'/p/\d+',          # é¡µé¢ID
                        r'[^/]+/[^/]+/[^/]+',  # è‡³å°‘3å±‚è·¯å¾„
                    ]

                    has_detail_indicator = False
                    for pattern in detail_page_indicators:
                        if re.search(pattern, url_lower):
                            has_detail_indicator = True
                            self.logger.debug(
                                f"âœ… ä¿ç•™ï¼ˆè™½åŒ¹é…é¦–é¡µä½†æœ‰è¯¦æƒ…é¡µç‰¹å¾ï¼‰: {result.url}"
                            )
                            break

                    if not has_detail_indicator:
                        continue  # è·³è¿‡é¦–é¡µURL

            # 3. é€šè¿‡æ‰€æœ‰è¿‡æ»¤å™¨ï¼Œä¿ç•™è¯¥ç»“æœ
            filtered_results.append(result)

        # è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡
        total_filtered = filter_stats['homepage'] + filter_stats['excluded_domain']
        if total_filtered > 0:
            self.logger.info(
                f"ğŸ” URLè¿‡æ»¤ç»Ÿè®¡: æ€»è®¡ {filter_stats['total']} ä¸ª â†’ "
                f"è¿‡æ»¤é¦–é¡µ {filter_stats['homepage']} ä¸ª, "
                f"è¿‡æ»¤é»‘åå•åŸŸå {filter_stats['excluded_domain']} ä¸ª, "
                f"ä¿ç•™ {len(filtered_results)} ä¸ª"
            )

        return filtered_results

    def _validate_content_quality(
        self,
        content: str,
        query: str,
        url: str
    ) -> Optional[str]:
        """éªŒè¯å†…å®¹è´¨é‡ï¼Œæ£€æµ‹æ˜¯å¦ä¸ºé¦–é¡µå†…å®¹

        Args:
            content: é¡µé¢å†…å®¹
            query: æœç´¢å…³é”®è¯
            url: é¡µé¢URL

        Returns:
            Optional[str]: å¦‚æœå†…å®¹æ— æ•ˆï¼Œè¿”å›åŸå› ï¼›å¦åˆ™è¿”å› None
        """
        if not content:
            return "å†…å®¹ä¸ºç©º"

        # 1. å†…å®¹é•¿åº¦æ£€æŸ¥
        content_length = len(content)
        if content_length < 500:
            return f"å†…å®¹è¿‡çŸ­ ({content_length} å­—ç¬¦)ï¼Œå¯èƒ½ä¸ºé¦–é¡µæˆ–æ— æ•ˆé¡µé¢"

        if content_length > 50000:
            return f"å†…å®¹è¿‡é•¿ ({content_length} å­—ç¬¦)ï¼Œå¯èƒ½ä¸ºé¦–é¡µæˆ–åˆ—è¡¨é¡µ"

        # 2. å…³é”®è¯ç›¸å…³æ€§æ£€æŸ¥
        content_lower = content.lower()
        query_lower = query.lower()

        # æ£€æŸ¥æŸ¥è¯¢è¯æ˜¯å¦å‡ºç°åœ¨å†…å®¹ä¸­
        if query_lower not in content_lower:
            # æ£€æŸ¥æŸ¥è¯¢è¯çš„å„ä¸ªéƒ¨åˆ†ï¼ˆåˆ†è¯ï¼‰
            query_words = query_lower.split()
            matched_words = sum(1 for word in query_words if word in content_lower)
            match_ratio = matched_words / len(query_words) if query_words else 0

            if match_ratio < 0.5:
                return f"å…³é”®è¯ç›¸å…³æ€§ä½ ({match_ratio:.1%})ï¼Œå¯èƒ½ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µ"

        # 3. é¦–é¡µç‰¹å¾æ£€æµ‹
        # ç»Ÿè®¡é“¾æ¥å¯†åº¦ï¼ˆé¦–é¡µé€šå¸¸æœ‰å¤§é‡é“¾æ¥ï¼‰
        link_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'  # Markdown é“¾æ¥
        links = re.findall(link_pattern, content)
        link_density = len(links) / max(content_length / 1000, 1)  # æ¯åƒå­—ç¬¦çš„é“¾æ¥æ•°

        if link_density > 20:
            return f"é“¾æ¥å¯†åº¦è¿‡é«˜ ({link_density:.1f} ä¸ª/åƒå­—)ï¼Œå¯èƒ½ä¸ºé¦–é¡µæˆ–å¯¼èˆªé¡µ"

        # 4. å¯¼èˆªå…³é”®è¯æ£€æµ‹ï¼ˆé¦–é¡µå¸¸è§è¯æ±‡ï¼‰
        homepage_keywords = [
            'é¦–é¡µ', 'å¯¼èˆª', 'èœå•', 'æ›´å¤š', 'æŸ¥çœ‹æ›´å¤š', 'æœ€æ–°', 'çƒ­é—¨', 'æ¨è',
            'home', 'navigation', 'menu', 'more', 'latest', 'popular', 'recommended'
        ]
        homepage_keyword_count = sum(
            1 for keyword in homepage_keywords
            if keyword in content_lower
        )

        if homepage_keyword_count > 5:
            return f"é¦–é¡µç‰¹å¾è¯è¿‡å¤š ({homepage_keyword_count} ä¸ª)ï¼Œå¯èƒ½ä¸ºé¦–é¡µ"

        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        return None

    async def _enrich_with_details(
        self,
        results: List[SearchResult],
        config: SearchConfig,
        query: str
    ) -> int:
        """é˜¶æ®µ2ï¼šæ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µå†…å®¹

        ä½¿ç”¨å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†ï¼Œç¡®ä¿éƒ¨åˆ†å¤±è´¥ä¸å½±å“æ•´ä½“

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            config: æœç´¢é…ç½®
            query: æœç´¢å…³é”®è¯ï¼ˆç”¨äºå†…å®¹è´¨é‡éªŒè¯ï¼‰

        Returns:
            int: æˆåŠŸçˆ¬å–çš„é¡µé¢æ•°ï¼ˆç”¨äºç§¯åˆ†è®¡ç®—ï¼‰
        """
        # URLè´¨é‡è¿‡æ»¤
        filtered_results = self._filter_homepage_urls(results, config)

        if len(filtered_results) == 0:
            self.logger.warning("âš ï¸ URLè¿‡æ»¤åæ— å¯ç”¨è¯¦æƒ…é¡µï¼Œè·³è¿‡é˜¶æ®µ2")
            return 0

        self.logger.info(
            f"ğŸ“„ é˜¶æ®µ2ï¼šçˆ¬å– {len(filtered_results)} ä¸ªè¯¦æƒ…é¡µ "
            f"(å¹¶å‘æ•°: {config.max_concurrent_scrapes})"
        )

        # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = asyncio.Semaphore(config.max_concurrent_scrapes)

        # åˆ›å»ºçˆ¬å–ä»»åŠ¡ (ä½¿ç”¨è¿‡æ»¤åçš„ç»“æœ)
        tasks = [
            self._scrape_single_detail(result, config, semaphore, idx, query)
            for idx, result in enumerate(filtered_results)
        ]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡
        scrape_results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        success_count = sum(1 for r in scrape_results if r is True)
        failure_count = len(results) - success_count

        self.logger.info(
            f"âœ… é˜¶æ®µ2å®Œæˆï¼šæˆåŠŸ {success_count}/{len(results)}, "
            f"å¤±è´¥ {failure_count}"
        )

        return success_count

    async def _scrape_single_detail(
        self,
        result: SearchResult,
        config: SearchConfig,
        semaphore: asyncio.Semaphore,
        index: int,
        query: str
    ) -> bool:
        """çˆ¬å–å•ä¸ªè¯¦æƒ…é¡µ

        Args:
            result: æœç´¢ç»“æœï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            config: æœç´¢é…ç½®
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            index: ç»“æœç´¢å¼•
            query: æœç´¢å…³é”®è¯ï¼ˆç”¨äºå†…å®¹è´¨é‡éªŒè¯ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        async with semaphore:
            try:
                self.logger.debug(
                    f"ğŸ” [{index + 1}] çˆ¬å–è¯¦æƒ…é¡µ: {result.url[:60]}..."
                )

                # æ„å»º Scrape å‚æ•°
                scrape_options = {
                    "only_main_content": config.only_main_content,
                    "wait_for": config.wait_for,
                    "exclude_tags": config.exclude_tags,
                    "timeout": config.timeout
                }

                # è°ƒç”¨ Scrape API
                crawl_result = await self.scrape_adapter.scrape(
                    result.url,
                    **scrape_options
                )

                # æå–å†…å®¹
                content = crawl_result.markdown or crawl_result.content

                # å†…å®¹è´¨é‡éªŒè¯
                validation_error = self._validate_content_quality(
                    content=content,
                    query=query,
                    url=result.url
                )

                if validation_error:
                    self.logger.warning(
                        f"âš ï¸ [{index + 1}] å†…å®¹è´¨é‡æ£€æŸ¥å¤±è´¥ {result.url}: {validation_error}"
                    )
                    # éªŒè¯å¤±è´¥ï¼Œä¸æ›´æ–°å†…å®¹ï¼Œä¿ç•™åŸå§‹æœç´¢ç»“æœ
                    return False

                # æ›´æ–°æœç´¢ç»“æœçš„å†…å®¹
                result.markdown_content = content
                result.html_content = crawl_result.html
                # v2.1.0: ä¸å†æ›´æ–°metadataï¼Œæ‰€æœ‰å­—æ®µå·²åœ¨é˜¶æ®µ1æå–ä¸ºç‹¬ç«‹å­—æ®µ

                self.logger.debug(f"âœ… [{index + 1}] çˆ¬å–æˆåŠŸä¸”å†…å®¹è´¨é‡åˆæ ¼")

                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if config.scrape_delay > 0:
                    await asyncio.sleep(config.scrape_delay)

                return True

            except Exception as e:
                self.logger.warning(
                    f"âŒ [{index + 1}] çˆ¬å–è¯¦æƒ…é¡µå¤±è´¥ {result.url}: {e}"
                )
                # å¤±è´¥æ—¶ä¿ç•™åŸå§‹æœç´¢ç»“æœï¼Œä¸æŠ›å‡ºå¼‚å¸¸
                return False
