"""
Map + Scrape ç»„åˆæ‰§è¡Œå™¨

ä½¿ç”¨ Firecrawl Map API å¿«é€Ÿå‘ç°URLï¼Œå†é€šè¿‡ Scrape API æ‰¹é‡è·å–å†…å®¹
æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼Œå®ç°ç²¾ç¡®ã€é«˜æ•ˆã€ä½æˆæœ¬çš„å†…å®¹çˆ¬å–
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

from src.core.domain.entities.search_task import SearchTask
from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter, MapAPIError
from src.core.domain.interfaces.crawler_interface import CrawlResult
from src.core.domain.entities.firecrawl_raw_response import create_firecrawl_raw_response
from src.infrastructure.database.firecrawl_raw_repositories import get_firecrawl_raw_repository
from src.infrastructure.database.repositories import SearchResultRepository

from ..base import TaskExecutor, ConfigValidationError, ExecutionError
from ..config import MapScrapeConfig, ConfigFactory
from ..credits_calculator import FirecrawlCreditsCalculator
from ..filters import PipelineBuilder, FilterContext


class MapScrapeExecutor(TaskExecutor):
    """Map + Scrape ç»„åˆä»»åŠ¡æ‰§è¡Œå™¨

    æ‰§è¡Œæµç¨‹ï¼š
    1. Map API å‘ç°æ‰€æœ‰URLï¼ˆå›ºå®š1 creditï¼‰
    2. æ‰¹é‡ Scrape API è·å–é¡µé¢å†…å®¹ï¼ˆN creditsï¼‰
    3. æ—¶é—´è¿‡æ»¤ï¼ˆæ ¹æ®publishedDateï¼‰
    4. ä¿å­˜ç»“æœ

    ä¼˜åŠ¿ï¼š
    - ç²¾ç¡®æ§åˆ¶ï¼šåªçˆ¬å–éœ€è¦çš„é¡µé¢
    - æˆæœ¬ä¼˜åŒ–ï¼šç›¸æ¯”Crawl APIèŠ‚çœ80-90%ç§¯åˆ†
    - æ—¶é—´è¿‡æ»¤ï¼šæ”¯æŒæŒ‰å‘å¸ƒæ—¥æœŸè¿‡æ»¤å†…å®¹
    """

    def __init__(self):
        super().__init__()
        self.adapter = FirecrawlAdapter()
        self.result_repo = SearchResultRepository()

    def _extract_metadata_fields(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ä»çˆ¬å–ç»“æœçš„metadataä¸­æå–ç»“æ„åŒ–å­—æ®µ

        ä¸å…¶ä»–Executorä¿æŒä¸€è‡´çš„å­—æ®µæå–é€»è¾‘

        Args:
            metadata: CrawlResult.metadataå­—å…¸

        Returns:
            åŒ…å«æå–å­—æ®µçš„å­—å…¸
        """
        extracted = {}

        # 1. æå–ä½œè€…
        extracted['author'] = metadata.get('author')

        # 2. æå–è¯­è¨€
        extracted['language'] = metadata.get('language')

        # 3. æå–æ–‡ç« æ ‡ç­¾ï¼ˆå¤„ç†åˆ—è¡¨æ ¼å¼ï¼‰
        article_tag_raw = metadata.get('article:tag')
        if isinstance(article_tag_raw, list):
            extracted['article_tag'] = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
        else:
            extracted['article_tag'] = article_tag_raw

        # 4. æå–æ–‡ç« å‘å¸ƒæ—¶é—´
        extracted['article_published_time'] = metadata.get('article:published_time')

        # 5. æå–æºURLï¼ˆé‡å®šå‘åœºæ™¯ï¼‰
        extracted['source_url'] = metadata.get('sourceURL')

        # 6. æå–HTTPçŠ¶æ€ç 
        extracted['http_status_code'] = metadata.get('statusCode')

        # 7. è§£æå‘å¸ƒæ—¥æœŸ
        published_date = None
        published_date_str = metadata.get('publishedDate') or metadata.get('published_date')
        if published_date_str:
            try:
                published_date = datetime.fromisoformat(published_date_str)
            except:
                self.logger.debug(f"æ— æ³•è§£æå‘å¸ƒæ—¥æœŸ: {published_date_str}")
        extracted['published_date'] = published_date

        return extracted

    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if not task.crawl_url:
            self.logger.error("Map + Scrape ä»»åŠ¡å¿…é¡»æä¾› crawl_url å‚æ•°")
            return False

        if not task.crawl_url.startswith(('http://', 'https://')):
            self.logger.error(f"crawl_url æ ¼å¼æ— æ•ˆ: {task.crawl_url}")
            return False

        # éªŒè¯é…ç½®å¯¹è±¡
        config = ConfigFactory.create_map_scrape_config(task.crawl_config)
        if not config.is_valid():
            self.logger.error("MapScrapeConfig é…ç½®æ— æ•ˆ")
            return False

        return True

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œ Map + Scrape ä»»åŠ¡

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            SearchResultBatch: çˆ¬å–ç»“æœæ‰¹æ¬¡

        Raises:
            ConfigValidationError: é…ç½®éªŒè¯å¤±è´¥
            ExecutionError: æ‰§è¡Œè¿‡ç¨‹é”™è¯¯
        """
        start_time = datetime.utcnow()
        self._log_execution_start(task)

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_map_scrape_config(task.crawl_config)
        self.logger.info(f"ğŸ“‹ é…ç½®: {config}")

        try:
            # 3. æ‰§è¡Œ Map API å‘ç° URL
            self.logger.info(f"ğŸ—ºï¸  Step 1: ä½¿ç”¨ Map API å‘ç° URL")
            discovered_urls = await self._execute_map(task.crawl_url, config)

            if not discovered_urls:
                self.logger.warning(f"âš ï¸  Map API æœªå‘ç°ä»»ä½•URL")
                return self._create_empty_batch(task)

            self.logger.info(f"âœ… å‘ç° {len(discovered_urls)} ä¸ªURL")

            # 3.3. URL è¿‡æ»¤ (v2.1.2)
            discovered_urls = await self._filter_urls(discovered_urls, task, config)

            if not discovered_urls:
                self.logger.warning(f"âš ï¸  è¿‡æ»¤åæ— å‰©ä½™URL")
                return self._create_empty_batch(task)

            # 3.4. é™åˆ¶ Scrape URL æ•°é‡ (v2.1.3)
            if config.max_scrape_urls and len(discovered_urls) > config.max_scrape_urls:
                self.logger.info(
                    f"ğŸ“Š é™åˆ¶ URL æ•°é‡: {len(discovered_urls)} â†’ {config.max_scrape_urls}"
                )
                discovered_urls = discovered_urls[:config.max_scrape_urls]

            # 3.5. URLå»é‡æ£€æŸ¥ï¼ˆv2.1.1ï¼‰
            if config.enable_dedup:
                self.logger.info(f"ğŸ” æ£€æŸ¥å·²çˆ¬å–URLå»é‡")
                existing_urls = await self.result_repo.check_existing_urls(
                    task_id=str(task.id),
                    urls=discovered_urls
                )

                if existing_urls:
                    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„URL
                    new_urls = [url for url in discovered_urls if url not in existing_urls]
                    self.logger.info(
                        f"âœ… URLå»é‡: å‘ç°{len(discovered_urls)}ä¸ª, "
                        f"å·²å­˜åœ¨{len(existing_urls)}ä¸ª, "
                        f"å¾…çˆ¬å–{len(new_urls)}ä¸ª"
                    )
                    discovered_urls = new_urls

                    if not discovered_urls:
                        self.logger.warning(f"âš ï¸  æ‰€æœ‰URLå‡å·²çˆ¬å–ï¼Œè·³è¿‡æ‰§è¡Œ")
                        return self._create_empty_batch(task)

            # 4. æ‰¹é‡ Scrape è·å–å†…å®¹
            self.logger.info(f"ğŸ“¥ Step 2: æ‰¹é‡ Scrape è·å–é¡µé¢å†…å®¹")
            scrape_results = await self._batch_scrape(discovered_urls, config)

            if not scrape_results:
                self.logger.warning(f"âš ï¸  Scrape æœªè·å–åˆ°ä»»ä½•å†…å®¹")
                return self._create_empty_batch(task)

            self.logger.info(f"âœ… æˆåŠŸçˆ¬å– {len(scrape_results)} ä¸ªé¡µé¢")

            # 5. æ—¶é—´è¿‡æ»¤
            if config.has_time_filter():
                self.logger.info(f"ğŸ•’ Step 3: åº”ç”¨æ—¶é—´è¿‡æ»¤")
                scrape_results = self._filter_by_date(scrape_results, config)
                self.logger.info(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(scrape_results)} ä¸ªé¡µé¢")

            if not scrape_results:
                self.logger.warning(f"âš ï¸  æ—¶é—´è¿‡æ»¤åæ— ç»“æœ")
                return self._create_empty_batch(task)

            # 6. ä¿å­˜åŸå§‹å“åº”æ•°æ®
            await self._save_raw_responses(scrape_results, task)

            # 7. è½¬æ¢ä¸º SearchResult
            search_results = self._convert_to_search_results(scrape_results, task)

            # 8. åˆ›å»ºç»“æœæ‰¹æ¬¡
            batch = self._create_result_batch(
                task,
                query=f"Map+Scrape: {task.crawl_url}"
            )

            for result in search_results:
                batch.add_result(result)

            batch.total_count = len(search_results)

            # 9. è®¡ç®—å®é™…ç§¯åˆ†æ¶ˆè€—
            batch.credits_used = FirecrawlCreditsCalculator.calculate_map_scrape_credits(
                urls_discovered=len(discovered_urls),
                pages_scraped=len(scrape_results)
            )
            self.logger.info(
                f"ğŸ’° ç§¯åˆ†æ¶ˆè€—: {batch.credits_used} "
                f"(Map: 1 + Scrape: {len(scrape_results)})"
            )

            # 10. è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.utcnow()
            batch.execution_time_ms = int(
                (end_time - start_time).total_seconds() * 1000
            )

            self._log_execution_end(
                task,
                len(search_results),
                batch.execution_time_ms
            )

            return batch

        except MapAPIError as e:
            self.logger.error(f"Map API è°ƒç”¨å¤±è´¥: {e}")
            raise ExecutionError(f"Map API å¤±è´¥: {str(e)}")
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œ Map + Scrape å¤±è´¥: {e}")
            raise ExecutionError(f"Map + Scrape æ‰§è¡Œå¤±è´¥: {str(e)}")

    async def _execute_map(
        self,
        url: str,
        config: MapScrapeConfig
    ) -> List[str]:
        """æ‰§è¡Œ Map API å‘ç° URL

        Args:
            url: ç½‘ç«™èµ·å§‹URL
            config: Map + Scrape é…ç½®

        Returns:
            List[str]: å‘ç°çš„URLåˆ—è¡¨
        """
        try:
            # è°ƒç”¨ Map API
            links = await self.adapter.map(
                url=url,
                search=config.search,
                limit=config.map_limit
            )

            # æå–URLåˆ—è¡¨
            urls = [link['url'] for link in links if link.get('url')]

            return urls

        except Exception as e:
            self.logger.error(f"Map API æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _batch_scrape(
        self,
        urls: List[str],
        config: MapScrapeConfig
    ) -> List[CrawlResult]:
        """æ‰¹é‡ Scrape è·å–é¡µé¢å†…å®¹

        ä½¿ç”¨ asyncio.Semaphore æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è§¦å‘é™æµ

        Args:
            urls: URLåˆ—è¡¨
            config: Map + Scrape é…ç½®

        Returns:
            List[CrawlResult]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(config.max_concurrent_scrapes)
        results = []
        failed_count = 0

        async def scrape_with_semaphore(url: str) -> Optional[CrawlResult]:
            """å¸¦ä¿¡å·é‡æ§åˆ¶çš„ Scrape"""
            async with semaphore:
                try:
                    # å»¶è¿Ÿæ§åˆ¶
                    if config.scrape_delay > 0:
                        await asyncio.sleep(config.scrape_delay)

                    # è°ƒç”¨ Scrape API
                    result = await self.adapter.scrape(
                        url,
                        only_main_content=config.only_main_content,
                        wait_for=config.wait_for,
                        exclude_tags=config.exclude_tags,
                        timeout=config.timeout
                    )
                    return result

                except Exception as e:
                    self.logger.warning(f"Scrape å¤±è´¥ {url}: {e}")
                    return None

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ Scrape è¯·æ±‚
        self.logger.info(
            f"ğŸ”„ å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL "
            f"(å¹¶å‘æ•°: {config.max_concurrent_scrapes}, "
            f"å»¶è¿Ÿ: {config.scrape_delay}s)"
        )

        tasks = [scrape_with_semaphore(url) for url in urls]
        scrape_results = await asyncio.gather(*tasks)

        # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
        for result in scrape_results:
            if result is not None:
                results.append(result)
            else:
                failed_count += 1

        # æ£€æŸ¥æˆåŠŸç‡
        success_rate = len(results) / len(urls) if urls else 0
        self.logger.info(
            f"ğŸ“Š Scrape å®Œæˆ: æˆåŠŸ {len(results)}, å¤±è´¥ {failed_count}, "
            f"æˆåŠŸç‡ {success_rate:.1%}"
        )

        if config.allow_partial_failure:
            if success_rate < config.min_success_rate:
                self.logger.warning(
                    f"âš ï¸  æˆåŠŸç‡ {success_rate:.1%} ä½äºæœ€ä½è¦æ±‚ "
                    f"{config.min_success_rate:.1%}"
                )
        else:
            if failed_count > 0:
                raise ExecutionError(
                    f"Scrape å¤±è´¥ {failed_count} ä¸ªURLï¼Œä¸å…è®¸éƒ¨åˆ†å¤±è´¥"
                )

        return results

    def _filter_by_date(
        self,
        results: List[CrawlResult],
        config: MapScrapeConfig
    ) -> List[CrawlResult]:
        """æ ¹æ®å‘å¸ƒæ—¥æœŸè¿‡æ»¤ç»“æœ

        Args:
            results: çˆ¬å–ç»“æœåˆ—è¡¨
            config: Map + Scrape é…ç½®

        Returns:
            List[CrawlResult]: è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        if not config.has_time_filter():
            return results

        filtered = []

        for result in results:
            # è·å–å‘å¸ƒæ—¥æœŸ
            metadata = result.metadata if isinstance(result.metadata, dict) else {}
            published_date_str = (
                metadata.get('publishedDate') or
                metadata.get('published_date')
            )

            if not published_date_str:
                # æ²¡æœ‰å‘å¸ƒæ—¥æœŸçš„é¡µé¢ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿ç•™
                # è¿™é‡Œé€‰æ‹©ä¿ç•™ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                self.logger.debug(f"âš ï¸  æ— å‘å¸ƒæ—¥æœŸ: {result.url}")
                filtered.append(result)
                continue

            try:
                published_date = datetime.fromisoformat(published_date_str)

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                if config.start_date and published_date < config.start_date:
                    self.logger.debug(
                        f"âŒ æ—©äºèµ·å§‹æ—¥æœŸ: {result.url} "
                        f"({published_date.date()})"
                    )
                    continue

                if config.end_date and published_date > config.end_date:
                    self.logger.debug(
                        f"âŒ æ™šäºç»“æŸæ—¥æœŸ: {result.url} "
                        f"({published_date.date()})"
                    )
                    continue

                # é€šè¿‡æ—¶é—´è¿‡æ»¤
                filtered.append(result)

            except Exception as e:
                self.logger.warning(
                    f"âš ï¸  è§£æå‘å¸ƒæ—¥æœŸå¤±è´¥: {result.url}, {published_date_str}"
                )
                # è§£æå¤±è´¥çš„é¡µé¢ä¿ç•™
                filtered.append(result)

        return filtered

    async def _save_raw_responses(
        self,
        scrape_results: List[CrawlResult],
        task: SearchTask
    ) -> None:
        """ä¿å­˜ Firecrawl åŸå§‹å“åº”æ•°æ®

        Args:
            scrape_results: Scrape ç»“æœåˆ—è¡¨
            task: æœç´¢ä»»åŠ¡
        """
        if not scrape_results:
            return

        try:
            raw_repo = await get_firecrawl_raw_repository()
            raw_responses = []

            for result in scrape_results:
                # æ„å»ºåŸå§‹å“åº”æ•°æ®
                raw_data = {
                    "url": result.url,
                    "content": result.content or "",
                    "markdown": result.markdown,
                    "html": result.html,
                    "metadata": result.metadata if isinstance(result.metadata, dict) else {},
                    "screenshot": result.screenshot
                }

                # åˆ›å»ºåŸå§‹å“åº”å®ä½“
                raw_response = create_firecrawl_raw_response(
                    task_id=str(task.id),
                    result_url=result.url,
                    raw_data=raw_data,
                    api_endpoint="map_scrape",
                    response_time_ms=0
                )
                raw_responses.append(raw_response)

            # æ‰¹é‡ä¿å­˜
            if raw_responses:
                await raw_repo.batch_create(raw_responses)
                self.logger.info(
                    f"âœ… å·²ä¿å­˜ {len(raw_responses)} æ¡åŸå§‹å“åº”æ•°æ®åˆ° "
                    f"firecrawl_raw_responses"
                )

        except Exception as e:
            # åŸå§‹æ•°æ®ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            self.logger.warning(f"âš ï¸  ä¿å­˜åŸå§‹å“åº”æ•°æ®å¤±è´¥: {e}")

    def _convert_to_search_results(
        self,
        scrape_results: List[CrawlResult],
        task: SearchTask
    ) -> List[SearchResult]:
        """å°† CrawlResult è½¬æ¢ä¸º SearchResult

        Args:
            scrape_results: Scrape ç»“æœåˆ—è¡¨
            task: æœç´¢ä»»åŠ¡

        Returns:
            List[SearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        search_results = []

        for idx, result in enumerate(scrape_results, start=1):
            # å¤„ç† metadata
            metadata_dict = {}
            if result.metadata:
                if isinstance(result.metadata, dict):
                    metadata_dict = result.metadata
                else:
                    # å°†å¯¹è±¡å±æ€§è½¬æ¢ä¸ºå­—å…¸
                    metadata_dict = {
                        k: v for k, v in vars(result.metadata).items()
                        if not k.startswith('_')
                    }

            # æå–å…ƒæ•°æ®å­—æ®µ
            metadata_fields = self._extract_metadata_fields(metadata_dict)

            # è·å–æ ‡é¢˜å’ŒURL
            title = metadata_dict.get("title", "")
            result_url = (
                metadata_dict.get("url") or
                metadata_dict.get("source_url") or
                result.url or
                ""
            )

            search_result = SearchResult(
                task_id=str(task.id),
                title=title if title else result_url,
                url=result_url,
                snippet=(
                    result.content[:200] if result.content else ""
                ),
                source="map_scrape",
                # å…ƒæ•°æ®å­—æ®µ
                published_date=metadata_fields.get('published_date'),
                author=metadata_fields.get('author'),
                language=metadata_fields.get('language'),
                article_tag=metadata_fields.get('article_tag'),
                article_published_time=metadata_fields.get('article_published_time'),
                source_url=metadata_fields.get('source_url'),
                http_status_code=metadata_fields.get('http_status_code'),
                search_position=idx,
                # å†…å®¹å­—æ®µ
                markdown_content=(
                    result.markdown if result.markdown
                    else result.content
                ),
                html_content=result.html,
                metadata={},  # ä¸å†ä¼ é€’metadataï¼Œæ‰€æœ‰å­—æ®µå·²æå–
                relevance_score=1.0,
                status=ResultStatus.PENDING
            )
            search_results.append(search_result)

        return search_results

    async def _filter_urls(
        self,
        urls: List[str],
        task: SearchTask,
        config: MapScrapeConfig
    ) -> List[str]:
        """è¿‡æ»¤æ— ç”¨URL (v2.1.2)

        ä½¿ç”¨æ¨¡å—åŒ–è¿‡æ»¤ç®¡é“è¿‡æ»¤æ‰æ— ç”¨é“¾æ¥,å¦‚ç™»å½•é¡µé¢ã€PDFæ–‡ä»¶ã€å¤–éƒ¨é“¾æ¥ç­‰ã€‚

        Args:
            urls: å¾…è¿‡æ»¤çš„URLåˆ—è¡¨
            task: æœç´¢ä»»åŠ¡
            config: Map + Scrape é…ç½®

        Returns:
            List[str]: è¿‡æ»¤åçš„URLåˆ—è¡¨
        """
        if not urls:
            return urls

        self.logger.info(f"ğŸ” å¼€å§‹URLè¿‡æ»¤: {len(urls)} ä¸ªåŸå§‹é“¾æ¥")

        try:
            # æ„å»ºé»˜è®¤è¿‡æ»¤ç®¡é“
            # TODO: æœªæ¥å¯ä»¥ä» config ä¸­è¯»å–è¿‡æ»¤æ¨¡å¼é…ç½®
            pipeline = PipelineBuilder.build_default_pipeline(task.crawl_url)

            # åˆ›å»ºè¿‡æ»¤ä¸Šä¸‹æ–‡
            context = FilterContext(
                base_url=task.crawl_url,
                task_id=str(task.id),
                config=config.to_dict()
            )

            # æ‰§è¡Œè¿‡æ»¤
            filtered_urls = pipeline.execute(urls, context)

            # è¾“å‡ºç»Ÿè®¡
            stats = pipeline.get_statistics()
            self._log_filter_statistics(stats, len(urls), len(filtered_urls))

            return filtered_urls

        except Exception as e:
            self.logger.error(f"URLè¿‡æ»¤å¤±è´¥: {e}", exc_info=True)
            # è¿‡æ»¤å¤±è´¥æ—¶è¿”å›åŸå§‹URLåˆ—è¡¨,ä¸å½±å“ä¸»æµç¨‹
            return urls

    def _log_filter_statistics(
        self,
        stats: dict,
        original_count: int,
        filtered_count: int
    ) -> None:
        """è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯

        Args:
            stats: è¿‡æ»¤å™¨ç»Ÿè®¡ä¿¡æ¯
            original_count: åŸå§‹URLæ•°é‡
            filtered_count: è¿‡æ»¤åURLæ•°é‡
        """
        total_filtered = original_count - filtered_count
        filter_rate = total_filtered / original_count if original_count > 0 else 0

        self.logger.info(
            f"âœ… URLè¿‡æ»¤å®Œæˆ: {original_count} â†’ {filtered_count} "
            f"(è¿‡æ»¤ {total_filtered}, {filter_rate*100:.1f}%)"
        )

        # è¯¦ç»†ç»Ÿè®¡
        if stats:
            self.logger.info(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
            for filter_name, filter_stats in stats.items():
                self.logger.info(
                    f"  - {filter_name}: "
                    f"è¿‡æ»¤ {filter_stats['filtered']} "
                    f"({filter_stats['filter_rate']*100:.1f}%)"
                )

    def _create_empty_batch(self, task: SearchTask) -> SearchResultBatch:
        """åˆ›å»ºç©ºç»“æœæ‰¹æ¬¡

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            SearchResultBatch: ç©ºç»“æœæ‰¹æ¬¡
        """
        batch = self._create_result_batch(
            task,
            query=f"Map+Scrape: {task.crawl_url}"
        )
        batch.total_count = 0
        batch.credits_used = 1  # Map API å›ºå®šæ¶ˆè€—1ç§¯åˆ†
        batch.execution_time_ms = 0
        return batch
