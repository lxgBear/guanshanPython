"""
ç½‘ç«™çˆ¬å–æ‰§è¡Œå™¨

ä½¿ç”¨ Firecrawl Crawl API é€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™çš„æ‰€æœ‰é¡µé¢
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

from src.core.domain.entities.search_task import SearchTask
from src.core.domain.entities.search_result import SearchResult, SearchResultBatch, ResultStatus
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter
from src.core.domain.interfaces.crawler_interface import CrawlResult
from src.core.domain.entities.firecrawl_raw_response import create_firecrawl_raw_response
from src.infrastructure.database.firecrawl_raw_repositories import get_firecrawl_raw_repository

from ..base import TaskExecutor, ConfigValidationError, ExecutionError
from ..config import CrawlConfig, ConfigFactory
from ..credits_calculator import FirecrawlCreditsCalculator


class CrawlExecutor(TaskExecutor):
    """ç½‘ç«™çˆ¬å–ä»»åŠ¡æ‰§è¡Œå™¨

    é€‚ç”¨äºéœ€è¦çˆ¬å–æ•´ä¸ªç½‘ç«™å†…å®¹çš„åœºæ™¯
    ä½¿ç”¨ Firecrawl Crawl API çš„å¼‚æ­¥çˆ¬å–åŠŸèƒ½
    """

    def __init__(self):
        super().__init__()
        self.crawl_adapter = FirecrawlAdapter()

    def _extract_metadata_fields(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ä»çˆ¬å–ç»“æœçš„metadataä¸­æå–ç»“æ„åŒ–å­—æ®µ

        ä¸FirecrawlSearchAdapterå’ŒScrapeExecutorä¿æŒä¸€è‡´çš„å­—æ®µæå–é€»è¾‘

        Args:
            metadata: CrawlResult.metadataå­—å…¸

        Returns:
            åŒ…å«æå–å­—æ®µçš„å­—å…¸
        """
        extracted = {}

        # 1. æå–ä½œè€…
        extracted['author'] = metadata.get('author')

        # 2. æå–è¯­è¨€
        extracted['language'] = metadata.get('language')

        # 3. æå–æ–‡ç« æ ‡ç­¾ï¼ˆå¤„ç†åˆ—è¡¨æ ¼å¼ï¼‰
        article_tag_raw = metadata.get('article:tag')
        if isinstance(article_tag_raw, list):
            extracted['article_tag'] = ', '.join(str(tag) for tag in article_tag_raw) if article_tag_raw else None
        else:
            extracted['article_tag'] = article_tag_raw

        # 4. æå–æ–‡ç« å‘å¸ƒæ—¶é—´
        extracted['article_published_time'] = metadata.get('article:published_time')

        # 5. æå–æºURLï¼ˆé‡å®šå‘åœºæ™¯ï¼‰
        extracted['source_url'] = metadata.get('sourceURL')

        # 6. æå–HTTPçŠ¶æ€ç 
        extracted['http_status_code'] = metadata.get('statusCode')

        # 7. è§£æå‘å¸ƒæ—¥æœŸ
        published_date = None
        published_date_str = metadata.get('publishedDate') or metadata.get('published_date')
        if published_date_str:
            try:
                published_date = datetime.fromisoformat(published_date_str)
            except:
                self.logger.debug(f"æ— æ³•è§£æå‘å¸ƒæ—¥æœŸ: {published_date_str}")
        extracted['published_date'] = published_date

        return extracted

    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if not task.crawl_url:
            self.logger.error("ç½‘ç«™çˆ¬å–ä»»åŠ¡å¿…é¡»æä¾› crawl_url å‚æ•°")
            return False

        if not task.crawl_url.startswith(('http://', 'https://')):
            self.logger.error(f"crawl_url æ ¼å¼æ— æ•ˆ: {task.crawl_url}")
            return False

        return True

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œç½‘ç«™çˆ¬å–ä»»åŠ¡

        Args:
            task: æœç´¢ä»»åŠ¡

        Returns:
            SearchResultBatch: çˆ¬å–ç»“æœæ‰¹æ¬¡

        Raises:
            ConfigValidationError: é…ç½®éªŒè¯å¤±è´¥
            ExecutionError: æ‰§è¡Œè¿‡ç¨‹é”™è¯¯
        """
        start_time = datetime.utcnow()
        self._log_execution_start(task)

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_crawl_config(task.crawl_config)

        try:
            # 3. æ‰§è¡Œçˆ¬å–
            self.logger.info(
                f"ğŸŒ å¼€å§‹çˆ¬å–ç½‘ç«™: {task.crawl_url} "
                f"(é™åˆ¶: {config.limit}é¡µ, æ·±åº¦: {config.max_depth})"
            )

            crawl_results = await self._execute_crawl(task.crawl_url, config)

            # 4. ä¿å­˜åŸå§‹å“åº”æ•°æ®åˆ° firecrawl_raw_responses
            await self._save_raw_responses(crawl_results, task)

            # 5. è½¬æ¢ä¸º SearchResult åˆ—è¡¨
            search_results = self._convert_to_search_results(
                crawl_results,
                task
            )

            # 5. åˆ›å»ºç»“æœæ‰¹æ¬¡
            batch = self._create_result_batch(
                task,
                query=f"ç½‘ç«™çˆ¬å–: {task.crawl_url}"
            )

            for result in search_results:
                batch.add_result(result)

            batch.total_count = len(search_results)

            # è®¡ç®—å®é™…ç§¯åˆ†æ¶ˆè€—
            batch.credits_used = FirecrawlCreditsCalculator.calculate_actual_credits(
                operation="crawl",
                pages_crawled=len(search_results)
            )
            self.logger.info(f"ğŸ’° ç§¯åˆ†æ¶ˆè€—: {batch.credits_used} ({len(search_results)} ä¸ªé¡µé¢)")

            # 6. è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.utcnow()
            batch.execution_time_ms = int(
                (end_time - start_time).total_seconds() * 1000
            )

            self._log_execution_end(
                task,
                len(search_results),
                batch.execution_time_ms
            )

            return batch

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œç½‘ç«™çˆ¬å–å¤±è´¥: {e}")
            raise ExecutionError(f"ç½‘ç«™çˆ¬å–æ‰§è¡Œå¤±è´¥: {str(e)}")

    async def _execute_crawl(
        self,
        url: str,
        config: CrawlConfig
    ) -> List[CrawlResult]:
        """æ‰§è¡Œç½‘ç«™çˆ¬å–

        Args:
            url: ç½‘ç«™URL
            config: çˆ¬å–é…ç½®

        Returns:
            List[CrawlResult]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        # æ„å»ºçˆ¬å–é€‰é¡¹
        crawl_options = {
            'limit': config.limit,
            'max_depth': config.max_depth,
            'include_paths': config.include_paths,
            'exclude_paths': config.exclude_paths,
            'allow_backward_links': config.allow_backward_links,
            'only_main_content': config.only_main_content,
            'wait_for': config.wait_for,
            'exclude_tags': config.exclude_tags
        }

        self.logger.info(f"ğŸ“‹ çˆ¬å–å‚æ•°: {crawl_options}")

        # è°ƒç”¨ Crawl API
        crawl_results = await self.crawl_adapter.crawl(url, **crawl_options)

        self.logger.info(f"âœ… çˆ¬å–å®Œæˆ: è·å¾— {len(crawl_results)} ä¸ªé¡µé¢")

        return crawl_results

    async def _save_raw_responses(
        self,
        crawl_results: List[CrawlResult],
        task: SearchTask
    ) -> None:
        """ä¿å­˜ Firecrawl åŸå§‹å“åº”æ•°æ®

        Args:
            crawl_results: çˆ¬å–ç»“æœåˆ—è¡¨
            task: æœç´¢ä»»åŠ¡
        """
        if not crawl_results:
            return

        try:
            raw_repo = await get_firecrawl_raw_repository()
            raw_responses = []

            for result in crawl_results:
                # æ„å»ºåŸå§‹å“åº”æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰
                raw_data = {
                    "url": result.url,
                    "content": result.content or "",
                    "markdown": result.markdown,
                    "html": result.html,
                    "metadata": result.metadata if isinstance(result.metadata, dict) else {},
                    "screenshot": result.screenshot
                }

                # åˆ›å»ºåŸå§‹å“åº”å®ä½“
                raw_response = create_firecrawl_raw_response(
                    task_id=str(task.id),
                    result_url=result.url,
                    raw_data=raw_data,
                    api_endpoint="crawl",
                    response_time_ms=0  # crawl æ²¡æœ‰å•ç‹¬çš„å“åº”æ—¶é—´
                )
                raw_responses.append(raw_response)

            # æ‰¹é‡ä¿å­˜
            if raw_responses:
                await raw_repo.batch_create(raw_responses)
                self.logger.info(f"âœ… å·²ä¿å­˜ {len(raw_responses)} æ¡åŸå§‹å“åº”æ•°æ®åˆ° firecrawl_raw_responses")

        except Exception as e:
            # åŸå§‹æ•°æ®ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            self.logger.warning(f"âš ï¸ ä¿å­˜åŸå§‹å“åº”æ•°æ®å¤±è´¥: {e}")

    def _convert_to_search_results(
        self,
        crawl_results: List[CrawlResult],
        task: SearchTask
    ) -> List[SearchResult]:
        """å°† CrawlResult è½¬æ¢ä¸º SearchResultï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«å…ƒæ•°æ®å­—æ®µï¼‰

        Args:
            crawl_results: çˆ¬å–ç»“æœåˆ—è¡¨
            task: æœç´¢ä»»åŠ¡

        Returns:
            List[SearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        search_results = []

        for idx, crawl_result in enumerate(crawl_results, start=1):
            # v2 API: metadata æ˜¯å¯¹è±¡,éœ€è¦è½¬æ¢ä¸ºå­—å…¸
            metadata_dict = {}
            if crawl_result.metadata:
                if isinstance(crawl_result.metadata, dict):
                    metadata_dict = crawl_result.metadata
                else:
                    # å°†å¯¹è±¡å±æ€§è½¬æ¢ä¸ºå­—å…¸
                    metadata_dict = {
                        k: v for k, v in vars(crawl_result.metadata).items()
                        if not k.startswith('_')
                    }

            # æå–å…ƒæ•°æ®å­—æ®µï¼ˆä¸ScrapeExecutorå’ŒSearchExecutorä¿æŒä¸€è‡´ï¼‰
            metadata_fields = self._extract_metadata_fields(metadata_dict)

            # è·å–æ ‡é¢˜å’ŒURL (v2 API: URLåœ¨metadataä¸­)
            title = metadata_dict.get("title", "")
            # v2 API: ä¼˜å…ˆä» metadata ä¸­è·å– URL
            result_url = metadata_dict.get("url") or metadata_dict.get("source_url") or crawl_result.url or ""

            search_result = SearchResult(
                task_id=str(task.id),
                title=title if title else result_url,
                url=result_url,
                snippet=(
                    crawl_result.content[:200] if crawl_result.content else ""
                ),
                source="crawl",
                # æ–°å¢å­—æ®µï¼šä»metadataæå–
                published_date=metadata_fields.get('published_date'),
                author=metadata_fields.get('author'),
                language=metadata_fields.get('language'),
                article_tag=metadata_fields.get('article_tag'),
                article_published_time=metadata_fields.get('article_published_time'),
                source_url=metadata_fields.get('source_url'),
                http_status_code=metadata_fields.get('http_status_code'),
                search_position=idx,  # çˆ¬å–ç»“æœçš„é¡ºåºä½ç½®
                # å†…å®¹å­—æ®µ
                markdown_content=(
                    crawl_result.markdown if crawl_result.markdown
                    else crawl_result.content
                ),
                html_content=crawl_result.html,
                metadata={},  # v2.1.0: ä¸å†ä¼ é€’metadataï¼Œæ‰€æœ‰å­—æ®µå·²æå–ä¸ºç‹¬ç«‹å­—æ®µ
                relevance_score=1.0,
                status=ResultStatus.PENDING
            )
            search_results.append(search_result)

        return search_results
