"""
NL Search æ ¸å¿ƒæœåŠ¡
ç”¨äºç¼–æ’æ•´ä¸ªè‡ªç„¶è¯­è¨€æœç´¢æµç¨‹

ç‰ˆæœ¬: v2.0.0 (MongoDB)
æ—¥æœŸ: 2025-11-17
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.services.nl_search.config import nl_search_config
from src.services.nl_search.llm_processor import LLMProcessor
from src.services.nl_search.gpt5_search_adapter import GPT5SearchAdapter
from src.infrastructure.database.mongo_nl_search_repository import MongoNLSearchLogRepository
from src.infrastructure.database.user_selection_repository import user_selection_repository

logger = logging.getLogger(__name__)


class NLSearchService:
    """
    è‡ªç„¶è¯­è¨€æœç´¢æ ¸å¿ƒæœåŠ¡

    èŒè´£:
    1. ç¼–æ’æ•´ä¸ªæœç´¢æµç¨‹
    2. è°ƒç”¨LLMè§£æç”¨æˆ·æŸ¥è¯¢
    3. è°ƒç”¨æœç´¢é€‚é…å™¨æ‰§è¡Œæœç´¢
    4. ä¿å­˜æœç´¢è®°å½•åˆ°æ•°æ®åº“
    5. è¿”å›å®Œæ•´çš„æœç´¢ç»“æœ

    ä½¿ç”¨ç¤ºä¾‹:
        service = NLSearchService()
        result = await service.create_search(
            query_text="æœ€è¿‘æœ‰å“ªäº›AIæŠ€æœ¯çªç ´",
            user_id="user_123"
        )
    """

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.llm_processor = LLMProcessor()
        self.gpt5_adapter = GPT5SearchAdapter(
            test_mode=not nl_search_config.enabled  # åŠŸèƒ½å…³é—­æ—¶ä½¿ç”¨æµ‹è¯•æ¨¡å¼
        )
        self.repository = MongoNLSearchLogRepository()
        self.selection_repository = user_selection_repository

        logger.info("NLSearchService åˆå§‹åŒ–å®Œæˆ (MongoDB)")

    async def create_search(
        self,
        query_text: str,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºè‡ªç„¶è¯­è¨€æœç´¢

        æµç¨‹:
        1. éªŒè¯è¾“å…¥
        2. åˆ›å»ºæœç´¢è®°å½•
        3. LLMè§£ææŸ¥è¯¢
        4. æ›´æ–°åˆ†æç»“æœ
        5. ç²¾ç‚¼æŸ¥è¯¢
        6. æ‰§è¡Œæœç´¢
        7. è¿”å›ç»“æœ

        Args:
            query_text: ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸:
            {
                "log_id": int,
                "query_text": str,
                "analysis": dict,
                "refined_query": str,
                "results": list,
                "created_at": datetime
            }

        Raises:
            ValueError: è¾“å…¥éªŒè¯å¤±è´¥
            Exception: æœç´¢è¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯
        """
        # 1. éªŒè¯è¾“å…¥
        if not query_text or not query_text.strip():
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        query_text = query_text.strip()
        logger.info(f"å¼€å§‹å¤„ç†è‡ªç„¶è¯­è¨€æœç´¢: {query_text[:50]}...")

        try:
            # 2. åˆ›å»ºæœç´¢è®°å½•
            log_id = await self.repository.create(
                query_text=query_text,
                llm_analysis=None
            )
            logger.info(f"åˆ›å»ºæœç´¢è®°å½•: log_id={log_id}")

            # 3. LLMè§£ææŸ¥è¯¢
            logger.info("è°ƒç”¨LLMè§£ææŸ¥è¯¢...")
            analysis = await self.llm_processor.parse_query(query_text)
            logger.info(f"LLMè§£æå®Œæˆ: intent={analysis.get('intent')}, "
                       f"keywords={analysis.get('keywords')}")

            # 4. æ›´æ–°åˆ†æç»“æœ
            await self.repository.update_llm_analysis(
                log_id=log_id,
                llm_analysis=analysis
            )
            logger.info("åˆ†æç»“æœå·²ä¿å­˜")

            # 5. ç²¾ç‚¼æŸ¥è¯¢
            refined_query = await self.llm_processor.refine_query(query_text)
            logger.info(f"ç²¾ç‚¼åçš„æŸ¥è¯¢: {refined_query}")

            # 6. æ‰§è¡Œæœç´¢
            logger.info("å¼€å§‹æ‰§è¡Œæœç´¢...")
            search_results = await self.gpt5_adapter.search(
                query=refined_query,
                max_results=nl_search_config.max_results_per_query
            )
            logger.info(f"æœç´¢å®Œæˆ: è·å¾—{len(search_results)}ä¸ªç»“æœ")

            # ğŸ†• 7. ä¿å­˜æœç´¢ç»“æœåˆ°æ•°æ®åº“
            results_dict = [r.to_dict() for r in search_results]
            await self.repository.update_search_results(
                log_id=log_id,
                search_results=results_dict,
                results_count=len(search_results)
            )
            logger.info(f"æœç´¢ç»“æœå·²ä¿å­˜: log_id={log_id}")

            # 8. æ„å»ºè¿”å›ç»“æœ
            result = {
                "log_id": log_id,
                "query_text": query_text,
                "analysis": analysis,
                "refined_query": refined_query,
                "results": results_dict,
                "created_at": datetime.now().isoformat()
            }

            logger.info(f"æœç´¢æµç¨‹å®Œæˆ: log_id={log_id}")
            return result

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}", exc_info=True)
            # ä¸é‡æ–°æŠ›å‡ºï¼Œè®©APIå±‚å¤„ç†
            raise

    async def get_search_log(self, log_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æœç´¢è®°å½•

        Args:
            log_id: æœç´¢è®°å½•IDï¼ˆé›ªèŠ±ç®—æ³•IDå­—ç¬¦ä¸²ï¼‰

        Returns:
            æœç´¢è®°å½•å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        logger.info(f"è·å–æœç´¢è®°å½•: log_id={log_id}")

        try:
            log = await self.repository.get_by_id(log_id)

            if not log:
                logger.warning(f"æœç´¢è®°å½•ä¸å­˜åœ¨: log_id={log_id}")
                return None

            return {
                "log_id": log["_id"],
                "query_text": log["query_text"],
                "analysis": log.get("llm_analysis"),
                "created_at": log["created_at"].isoformat() if log.get("created_at") else None
            }

        except Exception as e:
            logger.error(f"è·å–æœç´¢è®°å½•å¤±è´¥: {e}", exc_info=True)
            raise

    async def list_search_logs(
        self,
        limit: int = 10,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæœç´¢å†å²

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶
            offset: åˆ†é¡µåç§»é‡

        Returns:
            æœç´¢è®°å½•åˆ—è¡¨
        """
        logger.info(f"æŸ¥è¯¢æœç´¢å†å²: limit={limit}, offset={offset}")

        try:
            logs = await self.repository.get_recent(limit=limit, offset=offset)

            results = [
                {
                    "log_id": log["_id"],
                    "query_text": log["query_text"],
                    "analysis": log.get("llm_analysis"),
                    "created_at": log["created_at"].isoformat() if log.get("created_at") else None
                }
                for log in logs
            ]

            logger.info(f"è¿”å›{len(results)}æ¡æœç´¢è®°å½•")
            return results

        except Exception as e:
            logger.error(f"æŸ¥è¯¢æœç´¢å†å²å¤±è´¥: {e}", exc_info=True)
            raise

    async def search_by_keyword(
        self,
        keyword: str,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢å†å²è®°å½•

        Args:
            keyword: æœç´¢å…³é”®è¯
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            åŒ¹é…çš„æœç´¢è®°å½•åˆ—è¡¨
        """
        logger.info(f"æ ¹æ®å…³é”®è¯æœç´¢: keyword={keyword}")

        try:
            logs = await self.repository.search_by_keyword(
                keyword=keyword,
                limit=limit
            )

            results = [
                {
                    "log_id": log["_id"],
                    "query_text": log["query_text"],
                    "analysis": log.get("llm_analysis"),
                    "created_at": log["created_at"].isoformat() if log.get("created_at") else None
                }
                for log in logs
            ]

            logger.info(f"æ‰¾åˆ°{len(results)}æ¡åŒ¹é…è®°å½•")
            return results

        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_service_status(self) -> Dict[str, Any]:
        """
        è·å–æœåŠ¡çŠ¶æ€

        Returns:
            æœåŠ¡çŠ¶æ€ä¿¡æ¯
        """
        return {
            "enabled": nl_search_config.enabled,
            "llm_configured": bool(self.llm_processor.client),
            "search_configured": bool(self.gpt5_adapter.api_key or self.gpt5_adapter.test_mode),
            "test_mode": self.gpt5_adapter.test_mode,
            "version": "1.0.0-beta"
        }

    async def get_search_results(
        self,
        log_id: str,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–æœç´¢ç»“æœ

        Args:
            log_id: æœç´¢è®°å½•IDï¼ˆé›ªèŠ±ç®—æ³•IDå­—ç¬¦ä¸²ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆå¯é€‰ï¼‰
            offset: åˆ†é¡µåç§»é‡ï¼ˆé»˜è®¤ 0ï¼‰

        Returns:
            Optional[Dict]: æœç´¢ç»“æœæ•°æ®ï¼Œä¸å­˜åœ¨æ—¶è¿”å› None

        Example:
            >>> result = await service.get_search_results("248728141926559744")
            >>> print(f"å…± {result['total_count']} æ¡ç»“æœ")
            >>> for item in result['results']:
            ...     print(item['title'])
        """
        logger.info(f"è·å–æœç´¢ç»“æœ: log_id={log_id}")

        try:
            # 1. è·å–æœç´¢è®°å½•ï¼ˆåŒ…å«åŸºæœ¬ä¿¡æ¯ï¼‰
            log = await self.repository.get_by_id(log_id)
            if not log:
                logger.warning(f"æœç´¢è®°å½•ä¸å­˜åœ¨: log_id={log_id}")
                return None

            # 2. è·å–æœç´¢ç»“æœ
            search_results = await self.repository.get_search_results(log_id)
            if search_results is None:
                logger.warning(f"æœç´¢ç»“æœä¸å­˜åœ¨: log_id={log_id}")
                return None

            # 3. åˆ†é¡µå¤„ç†
            total_count = len(search_results)
            if limit is not None:
                search_results = search_results[offset:offset + limit]

            # 4. æ„å»ºå“åº”
            return {
                "log_id": log_id,
                "query_text": log["query_text"],
                "total_count": total_count,
                "results": search_results,
                "llm_analysis": log.get("llm_analysis"),
                "status": log.get("status", "completed"),
                "created_at": log["created_at"].isoformat() if log.get("created_at") else None
            }

        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»“æœå¤±è´¥: {e}", exc_info=True)
            raise

    async def record_user_selection(
        self,
        log_id: str,
        result_url: str,
        action_type: str,
        user_id: Optional[str] = None,
        user_agent: Optional[str] = None,
        ip_address: Optional[str] = None
    ) -> str:
        """
        è®°å½•ç”¨æˆ·é€‰æ‹©äº‹ä»¶

        Args:
            log_id: æœç´¢è®°å½•ID
            result_url: é€‰ä¸­çš„ç»“æœURL
            action_type: æ“ä½œç±»å‹ï¼ˆclick, bookmark, archiveï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰
            ip_address: å®¢æˆ·ç«¯IPåœ°å€ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: äº‹ä»¶ID

        Raises:
            ValueError: æœç´¢è®°å½•ä¸å­˜åœ¨

        Example:
            >>> event_id = await service.record_user_selection(
            ...     log_id="248728141926559744",
            ...     result_url="https://example.com/gpt5",
            ...     action_type="click"
            ... )
        """
        logger.info(
            f"è®°å½•ç”¨æˆ·é€‰æ‹©: log_id={log_id}, "
            f"url={result_url}, action={action_type}"
        )

        try:
            # 1. éªŒè¯æœç´¢è®°å½•å­˜åœ¨
            log = await self.repository.get_by_id(log_id)
            if not log:
                raise ValueError(f"æœç´¢è®°å½•ä¸å­˜åœ¨: log_id={log_id}")

            # 2. åˆ›å»ºé€‰æ‹©äº‹ä»¶
            event_id = await self.selection_repository.create(
                log_id=log_id,
                result_url=result_url,
                action_type=action_type,
                user_id=user_id,
                user_agent=user_agent,
                ip_address=ip_address
            )

            logger.info(f"ç”¨æˆ·é€‰æ‹©å·²è®°å½•: event_id={event_id}")
            return event_id

        except Exception as e:
            logger.error(f"è®°å½•ç”¨æˆ·é€‰æ‹©å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_selection_statistics(
        self,
        log_id: str
    ) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·é€‰æ‹©ç»Ÿè®¡

        Args:
            log_id: æœç´¢è®°å½•ID

        Returns:
            Dict: ç»Ÿè®¡æ•°æ®

        Example:
            >>> stats = await service.get_selection_statistics("248728141926559744")
            >>> print(f"æ€»ç‚¹å‡»æ•°: {stats['total_clicks']}")
        """
        logger.info(f"è·å–é€‰æ‹©ç»Ÿè®¡: log_id={log_id}")

        try:
            # è·å–æ‰€æœ‰é€‰æ‹©äº‹ä»¶
            events = await self.selection_repository.get_by_log_id(log_id)

            # ç»Ÿè®¡æ•°æ®
            total_count = len(events)
            click_count = sum(1 for e in events if e["action_type"] == "click")
            bookmark_count = sum(1 for e in events if e["action_type"] == "bookmark")
            archive_count = sum(1 for e in events if e["action_type"] == "archive")

            # ç»Ÿè®¡ URL ç‚¹å‡»æ¬¡æ•°
            url_clicks = {}
            for event in events:
                url = event["result_url"]
                url_clicks[url] = url_clicks.get(url, 0) + 1

            return {
                "log_id": log_id,
                "total_count": total_count,
                "click_count": click_count,
                "bookmark_count": bookmark_count,
                "archive_count": archive_count,
                "top_urls": sorted(
                    url_clicks.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]  # å‰5ä¸ªæœ€çƒ­é—¨URL
            }

        except Exception as e:
            logger.error(f"è·å–é€‰æ‹©ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
nl_search_service = NLSearchService()
