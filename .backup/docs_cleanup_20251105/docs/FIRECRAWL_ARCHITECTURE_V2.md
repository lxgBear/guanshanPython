# Firecrawl æ¨¡å—åŒ–æ¶æ„æ–‡æ¡£ v2.0.0

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
3. [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
4. [ä»»åŠ¡ç±»å‹](#ä»»åŠ¡ç±»å‹)
5. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
6. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
7. [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)
8. [è¿ç§»æŒ‡å—](#è¿ç§»æŒ‡å—)

---

## æ¦‚è¿°

### è®¾è®¡ç›®æ ‡

Firecrawl v2.0.0 é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„é‡æ„ï¼Œå®ç°ä»¥ä¸‹ç›®æ ‡ï¼š

- âœ… **èŒè´£åˆ†ç¦»**ï¼šæ¯ä¸ªæ‰§è¡Œå™¨ä¸“æ³¨äºå•ä¸€ä»»åŠ¡ç±»å‹
- âœ… **é«˜å¯ç»´æŠ¤æ€§**ï¼šæ¸…æ™°çš„ä»£ç ç»„ç»‡å’Œä¾èµ–å…³ç³»
- âœ… **æ˜“æ‰©å±•æ€§**ï¼šé€šè¿‡å·¥å‚æ¨¡å¼è½»æ¾æ·»åŠ æ–°æ‰§è¡Œå™¨
- âœ… **ç±»å‹å®‰å…¨**ï¼šé…ç½®ç±»å’Œæ¥å£æä¾›ç±»å‹æ£€æŸ¥
- âœ… **æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„æ‰§è¡Œæµç¨‹å’Œé”™è¯¯å¤„ç†

### æ ¸å¿ƒæ”¹è¿›

| æ–¹é¢ | v1.x (æ—§ç‰ˆæœ¬) | v2.0.0 (æ–°ç‰ˆæœ¬) |
|------|---------------|-----------------|
| **ä»£ç ç»„ç»‡** | é›†ä¸­åœ¨ TaskScheduler | æ¨¡å—åŒ–åˆ†ç¦» |
| **ä»»åŠ¡ç±»å‹** | éšå¼åˆ¤æ–­ (crawl_url vs query) | æ˜¾å¼æšä¸¾ (TaskType) |
| **æ‰§è¡Œå™¨** | å†…è”æ–¹æ³• | ç‹¬ç«‹æ‰§è¡Œå™¨ç±» |
| **é…ç½®ç®¡ç†** | å­—å…¸é…ç½® | ç±»å‹å®‰å…¨çš„é…ç½®ç±» |
| **æ‰©å±•æ€§** | ä¿®æ”¹æ ¸å¿ƒä»£ç  | æ³¨å†Œæ–°æ‰§è¡Œå™¨ |
| **å¯æµ‹è¯•æ€§** | éš¾ä»¥å•å…ƒæµ‹è¯• | æ¯ä¸ªæ‰§è¡Œå™¨ç‹¬ç«‹æµ‹è¯• |

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TaskScheduler                          â”‚
â”‚  (è°ƒåº¦å™¨ - è´Ÿè´£ä»»åŠ¡è°ƒåº¦å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ å§”æ‰˜æ‰§è¡Œ
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ExecutorFactory                           â”‚
â”‚  (å·¥å‚ - æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºå¯¹åº”æ‰§è¡Œå™¨)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚               â”‚
              â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Search    â”‚  â”‚    Crawl    â”‚  â”‚   Scrape    â”‚
    â”‚  Executor   â”‚  â”‚  Executor   â”‚  â”‚  Executor   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
           â”‚ ä½¿ç”¨           â”‚ ä½¿ç”¨           â”‚ ä½¿ç”¨
           â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          Firecrawl Adapters                 â”‚
    â”‚  - FirecrawlSearchAdapter (Search API)      â”‚
    â”‚  - FirecrawlAdapter (Scrape/Crawl API)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®¾è®¡æ¨¡å¼

#### 1. ç­–ç•¥æ¨¡å¼ (Strategy Pattern)

ä¸åŒçš„æ‰§è¡Œå™¨å®ç°ç›¸åŒçš„æ¥å£ï¼Œæä¾›ä¸åŒçš„æ‰§è¡Œç­–ç•¥ï¼š

```python
class TaskExecutor(ABC):
    @abstractmethod
    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œä»»åŠ¡çš„ç­–ç•¥æ–¹æ³•"""
        pass
```

#### 2. å·¥å‚æ¨¡å¼ (Factory Pattern)

æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€åˆ›å»ºæ‰§è¡Œå™¨å®ä¾‹ï¼š

```python
executor = ExecutorFactory.create(task_type)
result = await executor.execute(task)
```

#### 3. æ¨¡æ¿æ–¹æ³•æ¨¡å¼ (Template Method Pattern)

åŸºç±»å®šä¹‰é€šç”¨çš„æ‰§è¡Œæµç¨‹ï¼Œå­ç±»å®ç°å…·ä½“ç»†èŠ‚ï¼š

```python
class TaskExecutor(ABC):
    async def execute(self, task: SearchTask):
        # 1. éªŒè¯é…ç½®
        self.validate_config(task)
        # 2. æ‰§è¡Œä»»åŠ¡ï¼ˆå­ç±»å®ç°ï¼‰
        # 3. è®°å½•æ—¥å¿—
        # 4. è¿”å›ç»“æœ
```

#### 4. é€‚é…å™¨æ¨¡å¼ (Adapter Pattern)

å°è£… Firecrawl API è°ƒç”¨ï¼Œæä¾›ç»Ÿä¸€æ¥å£ï¼š

```python
class FirecrawlAdapter:
    async def scrape(self, url: str, **options) -> CrawlResult:
        """é€‚é… Firecrawl Scrape API"""
        pass

    async def crawl(self, url: str, **options) -> List[CrawlResult]:
        """é€‚é… Firecrawl Crawl API"""
        pass
```

---

## æ ¸å¿ƒç»„ä»¶

### 1. TaskExecutor (åŸºç±»)

**ä½ç½®**: `src/services/firecrawl/base.py`

**èŒè´£**:
- å®šä¹‰æ‰§è¡Œå™¨æ¥å£
- æä¾›é€šç”¨è¾…åŠ©æ–¹æ³•
- ç»Ÿä¸€å¼‚å¸¸å¤„ç†

**å…³é”®æ–¹æ³•**:

```python
class TaskExecutor(ABC):
    @abstractmethod
    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œä»»åŠ¡å¹¶è¿”å›ç»“æœæ‰¹æ¬¡"""
        pass

    @abstractmethod
    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®"""
        pass

    def _create_result_batch(self, task: SearchTask, query: str) -> SearchResultBatch:
        """åˆ›å»ºç»“æœæ‰¹æ¬¡å¯¹è±¡"""
        pass
```

### 2. SearchExecutor (å…³é”®è¯æœç´¢æ‰§è¡Œå™¨)

**ä½ç½®**: `src/services/firecrawl/executors/search_executor.py`

**èŒè´£**:
- æ‰§è¡Œå…³é”®è¯æœç´¢ (Search API)
- æ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µ (Scrape API)
- å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†

**å·¥ä½œæµç¨‹**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Search API                                        â”‚
â”‚  - è¾“å…¥: å…³é”®è¯ (query)                                   â”‚
â”‚  - è¾“å‡º: æœç´¢ç»“æœåˆ—è¡¨ (æ ‡é¢˜ã€URLã€æ‘˜è¦)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: Scrape API (æ‰¹é‡å¹¶å‘)                             â”‚
â”‚  - è¾“å…¥: æœç´¢ç»“æœä¸­çš„ URL åˆ—è¡¨                            â”‚
â”‚  - è¾“å‡º: æ¯ä¸ª URL çš„å®Œæ•´å†…å®¹ (æ­£æ–‡ã€Markdownã€HTML)      â”‚
â”‚  - ç‰¹æ€§:                                                  â”‚
â”‚    â€¢ å¹¶å‘æ§åˆ¶ (Semaphore)                                â”‚
â”‚    â€¢ éƒ¨åˆ†å¤±è´¥å®¹å¿                                         â”‚
â”‚    â€¢ å¯é…ç½®å»¶è¿Ÿ                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®é…ç½®**:

```python
SearchConfig(
    limit=10,                     # æœç´¢ç»“æœæ•°é‡
    language="zh",                # æœç´¢è¯­è¨€
    enable_detail_scrape=True,    # æ˜¯å¦å¯ç”¨è¯¦æƒ…é¡µçˆ¬å–
    max_concurrent_scrapes=3,     # æœ€å¤§å¹¶å‘æ•°
    scrape_delay=1.0              # çˆ¬å–é—´éš”ï¼ˆç§’ï¼‰
)
```

### 3. CrawlExecutor (ç½‘ç«™çˆ¬å–æ‰§è¡Œå™¨)

**ä½ç½®**: `src/services/firecrawl/executors/crawl_executor.py`

**èŒè´£**:
- é€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™ (Crawl API)
- æ·±åº¦æ§åˆ¶å’Œè·¯å¾„è¿‡æ»¤
- å¤§é‡é¡µé¢å¤„ç†

**å·¥ä½œæµç¨‹**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawl API (å¼‚æ­¥é€’å½’çˆ¬å–)                                 â”‚
â”‚  - è¾“å…¥: èµ·å§‹ URL                                         â”‚
â”‚  - é€’å½’çˆ¬å–æ‰€æœ‰å­é¡µé¢                                     â”‚
â”‚  - åº”ç”¨è·¯å¾„è¿‡æ»¤å’Œæ·±åº¦é™åˆ¶                                 â”‚
â”‚  - è¾“å‡º: æ‰€æœ‰é¡µé¢çš„å®Œæ•´å†…å®¹                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®é…ç½®**:

```python
CrawlConfig(
    limit=100,                    # æœ€å¤§é¡µé¢æ•°
    max_depth=3,                  # æœ€å¤§çˆ¬å–æ·±åº¦
    include_paths=["/blog/*"],    # åŒ…å«çš„è·¯å¾„æ¨¡å¼
    exclude_paths=["/admin/*"],   # æ’é™¤çš„è·¯å¾„æ¨¡å¼
    allow_backward_links=False    # æ˜¯å¦å…è®¸å‘åé“¾æ¥
)
```

### 4. ScrapeExecutor (å•é¡µé¢çˆ¬å–æ‰§è¡Œå™¨)

**ä½ç½®**: `src/services/firecrawl/executors/scrape_executor.py`

**èŒè´£**:
- çˆ¬å–å•ä¸ªé¡µé¢å†…å®¹ (Scrape API)
- é€‚ç”¨äºå®šæœŸç›‘æ§ç‰¹å®šé¡µé¢

**å·¥ä½œæµç¨‹**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrape API (å•é¡µé¢çˆ¬å–)                                  â”‚
â”‚  - è¾“å…¥: å•ä¸ª URL                                         â”‚
â”‚  - è¾“å‡º: é¡µé¢å®Œæ•´å†…å®¹                                     â”‚
â”‚  - ç”¨é€”: å®šæœŸç›‘æ§ç‰¹å®šé¡µé¢å˜åŒ–                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®é…ç½®**:

```python
ScrapeConfig(
    only_main_content=True,       # åªæå–ä¸»è¦å†…å®¹
    wait_for=1000,                # ç­‰å¾…é¡µé¢åŠ è½½æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    exclude_tags=["nav", "footer"] # æ’é™¤çš„HTMLæ ‡ç­¾
)
```

### 5. ExecutorFactory (æ‰§è¡Œå™¨å·¥å‚)

**ä½ç½®**: `src/services/firecrawl/factory.py`

**èŒè´£**:
- æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºæ‰§è¡Œå™¨
- æ”¯æŒè‡ªå®šä¹‰æ‰§è¡Œå™¨æ³¨å†Œ
- éªŒè¯æ‰§è¡Œå™¨ç±»å‹

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# åˆ›å»ºæ‰§è¡Œå™¨
executor = ExecutorFactory.create(TaskType.SEARCH_KEYWORD)

# ä»å­—ç¬¦ä¸²åˆ›å»ºï¼ˆå…¼å®¹æ€§ï¼‰
executor = ExecutorFactory.create_from_string("search_keyword")

# æ³¨å†Œè‡ªå®šä¹‰æ‰§è¡Œå™¨
ExecutorFactory.register_executor(
    task_type=TaskType.CUSTOM,
    executor_class=MyCustomExecutor
)

# è·å–æ”¯æŒçš„ç±»å‹
supported = ExecutorFactory.get_supported_types()
```

---

## ä»»åŠ¡ç±»å‹

### TaskType æšä¸¾

**ä½ç½®**: `src/core/domain/entities/search_task.py`

```python
class TaskType(str, Enum):
    SEARCH_KEYWORD = "search_keyword"  # å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–
    CRAWL_WEBSITE = "crawl_website"    # ç½‘ç«™é€’å½’çˆ¬å–
    SCRAPE_URL = "scrape_url"          # å•é¡µé¢çˆ¬å–
```

### ä»»åŠ¡ç±»å‹å¯¹æ¯”

| ä»»åŠ¡ç±»å‹ | è¾“å…¥ | APIä½¿ç”¨ | è¾“å‡º | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|------|----------|
| **SEARCH_KEYWORD** | å…³é”®è¯ | Search + Scrape | æœç´¢ç»“æœ + è¯¦æƒ…é¡µ | è¡Œä¸šèµ„è®¯ã€ç«å“åˆ†æ |
| **CRAWL_WEBSITE** | èµ·å§‹URL | Crawl | æ•´ç«™é¡µé¢å†…å®¹ | ç½‘ç«™å½’æ¡£ã€çŸ¥è¯†åº“æ„å»º |
| **SCRAPE_URL** | å•ä¸ªURL | Scrape | å•é¡µé¢å†…å®¹ | é¡µé¢ç›‘æ§ã€æ•°æ®æ›´æ–° |

### ä»»åŠ¡ç±»å‹åˆ¤æ–­é€»è¾‘

**SearchTask å®ä½“**æä¾›è‡ªåŠ¨åˆ¤æ–­æ–¹æ³•ï¼š

```python
class SearchTask:
    def get_task_type(self) -> TaskType:
        """è‡ªåŠ¨åˆ¤æ–­ä»»åŠ¡ç±»å‹"""
        # ä¼˜å…ˆçº§: task_typeå­—æ®µ > crawl_url > query
        if self.task_type:
            return TaskType(self.task_type)
        elif self.crawl_url:
            return TaskType.SCRAPE_URL
        else:
            return TaskType.SEARCH_KEYWORD

    def is_search_keyword_mode(self) -> bool:
        return self.get_task_type() == TaskType.SEARCH_KEYWORD

    def is_crawl_website_mode(self) -> bool:
        return self.get_task_type() == TaskType.CRAWL_WEBSITE

    def is_scrape_url_mode(self) -> bool:
        return self.get_task_type() == TaskType.SCRAPE_URL
```

---

## ä½¿ç”¨æŒ‡å—

### åˆ›å»ºæœç´¢ä»»åŠ¡

#### 1. å…³é”®è¯æœç´¢ä»»åŠ¡

```python
from src.core.domain.entities.search_task import SearchTask, TaskType

task = SearchTask(
    name="Python æœ€æ–°æŠ€æœ¯åŠ¨æ€",
    query="Python 3.12 æ–°ç‰¹æ€§",
    task_type=TaskType.SEARCH_KEYWORD,  # æ˜ç¡®æŒ‡å®šä»»åŠ¡ç±»å‹
    search_config={
        "limit": 10,
        "language": "zh",
        "enable_detail_scrape": True,
        "max_concurrent_scrapes": 3,
        "scrape_delay": 1.0
    }
)
```

#### 2. ç½‘ç«™çˆ¬å–ä»»åŠ¡

```python
task = SearchTask(
    name="æŠ€æœ¯åšå®¢å½’æ¡£",
    crawl_url="https://example.com/blog",
    task_type=TaskType.CRAWL_WEBSITE,  # æ˜ç¡®æŒ‡å®šä»»åŠ¡ç±»å‹
    crawl_config={
        "limit": 100,
        "max_depth": 3,
        "include_paths": ["/blog/*", "/articles/*"],
        "exclude_paths": ["/admin/*"],
        "only_main_content": True
    }
)
```

#### 3. å•é¡µé¢çˆ¬å–ä»»åŠ¡

```python
task = SearchTask(
    name="é¦–é¡µç›‘æ§",
    crawl_url="https://example.com",
    task_type=TaskType.SCRAPE_URL,  # æ˜ç¡®æŒ‡å®šä»»åŠ¡ç±»å‹
    search_config={  # æ³¨æ„ï¼šScrapeExecutor ä½¿ç”¨ search_config
        "only_main_content": True,
        "wait_for": 2000,
        "exclude_tags": ["nav", "footer", "header"]
    }
)
```

### æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡

```python
from src.services.firecrawl import ExecutorFactory

# åˆ›å»ºæ‰§è¡Œå™¨
task_type = task.get_task_type()
executor = ExecutorFactory.create(task_type)

# æ‰§è¡Œä»»åŠ¡
result_batch = await executor.execute(task)

# å¤„ç†ç»“æœ
for result in result_batch.results:
    print(f"æ ‡é¢˜: {result.title}")
    print(f"URL: {result.url}")
    print(f"å†…å®¹: {result.markdown_content[:200]}...")
```

### é€šè¿‡è°ƒåº¦å™¨æ‰§è¡Œ

```python
from src.services.task_scheduler import get_scheduler

# è·å–è°ƒåº¦å™¨å®ä¾‹
scheduler = await get_scheduler()

# å¯åŠ¨è°ƒåº¦å™¨
await scheduler.start()

# æ·»åŠ ä»»åŠ¡ï¼ˆä¼šè‡ªåŠ¨æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œï¼‰
await scheduler.add_task(task)

# ç«‹å³æ‰§è¡Œ
await scheduler.execute_task_now(str(task.id))
```

---

## é…ç½®è¯´æ˜

### SearchConfig (å…³é”®è¯æœç´¢é…ç½®)

```python
@dataclass
class SearchConfig:
    # æœç´¢å‚æ•°
    limit: int = 10                      # æœç´¢ç»“æœæ•°é‡
    language: str = "zh"                 # æœç´¢è¯­è¨€
    include_domains: Optional[List[str]] = None  # é™åˆ¶åŸŸå
    strict_language_filter: bool = True  # ä¸¥æ ¼è¯­è¨€è¿‡æ»¤

    # è¯¦æƒ…é¡µçˆ¬å–æ§åˆ¶
    enable_detail_scrape: bool = True    # æ˜¯å¦å¯ç”¨è¯¦æƒ…é¡µçˆ¬å–
    max_concurrent_scrapes: int = 3      # æœ€å¤§å¹¶å‘çˆ¬å–æ•°
    scrape_delay: float = 1.0            # çˆ¬å–é—´éš”ï¼ˆç§’ï¼‰

    # Scrape é€‰é¡¹ï¼ˆç”¨äºè¯¦æƒ…é¡µçˆ¬å–ï¼‰
    only_main_content: bool = True       # åªæå–ä¸»è¦å†…å®¹
    wait_for: int = 2000                 # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    exclude_tags: List[str] = field(     # æ’é™¤çš„HTMLæ ‡ç­¾
        default_factory=lambda: ["nav", "footer", "header", "aside"]
    )
    timeout: int = 90                    # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
```

### CrawlConfig (ç½‘ç«™çˆ¬å–é…ç½®)

```python
@dataclass
class CrawlConfig:
    # çˆ¬å–é™åˆ¶
    limit: int = 100                     # æœ€å¤§é¡µé¢æ•°
    max_depth: int = 3                   # æœ€å¤§çˆ¬å–æ·±åº¦

    # è·¯å¾„è¿‡æ»¤
    include_paths: List[str] = field(default_factory=list)  # åŒ…å«è·¯å¾„
    exclude_paths: List[str] = field(default_factory=list)  # æ’é™¤è·¯å¾„
    allow_backward_links: bool = False   # æ˜¯å¦å…è®¸å‘åé“¾æ¥

    # Scrape é€‰é¡¹ï¼ˆç”¨äºæ¯ä¸ªçˆ¬å–çš„é¡µé¢ï¼‰
    only_main_content: bool = True       # åªæå–ä¸»è¦å†…å®¹
    wait_for: int = 1000                 # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    exclude_tags: List[str] = field(     # æ’é™¤çš„HTMLæ ‡ç­¾
        default_factory=lambda: ["nav", "footer", "header"]
    )

    # è¶…æ—¶è®¾ç½®
    timeout: int = 300                   # æ•´ä½“çˆ¬å–è¶…æ—¶ï¼ˆç§’ï¼‰
    poll_interval: int = 10              # çŠ¶æ€è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
```

### ScrapeConfig (å•é¡µé¢çˆ¬å–é…ç½®)

```python
@dataclass
class ScrapeConfig:
    # å†…å®¹æå–
    only_main_content: bool = True       # åªæå–ä¸»è¦å†…å®¹
    wait_for: int = 1000                 # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    # æ ‡ç­¾è¿‡æ»¤
    include_tags: Optional[List[str]] = None  # åŒ…å«çš„HTMLæ ‡ç­¾
    exclude_tags: List[str] = field(     # æ’é™¤çš„HTMLæ ‡ç­¾
        default_factory=lambda: ["nav", "footer", "header"]
    )

    # è¶…æ—¶è®¾ç½®
    timeout: int = 90                    # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
```

### é…ç½®åˆ›å»ºæ–¹æ³•

```python
from src.services.firecrawl.config import ConfigFactory

# ä»å­—å…¸åˆ›å»ºé…ç½®
search_config = ConfigFactory.create_search_config({
    "limit": 20,
    "language": "en",
    "enable_detail_scrape": True
})

crawl_config = ConfigFactory.create_crawl_config({
    "limit": 50,
    "max_depth": 2,
    "include_paths": ["/blog/*"]
})

scrape_config = ConfigFactory.create_scrape_config({
    "only_main_content": True,
    "timeout": 60
})
```

---

## æ‰©å±•å¼€å‘

### æ·»åŠ è‡ªå®šä¹‰æ‰§è¡Œå™¨

#### 1. åˆ›å»ºæ‰§è¡Œå™¨ç±»

```python
from src.services.firecrawl.base import TaskExecutor, ExecutionError
from src.core.domain.entities.search_result import SearchResultBatch

class MyCustomExecutor(TaskExecutor):
    """è‡ªå®šä¹‰æ‰§è¡Œå™¨"""

    def validate_config(self, task: SearchTask) -> bool:
        """éªŒè¯ä»»åŠ¡é…ç½®"""
        if not task.custom_field:
            self.logger.error("ç¼ºå°‘å¿…è¦çš„ custom_field")
            return False
        return True

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œè‡ªå®šä¹‰ä»»åŠ¡"""
        start_time = datetime.utcnow()
        self._log_execution_start(task)

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"é…ç½®æ— æ•ˆ: {task.id}")

        # 2. æ‰§è¡Œè‡ªå®šä¹‰é€»è¾‘
        try:
            # ä½ çš„è‡ªå®šä¹‰é€»è¾‘
            results = await self._my_custom_logic(task)

            # 3. åˆ›å»ºç»“æœæ‰¹æ¬¡
            batch = self._create_result_batch(task, query="è‡ªå®šä¹‰ä»»åŠ¡")
            for result in results:
                batch.add_result(result)

            # 4. è®°å½•æ‰§è¡Œæ—¶é—´
            end_time = datetime.utcnow()
            batch.execution_time_ms = int(
                (end_time - start_time).total_seconds() * 1000
            )

            self._log_execution_end(task, len(results), batch.execution_time_ms)
            return batch

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
            raise ExecutionError(f"è‡ªå®šä¹‰ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")

    async def _my_custom_logic(self, task: SearchTask):
        """ä½ çš„è‡ªå®šä¹‰é€»è¾‘"""
        # å®ç°ä½ çš„é€»è¾‘
        pass
```

#### 2. æ³¨å†Œæ‰§è¡Œå™¨

```python
from src.services.firecrawl import ExecutorFactory
from src.core.domain.entities.search_task import TaskType

# æ·»åŠ æ–°çš„ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
class TaskType(str, Enum):
    SEARCH_KEYWORD = "search_keyword"
    CRAWL_WEBSITE = "crawl_website"
    SCRAPE_URL = "scrape_url"
    CUSTOM = "custom"  # æ–°å¢

# æ³¨å†Œæ‰§è¡Œå™¨
ExecutorFactory.register_executor(
    task_type=TaskType.CUSTOM,
    executor_class=MyCustomExecutor
)
```

#### 3. ä½¿ç”¨è‡ªå®šä¹‰æ‰§è¡Œå™¨

```python
# åˆ›å»ºä»»åŠ¡
task = SearchTask(
    name="è‡ªå®šä¹‰ä»»åŠ¡",
    task_type=TaskType.CUSTOM,
    custom_field="some_value"
)

# æ‰§è¡Œ
executor = ExecutorFactory.create(TaskType.CUSTOM)
result = await executor.execute(task)
```

---

## è¿ç§»æŒ‡å—

### ä» v1.x è¿ç§»åˆ° v2.0.0

#### 1. æ•°æ®åº“å…¼å®¹æ€§

**å‘åå…¼å®¹**: v2.0.0 å®Œå…¨å…¼å®¹ç°æœ‰æ•°æ®åº“ä¸­çš„ä»»åŠ¡æ•°æ®ã€‚

- æ—§ä»»åŠ¡ï¼ˆæ²¡æœ‰ `task_type` å­—æ®µï¼‰ä¼šè‡ªåŠ¨åˆ¤æ–­ç±»å‹ï¼š
  - æœ‰ `crawl_url` â†’ `SCRAPE_URL`
  - æœ‰ `query` â†’ `SEARCH_KEYWORD`

- æ–°ä»»åŠ¡åº”æ˜ç¡®æŒ‡å®š `task_type` å­—æ®µ

#### 2. API ç«¯ç‚¹æ›´æ–°

**æ·»åŠ ä»»åŠ¡æ—¶æŒ‡å®šç±»å‹**:

```python
# æ—§æ–¹å¼ï¼ˆä»ç„¶å…¼å®¹ï¼‰
task_data = {
    "name": "æµ‹è¯•ä»»åŠ¡",
    "query": "Python",
    "crawl_url": None
}

# æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
task_data = {
    "name": "æµ‹è¯•ä»»åŠ¡",
    "query": "Python",
    "task_type": "search_keyword"  # æ˜ç¡®æŒ‡å®šç±»å‹
}
```

#### 3. é…ç½®è¿ç§»

**search_config vs crawl_config**:

```python
# æ—§æ–¹å¼ï¼šæ‰€æœ‰é…ç½®æ”¾åœ¨ search_config
task = SearchTask(
    crawl_url="https://example.com",
    search_config={
        "only_main_content": True,
        "wait_for": 2000
    }
)

# æ–°æ–¹å¼ï¼šä½¿ç”¨ä¸“ç”¨é…ç½®å­—æ®µ
task = SearchTask(
    crawl_url="https://example.com",
    task_type=TaskType.CRAWL_WEBSITE,
    crawl_config={  # ä½¿ç”¨ crawl_config
        "limit": 100,
        "max_depth": 3
    }
)
```

#### 4. ä»£ç è¿ç§»

**è°ƒåº¦å™¨ä»£ç æ— éœ€ä¿®æ”¹**:

v2.0.0 çš„ TaskScheduler å·²ç»å®Œå…¨é›†æˆæ–°æ¶æ„ï¼Œç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨ã€‚

```python
# è¿™æ®µä»£ç åœ¨ v1.x å’Œ v2.0.0 ä¸­éƒ½èƒ½æ­£å¸¸å·¥ä½œ
scheduler = await get_scheduler()
await scheduler.start()
await scheduler.add_task(task)
```

---

## æœ€ä½³å®è·µ

### 1. ä»»åŠ¡ç±»å‹é€‰æ‹©

- **å…³é”®è¯æœç´¢ (SEARCH_KEYWORD)**: éœ€è¦è·å–å¤šä¸ªæ¥æºçš„ä¿¡æ¯æ—¶
- **ç½‘ç«™çˆ¬å– (CRAWL_WEBSITE)**: éœ€è¦å½’æ¡£æ•´ä¸ªç½‘ç«™æ—¶
- **å•é¡µé¢çˆ¬å– (SCRAPE_URL)**: éœ€è¦ç›‘æ§ç‰¹å®šé¡µé¢å˜åŒ–æ—¶

### 2. é…ç½®ä¼˜åŒ–

**å…³é”®è¯æœç´¢ä¼˜åŒ–**:
```python
{
    "enable_detail_scrape": True,   # å¯ç”¨è¯¦æƒ…é¡µçˆ¬å–
    "max_concurrent_scrapes": 3,    # å¹³è¡¡é€Ÿåº¦å’Œèµ„æº
    "scrape_delay": 1.0,            # é¿å…è¯·æ±‚è¿‡å¿«
    "only_main_content": True       # å‡å°‘å™ªéŸ³
}
```

**ç½‘ç«™çˆ¬å–ä¼˜åŒ–**:
```python
{
    "limit": 50,                    # åˆç†çš„é¡µé¢é™åˆ¶
    "max_depth": 2,                 # é¿å…çˆ¬å–è¿‡æ·±
    "exclude_paths": ["/admin/*"],  # æ’é™¤ä¸ç›¸å…³è·¯å¾„
    "timeout": 300                  # è¶³å¤Ÿçš„è¶…æ—¶æ—¶é—´
}
```

### 3. é”™è¯¯å¤„ç†

æ‰€æœ‰æ‰§è¡Œå™¨éƒ½éµå¾ªç»Ÿä¸€çš„é”™è¯¯å¤„ç†ï¼š

```python
try:
    executor = ExecutorFactory.create(task_type)
    result = await executor.execute(task)
except ConfigValidationError as e:
    # é…ç½®éªŒè¯å¤±è´¥
    logger.error(f"é…ç½®é”™è¯¯: {e}")
except ExecutionError as e:
    # æ‰§è¡Œè¿‡ç¨‹é”™è¯¯
    logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
except Exception as e:
    # æœªé¢„æœŸçš„é”™è¯¯
    logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
```

### 4. æ€§èƒ½ç›‘æ§

```python
# æŸ¥çœ‹æ‰§è¡Œå™¨æ€§èƒ½
logger.info(f"æ‰§è¡Œæ—¶é—´: {result_batch.execution_time_ms}ms")
logger.info(f"ç»“æœæ•°é‡: {result_batch.returned_count}")
logger.info(f"ç§¯åˆ†æ¶ˆè€—: {result_batch.credits_used}")
```

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. ä»»åŠ¡ç±»å‹åˆ¤æ–­é”™è¯¯

**ç—‡çŠ¶**: ä»»åŠ¡ä½¿ç”¨äº†é”™è¯¯çš„æ‰§è¡Œå™¨

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ˜ç¡®æŒ‡å®š task_type
task.task_type = TaskType.SEARCH_KEYWORD

# æˆ–è€…ä½¿ç”¨è¾…åŠ©æ–¹æ³•éªŒè¯
print(f"ä»»åŠ¡ç±»å‹: {task.get_task_type()}")
```

#### 2. è¯¦æƒ…é¡µçˆ¬å–å¤±è´¥

**ç—‡çŠ¶**: SearchExecutor å®Œæˆæœç´¢ä½†è¯¦æƒ…é¡µå†…å®¹ä¸ºç©º

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥é…ç½®
config = {
    "enable_detail_scrape": True,     # ç¡®ä¿å¯ç”¨
    "max_concurrent_scrapes": 3,      # é™ä½å¹¶å‘æ•°
    "scrape_delay": 2.0,              # å¢åŠ å»¶è¿Ÿ
    "timeout": 120                    # å¢åŠ è¶…æ—¶
}
```

#### 3. ç½‘ç«™çˆ¬å–è¶…æ—¶

**ç—‡çŠ¶**: CrawlExecutor è¶…æ—¶å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# è°ƒæ•´é…ç½®
config = {
    "limit": 20,          # å‡å°‘é¡µé¢æ•°
    "max_depth": 2,       # å‡å°‘æ·±åº¦
    "timeout": 600,       # å¢åŠ è¶…æ—¶
    "poll_interval": 15   # å¢åŠ è½®è¯¢é—´éš”
}
```

---

## é™„å½•

### ç›®å½•ç»“æ„

```
src/services/firecrawl/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base.py                  # TaskExecutor åŸºç±»
â”œâ”€â”€ factory.py               # ExecutorFactory å·¥å‚ç±»
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ task_config.py       # é…ç½®ç±»å®šä¹‰
â””â”€â”€ executors/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ search_executor.py   # å…³é”®è¯æœç´¢æ‰§è¡Œå™¨
    â”œâ”€â”€ crawl_executor.py    # ç½‘ç«™çˆ¬å–æ‰§è¡Œå™¨
    â””â”€â”€ scrape_executor.py   # å•é¡µé¢çˆ¬å–æ‰§è¡Œå™¨
```

### ç›¸å…³æ–‡æ¡£

- [Firecrawl API æ–‡æ¡£](https://docs.firecrawl.dev/)
- [Firecrawl Crawl API](https://docs.firecrawl.dev/features/crawl)
- [Firecrawl Search API](https://docs.firecrawl.dev/features/search)
- [Firecrawl Scrape API](https://docs.firecrawl.dev/features/scrape)

### æ›´æ–°æ—¥å¿—

#### v2.0.0 (å½“å‰ç‰ˆæœ¬)

- âœ… æ¨¡å—åŒ–æ¶æ„é‡æ„
- âœ… æ–°å¢ TaskType æšä¸¾
- âœ… å®ç°ä¸‰ç§æ‰§è¡Œå™¨ï¼ˆSearch, Crawl, Scrapeï¼‰
- âœ… ç±»å‹å®‰å…¨çš„é…ç½®ç®¡ç†
- âœ… å·¥å‚æ¨¡å¼æ”¯æŒæ‰©å±•
- âœ… å‘åå…¼å®¹ v1.x æ•°æ®

#### v1.x (æ—§ç‰ˆæœ¬)

- é›†ä¸­å¼è°ƒåº¦å™¨å®ç°
- éšå¼ä»»åŠ¡ç±»å‹åˆ¤æ–­
- å­—å…¸é…ç½®ç®¡ç†

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0.0
**æœ€åæ›´æ–°**: 2025-01-XX
**ç»´æŠ¤è€…**: Development Team
