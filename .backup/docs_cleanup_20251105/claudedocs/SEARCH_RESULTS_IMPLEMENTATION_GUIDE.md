# æœç´¢ç»“æœèŒè´£åˆ†ç¦» - å®æ–½æŒ‡å—

**æ—¥æœŸ**: 2025-11-03
**ç‰ˆæœ¬**: v1.0.0
**èŒƒå›´**: å®šæ—¶ä»»åŠ¡ç³»ç»Ÿï¼ˆæ™ºèƒ½æœç´¢æš‚ä¸æ¶‰åŠï¼‰
**é¢„è®¡å·¥æœŸ**: 9å¤©
**é£é™©ç­‰çº§**: ä¸­ç­‰ï¼ˆæ¶‰åŠæ•°æ®æ¨¡å‹å˜æ›´å’Œè¿ç§»ï¼‰

---

## ğŸ“‹ å®æ–½æ¦‚è§ˆ

### æ€»ä½“ç­–ç•¥

æœ¬æ¬¡å®æ–½é‡‡ç”¨**æ¸è¿›å¼è¿ç§»**ç­–ç•¥ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ï¼š

1. **Phase 1-2**: æ–°åŠŸèƒ½å¼€å‘ï¼ˆå¹¶è¡Œè¿è¡Œï¼‰
2. **Phase 3**: APIé€‚é…ï¼ˆå…¼å®¹æ€§ä¿è¯ï¼‰
3. **Phase 4**: æ•°æ®è¿ç§»ï¼ˆå¯å›æ»šï¼‰
4. **Phase 5**: æµ‹è¯•éªŒè¯ï¼ˆå…¨é¢è¦†ç›–ï¼‰

### å…³é”®åŸåˆ™

- âœ… **å‘åå…¼å®¹**: ç°æœ‰APIç»§ç»­å·¥ä½œ
- âœ… **æ¸è¿›éƒ¨ç½²**: åˆ†é˜¶æ®µä¸Šçº¿
- âœ… **å¯å›æ»š**: æ¯ä¸ªé˜¶æ®µéƒ½å¯ä»¥å›æ»š
- âœ… **æ•°æ®å®‰å…¨**: è¿ç§»å‰å®Œæ•´å¤‡ä»½

---

## Phase 1: æ•°æ®æ¨¡å‹å’Œå®ä½“ï¼ˆDay 1-2ï¼‰

### Day 1: åˆ›å»ºæ–°å®ä½“ç±»

#### 1.1 åˆ›å»º ProcessedResult å®ä½“

**æ–‡ä»¶**: `src/core/domain/entities/processed_result.py`

```python
"""AIå¤„ç†ç»“æœå®ä½“æ¨¡å‹

v2.0.0 æ–°å¢ï¼š
- åˆ†ç¦»åŸå§‹æ•°æ®å’ŒAIå¤„ç†ç»“æœ
- æ”¯æŒAIå¤„ç†çŠ¶æ€ç®¡ç†
- æ”¯æŒç”¨æˆ·æ“ä½œï¼ˆç•™å­˜ã€åˆ é™¤ã€è¯„åˆ†ï¼‰
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Optional, Dict, Any, List

from src.infrastructure.id_generator import generate_string_id


class ProcessedStatus(Enum):
    """å¤„ç†ç»“æœçŠ¶æ€æšä¸¾"""
    PENDING = "pending"         # å¾…AIå¤„ç†
    PROCESSING = "processing"   # AIå¤„ç†ä¸­
    COMPLETED = "completed"     # AIå¤„ç†å®Œæˆ
    FAILED = "failed"           # AIå¤„ç†å¤±è´¥
    ARCHIVED = "archived"       # ç”¨æˆ·ç•™å­˜
    DELETED = "deleted"         # ç”¨æˆ·åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰


@dataclass
class ProcessedResult:
    """
    AIå¤„ç†ç»“æœå®ä½“

    èŒè´£ï¼š
    1. å­˜å‚¨AIåˆ†æã€ç¿»è¯‘ã€æ€»ç»“åçš„æ•°æ®
    2. ç®¡ç†ç”¨æˆ·æ“ä½œçŠ¶æ€ï¼ˆç•™å­˜ã€åˆ é™¤ï¼‰
    3. è®°å½•AIå¤„ç†å…ƒæ•°æ®

    v2.0.0 è®¾è®¡åŸåˆ™ï¼š
    - å…³è”åŸå§‹ç»“æœï¼ˆraw_result_idï¼‰
    - çŠ¶æ€é©±åŠ¨ï¼ˆProcessedStatusï¼‰
    - æ”¯æŒé‡è¯•æœºåˆ¶
    """
    # ä¸»é”®ï¼ˆé›ªèŠ±ç®—æ³•IDï¼‰
    id: str = field(default_factory=generate_string_id)

    # å…³è”åŸå§‹ç»“æœ
    raw_result_id: str = ""  # å…³è” search_results çš„ ID
    task_id: str = ""        # å…³è”çš„æœç´¢ä»»åŠ¡ID

    # AIå¤„ç†åçš„æ•°æ®
    translated_title: Optional[str] = None  # ç¿»è¯‘åçš„æ ‡é¢˜
    translated_content: Optional[str] = None  # ç¿»è¯‘åçš„å†…å®¹
    summary: Optional[str] = None  # AIç”Ÿæˆçš„æ‘˜è¦
    key_points: List[str] = field(default_factory=list)  # å…³é”®è¦ç‚¹
    sentiment: Optional[str] = None  # æƒ…æ„Ÿåˆ†æï¼ˆpositive/neutral/negativeï¼‰
    categories: List[str] = field(default_factory=list)  # AIåˆ†ç±»æ ‡ç­¾

    # AIå¤„ç†å…ƒæ•°æ®
    ai_model: Optional[str] = None  # ä½¿ç”¨çš„AIæ¨¡å‹ï¼ˆå¦‚ï¼šgpt-4ï¼‰
    ai_processing_time_ms: int = 0  # AIå¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    ai_confidence_score: float = 0.0  # AIç½®ä¿¡åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
    ai_metadata: Dict[str, Any] = field(default_factory=dict)  # AIé¢å¤–å…ƒæ•°æ®

    # ç”¨æˆ·æ“ä½œçŠ¶æ€
    status: ProcessedStatus = ProcessedStatus.PENDING
    user_rating: Optional[int] = None  # ç”¨æˆ·è¯„åˆ†ï¼ˆ1-5ï¼‰
    user_notes: Optional[str] = None  # ç”¨æˆ·å¤‡æ³¨

    # æ—¶é—´æˆ³
    created_at: datetime = field(default_factory=datetime.utcnow)
    processed_at: Optional[datetime] = None  # AIå¤„ç†å®Œæˆæ—¶é—´
    updated_at: datetime = field(default_factory=datetime.utcnow)

    # é”™è¯¯å¤„ç†
    processing_error: Optional[str] = None  # AIå¤„ç†é”™è¯¯ä¿¡æ¯
    retry_count: int = 0  # é‡è¯•æ¬¡æ•°

    def mark_as_processing(self) -> None:
        """æ ‡è®°ä¸ºAIå¤„ç†ä¸­"""
        self.status = ProcessedStatus.PROCESSING
        self.updated_at = datetime.utcnow()

    def mark_as_completed(self, ai_model: str, processing_time_ms: int) -> None:
        """æ ‡è®°ä¸ºAIå¤„ç†å®Œæˆ"""
        self.status = ProcessedStatus.COMPLETED
        self.processed_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()
        self.ai_model = ai_model
        self.ai_processing_time_ms = processing_time_ms

    def mark_as_failed(self, error_message: str) -> None:
        """æ ‡è®°ä¸ºAIå¤„ç†å¤±è´¥"""
        self.status = ProcessedStatus.FAILED
        self.processing_error = error_message
        self.retry_count += 1
        self.updated_at = datetime.utcnow()

    def mark_as_archived(self) -> None:
        """ç”¨æˆ·æ ‡è®°ä¸ºç•™å­˜"""
        self.status = ProcessedStatus.ARCHIVED
        self.updated_at = datetime.utcnow()

    def mark_as_deleted(self) -> None:
        """ç”¨æˆ·æ ‡è®°ä¸ºåˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰"""
        self.status = ProcessedStatus.DELETED
        self.updated_at = datetime.utcnow()

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºAPIå“åº”ï¼‰"""
        return {
            "id": self.id,
            "raw_result_id": self.raw_result_id,
            "task_id": self.task_id,
            "translated_title": self.translated_title,
            "translated_content": self.translated_content,
            "summary": self.summary,
            "key_points": self.key_points,
            "sentiment": self.sentiment,
            "categories": self.categories,
            "ai_model": self.ai_model,
            "ai_processing_time_ms": self.ai_processing_time_ms,
            "ai_confidence_score": self.ai_confidence_score,
            "status": self.status.value,
            "user_rating": self.user_rating,
            "user_notes": self.user_notes,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "processed_at": self.processed_at.isoformat() if self.processed_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            "processing_error": self.processing_error,
            "retry_count": self.retry_count
        }
```

**éªŒè¯ç‚¹**:
- [ ] æ‰€æœ‰å­—æ®µç±»å‹æ­£ç¡®
- [ ] çŠ¶æ€è½¬æ¢æ–¹æ³•å®Œæ•´
- [ ] to_dict() æ–¹æ³•å®Œæ•´

#### 1.2 ä¿®æ”¹ SearchResult å®ä½“

**æ–‡ä»¶**: `src/core/domain/entities/search_result.py`

**ä¿®æ”¹å†…å®¹**:

```python
# ç§»é™¤ä»¥ä¸‹å­—æ®µï¼ˆè¡Œ79-82ï¼‰:
# status: ResultStatus = ResultStatus.PENDING
# processed_at: Optional[datetime] = None

# ç§»é™¤ä»¥ä¸‹æ–¹æ³•ï¼ˆè¡Œ77-85ï¼‰:
# def mark_as_archived(self) -> None
# def mark_as_deleted(self) -> None
```

**ä¿®æ”¹åçš„å®ä½“**:
```python
@dataclass
class SearchResult:
    """æœç´¢ç»“æœå®ä½“ï¼ˆv2.0.0 ç®€åŒ–ç‰ˆ - çº¯åŸå§‹æ•°æ®å­˜å‚¨ï¼‰

    èŒè´£ï¼šåªè´Ÿè´£å­˜å‚¨ä»Firecrawlè·å–çš„åŸå§‹æ•°æ®
    ä¸åŒ…å«ï¼šçŠ¶æ€ç®¡ç†ã€ç”¨æˆ·æ“ä½œã€AIå¤„ç†æ ‡è®°
    """
    # ä¸»é”®ï¼ˆé›ªèŠ±ç®—æ³•IDï¼‰
    id: str = field(default_factory=generate_string_id)
    task_id: str = ""

    # æ ¸å¿ƒåŸå§‹æ•°æ®
    title: str = ""
    url: str = ""
    content: str = ""
    snippet: Optional[str] = None

    # ... å…¶ä»–åŸå§‹æ•°æ®å­—æ®µä¿æŒä¸å˜ ...

    # æ—¶é—´æˆ³ï¼ˆç®€åŒ–ï¼‰
    created_at: datetime = field(default_factory=datetime.utcnow)
    # âŒ ç§»é™¤ processed_at

    # æµ‹è¯•æ ‡è®°
    is_test_data: bool = False

    # âŒ ç§»é™¤çŠ¶æ€ç›¸å…³æ–¹æ³•
    # ä¿ç•™ to_summary() æ–¹æ³•
```

**éªŒè¯ç‚¹**:
- [ ] status å­—æ®µå·²ç§»é™¤
- [ ] processed_at å­—æ®µå·²ç§»é™¤
- [ ] mark_as_archived/deleted æ–¹æ³•å·²ç§»é™¤
- [ ] to_summary() æ–¹æ³•ä¿ç•™

---

### Day 2: åˆ›å»ºå’Œä¿®æ”¹ Repository

#### 2.1 åˆ›å»º ProcessedResultRepository

**æ–‡ä»¶**: `src/infrastructure/database/processed_result_repositories.py`

```python
"""AIå¤„ç†ç»“æœä»“å‚¨å®ç°

v2.0.0 æ–°å¢ï¼š
- ç®¡ç†AIå¤„ç†ç»“æœçš„CRUDæ“ä½œ
- æ”¯æŒçŠ¶æ€ç®¡ç†å’Œç»Ÿè®¡
- æ”¯æŒé‡è¯•æœºåˆ¶
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from motor.motor_asyncio import AsyncIOMotorDatabase

from src.core.domain.entities.processed_result import ProcessedResult, ProcessedStatus
from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


class ProcessedResultRepository:
    """AIå¤„ç†ç»“æœä»“å‚¨"""

    def __init__(self):
        self.collection_name = "processed_results_new"

    async def _get_collection(self):
        """è·å–é›†åˆ"""
        db = await get_mongodb_database()
        return db[self.collection_name]

    def _result_to_dict(self, result: ProcessedResult) -> Dict[str, Any]:
        """å°†ç»“æœå®ä½“è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "_id": result.id,
            "raw_result_id": result.raw_result_id,
            "task_id": result.task_id,
            "translated_title": result.translated_title,
            "translated_content": result.translated_content,
            "summary": result.summary,
            "key_points": result.key_points,
            "sentiment": result.sentiment,
            "categories": result.categories,
            "ai_model": result.ai_model,
            "ai_processing_time_ms": result.ai_processing_time_ms,
            "ai_confidence_score": result.ai_confidence_score,
            "ai_metadata": result.ai_metadata,
            "status": result.status.value,
            "user_rating": result.user_rating,
            "user_notes": result.user_notes,
            "created_at": result.created_at,
            "processed_at": result.processed_at,
            "updated_at": result.updated_at,
            "processing_error": result.processing_error,
            "retry_count": result.retry_count
        }

    def _dict_to_result(self, data: Dict[str, Any]) -> ProcessedResult:
        """å°†å­—å…¸è½¬æ¢ä¸ºç»“æœå®ä½“"""
        return ProcessedResult(
            id=data.get("_id", ""),
            raw_result_id=data.get("raw_result_id", ""),
            task_id=data.get("task_id", ""),
            translated_title=data.get("translated_title"),
            translated_content=data.get("translated_content"),
            summary=data.get("summary"),
            key_points=data.get("key_points", []),
            sentiment=data.get("sentiment"),
            categories=data.get("categories", []),
            ai_model=data.get("ai_model"),
            ai_processing_time_ms=data.get("ai_processing_time_ms", 0),
            ai_confidence_score=data.get("ai_confidence_score", 0.0),
            ai_metadata=data.get("ai_metadata", {}),
            status=ProcessedStatus(data.get("status", "pending")),
            user_rating=data.get("user_rating"),
            user_notes=data.get("user_notes"),
            created_at=data.get("created_at", datetime.utcnow()),
            processed_at=data.get("processed_at"),
            updated_at=data.get("updated_at", datetime.utcnow()),
            processing_error=data.get("processing_error"),
            retry_count=data.get("retry_count", 0)
        )

    async def create_pending_result(
        self,
        raw_result_id: str,
        task_id: str
    ) -> ProcessedResult:
        """
        åˆ›å»ºå¾…å¤„ç†çš„ç»“æœè®°å½•

        Args:
            raw_result_id: åŸå§‹ç»“æœID
            task_id: ä»»åŠ¡ID

        Returns:
            åˆ›å»ºçš„ProcessedResultå®ä½“
        """
        try:
            collection = await self._get_collection()

            result = ProcessedResult(
                raw_result_id=raw_result_id,
                task_id=task_id,
                status=ProcessedStatus.PENDING
            )

            result_dict = self._result_to_dict(result)
            await collection.insert_one(result_dict)

            logger.info(f"åˆ›å»ºå¾…å¤„ç†è®°å½•: raw_result_id={raw_result_id}")
            return result

        except Exception as e:
            logger.error(f"åˆ›å»ºå¾…å¤„ç†è®°å½•å¤±è´¥: {e}")
            raise

    async def update_processing_status(
        self,
        result_id: str,
        status: ProcessedStatus,
        **kwargs
    ) -> bool:
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        try:
            collection = await self._get_collection()

            update_data = {
                "status": status.value,
                "updated_at": datetime.utcnow()
            }
            update_data.update(kwargs)

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            return result.modified_count > 0

        except Exception as e:
            logger.error(f"æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            raise

    async def save_ai_result(
        self,
        result_id: str,
        translated_title: str,
        translated_content: str,
        summary: str,
        key_points: List[str],
        ai_model: str,
        processing_time_ms: int,
        **kwargs
    ) -> bool:
        """ä¿å­˜AIå¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()

            update_data = {
                "translated_title": translated_title,
                "translated_content": translated_content,
                "summary": summary,
                "key_points": key_points,
                "ai_model": ai_model,
                "ai_processing_time_ms": processing_time_ms,
                "status": ProcessedStatus.COMPLETED.value,
                "processed_at": datetime.utcnow(),
                "updated_at": datetime.utcnow()
            }

            # åˆå¹¶é¢å¤–å‚æ•°
            update_data.update(kwargs)

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            if result.modified_count > 0:
                logger.info(f"ä¿å­˜AIç»“æœæˆåŠŸ: {result_id}")
                return True

            return False

        except Exception as e:
            logger.error(f"ä¿å­˜AIç»“æœå¤±è´¥: {e}")
            raise

    async def get_by_id(self, result_id: str) -> Optional[ProcessedResult]:
        """æ ¹æ®IDè·å–å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"_id": result_id})

            if data:
                return self._dict_to_result(data)
            return None

        except Exception as e:
            logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise

    async def get_by_raw_result_id(
        self,
        raw_result_id: str
    ) -> Optional[ProcessedResult]:
        """æ ¹æ®åŸå§‹ç»“æœIDè·å–å¤„ç†ç»“æœ"""
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"raw_result_id": raw_result_id})

            if data:
                return self._dict_to_result(data)
            return None

        except Exception as e:
            logger.error(f"æ ¹æ®åŸå§‹IDè·å–å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise

    async def get_by_task(
        self,
        task_id: str,
        status: Optional[ProcessedStatus] = None,
        page: int = 1,
        page_size: int = 20
    ) -> tuple[List[ProcessedResult], int]:
        """è·å–ä»»åŠ¡çš„å¤„ç†ç»“æœï¼ˆæ”¯æŒçŠ¶æ€ç­›é€‰ï¼‰"""
        try:
            collection = await self._get_collection()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            filter_dict = {"task_id": task_id}
            if status:
                filter_dict["status"] = status.value

            # è®¡ç®—æ€»æ•°
            total = await collection.count_documents(filter_dict)

            # åˆ†é¡µæŸ¥è¯¢
            skip = (page - 1) * page_size
            cursor = collection.find(filter_dict).sort("updated_at", -1).skip(skip).limit(page_size)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results, total

        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise

    async def update_user_action(
        self,
        result_id: str,
        status: ProcessedStatus,
        user_rating: Optional[int] = None,
        user_notes: Optional[str] = None
    ) -> bool:
        """æ›´æ–°ç”¨æˆ·æ“ä½œï¼ˆç•™å­˜ã€åˆ é™¤ã€è¯„åˆ†ï¼‰"""
        try:
            collection = await self._get_collection()

            update_data = {
                "status": status.value,
                "updated_at": datetime.utcnow()
            }

            if user_rating is not None:
                update_data["user_rating"] = user_rating
            if user_notes is not None:
                update_data["user_notes"] = user_notes

            result = await collection.update_one(
                {"_id": result_id},
                {"$set": update_data}
            )

            return result.modified_count > 0

        except Exception as e:
            logger.error(f"æ›´æ–°ç”¨æˆ·æ“ä½œå¤±è´¥: {e}")
            raise

    async def get_status_statistics(self, task_id: str) -> Dict[str, int]:
        """è·å–ä»»åŠ¡çš„çŠ¶æ€ç»Ÿè®¡"""
        try:
            collection = await self._get_collection()

            pipeline = [
                {"$match": {"task_id": task_id}},
                {"$group": {
                    "_id": "$status",
                    "count": {"$sum": 1}
                }}
            ]

            status_counts = {status.value: 0 for status in ProcessedStatus}

            async for doc in collection.aggregate(pipeline):
                status_counts[doc["_id"]] = doc["count"]

            return status_counts

        except Exception as e:
            logger.error(f"è·å–çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}")
            raise

    async def get_failed_results(
        self,
        max_retry: int = 3
    ) -> List[ProcessedResult]:
        """è·å–å¤±è´¥çš„ç»“æœï¼ˆç”¨äºé‡è¯•ï¼‰"""
        try:
            collection = await self._get_collection()

            cursor = collection.find({
                "status": ProcessedStatus.FAILED.value,
                "retry_count": {"$lt": max_retry}
            }).limit(100)

            results = []
            async for data in cursor:
                results.append(self._dict_to_result(data))

            return results

        except Exception as e:
            logger.error(f"è·å–å¤±è´¥ç»“æœå¤±è´¥: {e}")
            raise

    async def delete_by_task(self, task_id: str) -> int:
        """åˆ é™¤ä»»åŠ¡çš„æ‰€æœ‰å¤„ç†ç»“æœï¼ˆçº§è”åˆ é™¤ï¼‰"""
        try:
            collection = await self._get_collection()
            result = await collection.delete_many({"task_id": task_id})

            logger.info(f"åˆ é™¤ä»»åŠ¡å¤„ç†ç»“æœ: {task_id}, åˆ é™¤æ•°é‡: {result.deleted_count}")
            return result.deleted_count

        except Exception as e:
            logger.error(f"åˆ é™¤ä»»åŠ¡å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise
```

**éªŒè¯ç‚¹**:
- [ ] æ‰€æœ‰CRUDæ–¹æ³•å®Œæ•´
- [ ] çŠ¶æ€ç®¡ç†æ–¹æ³•å®Œæ•´
- [ ] é”™è¯¯å¤„ç†å®Œå–„

#### 2.2 ä¿®æ”¹ SearchResultRepository

**æ–‡ä»¶**: `src/infrastructure/database/repositories.py`

**ä¿®æ”¹å†…å®¹**:

1. **ä¿®æ”¹ `save_results` æ–¹æ³•è¿”å›IDåˆ—è¡¨**:

```python
async def save_results(self, results: List[SearchResult]) -> List[str]:
    """æ‰¹é‡ä¿å­˜æœç´¢ç»“æœ

    v2.0.0: è¿”å›ä¿å­˜çš„IDåˆ—è¡¨ï¼Œç”¨äºé€šçŸ¥AIæœåŠ¡

    Returns:
        ä¿å­˜çš„ç»“æœIDåˆ—è¡¨
    """
    if not results:
        return []

    try:
        collection = await self._get_collection()
        result_dicts = [self._result_to_dict(result) for result in results]

        await collection.insert_many(result_dicts)
        saved_ids = [result.id for result in results]

        logger.info(f"ä¿å­˜æœç´¢ç»“æœæˆåŠŸ: {len(results)}æ¡")
        return saved_ids

    except Exception as e:
        logger.error(f"ä¿å­˜æœç´¢ç»“æœå¤±è´¥: {e}")
        raise
```

2. **ç§»é™¤çŠ¶æ€ç®¡ç†æ–¹æ³•**ï¼ˆçº¦422-599è¡Œï¼‰:

```python
# âŒ ç§»é™¤ä»¥ä¸‹æ–¹æ³•:
# - get_results_by_status()
# - count_by_status()
# - update_result_status()
# - bulk_update_status()
# - get_status_distribution()
```

**éªŒè¯ç‚¹**:
- [ ] save_results è¿”å›IDåˆ—è¡¨
- [ ] çŠ¶æ€ç®¡ç†æ–¹æ³•å·²ç§»é™¤
- [ ] åŸºç¡€CRUDæ–¹æ³•ä¿ç•™

#### 2.3 åˆ›å»ºæ•°æ®åº“ç´¢å¼•

**è„šæœ¬**: `scripts/create_processed_results_new_indexes.py`

```python
"""åˆ›å»º processed_results_new é›†åˆç´¢å¼•"""

import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
import os

async def create_indexes():
    # è¿æ¥æ•°æ®åº“
    mongodb_url = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
    db_name = os.getenv("MONGODB_DB_NAME", "intelligent_system")

    client = AsyncIOMotorClient(mongodb_url)
    db = client[db_name]
    collection = db["processed_results_new"]

    # åˆ›å»ºç´¢å¼•
    await collection.create_index("raw_result_id", unique=True)
    await collection.create_index([("task_id", 1), ("status", 1), ("updated_at", -1)])
    await collection.create_index([("status", 1), ("retry_count", 1)])

    print("âœ… processed_results_new ç´¢å¼•åˆ›å»ºæˆåŠŸ")
    client.close()

if __name__ == "__main__":
    asyncio.run(create_indexes())
```

**æ‰§è¡Œ**:
```bash
python scripts/create_processed_results_new_indexes.py
```

---

## Phase 2: å®šæ—¶ä»»åŠ¡é›†æˆï¼ˆDay 3-4ï¼‰

### Day 3: ä¿®æ”¹ TaskSchedulerService

**æ–‡ä»¶**: `src/services/task_scheduler.py`

**ä¿®æ”¹ä½ç½®**: `_execute_search_task` æ–¹æ³•ï¼ˆç¬¬278è¡Œå¼€å§‹ï¼‰

**ä¿®æ”¹å†…å®¹**:

```python
async def _execute_search_task(self, task_id: str):
    """æ‰§è¡Œå•ä¸ªæœç´¢ä»»åŠ¡ï¼ˆv2.0.0 èŒè´£åˆ†ç¦»ç‰ˆæœ¬ï¼‰

    æ–°å¢åŠŸèƒ½ï¼š
    1. ä¿å­˜åŸå§‹ç»“æœåˆ° search_results
    2. åˆ›å»ºå¾…å¤„ç†è®°å½•åˆ° processed_results_new
    3. é€šçŸ¥AIæœåŠ¡å¤„ç†
    """
    start_time = datetime.utcnow()
    logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡: {task_id}")

    try:
        # è·å–ä»»åŠ¡è¯¦æƒ…
        repo = await self._get_task_repository()
        task = await repo.get_by_id(task_id)

        if not task:
            logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return

        if not task.is_active:
            logger.info(f"ä»»åŠ¡å·²ç¦ç”¨ï¼Œè·³è¿‡æ‰§è¡Œ: {task.name}")
            return

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task.last_executed_at = start_time

        # ========================================
        # 1. æ‰§è¡Œæœç´¢/çˆ¬å–ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        # ========================================
        if task.crawl_url:
            logger.info(f"ğŸŒ ä½¿ç”¨ç½‘å€çˆ¬å–æ¨¡å¼: {task.crawl_url}")
            result_batch = await self._execute_crawl_task_internal(task, start_time)
        else:
            logger.info(f"ğŸ” ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼: {task.query}")
            user_config = UserSearchConfig.from_json(task.search_config)
            result_batch = await self.search_adapter.search(
                query=task.query,
                user_config=user_config,
                task_id=str(task.id)
            )

        # ========================================
        # 2. ä¿å­˜åŸå§‹ç»“æœï¼ˆv2.0.0: è¿”å›IDåˆ—è¡¨ï¼‰
        # ========================================
        saved_ids = []
        if result_batch.results:
            try:
                result_repo = await self._get_result_repository()
                if result_repo:
                    # v2.0.0: save_results è¿”å›IDåˆ—è¡¨
                    saved_ids = await result_repo.save_results(result_batch.results)
                    logger.info(f"âœ… åŸå§‹ç»“æœå·²ä¿å­˜: {len(saved_ids)}æ¡")
                else:
                    logger.warning("âš ï¸  MongoDBä¸å¯ç”¨ï¼Œæœç´¢ç»“æœæœªä¿å­˜")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æœç´¢ç»“æœå¤±è´¥: {e}")
                # å¤±è´¥ä¸å½±å“ä»»åŠ¡ç»§ç»­æ‰§è¡Œ

        # ========================================
        # 3. ã€æ–°å¢ã€‘åˆ›å»ºå¾…å¤„ç†è®°å½•
        # ========================================
        if saved_ids:
            try:
                from src.infrastructure.database.processed_result_repositories import ProcessedResultRepository
                processed_repo = ProcessedResultRepository()

                for raw_id in saved_ids:
                    await processed_repo.create_pending_result(
                        raw_result_id=raw_id,
                        task_id=task_id
                    )

                logger.info(f"âœ… åˆ›å»ºå¾…å¤„ç†è®°å½•: {len(saved_ids)}æ¡")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºå¾…å¤„ç†è®°å½•å¤±è´¥: {e}")
                # å¤±è´¥ä¸å½±å“ä»»åŠ¡ç»§ç»­æ‰§è¡Œ

        # ========================================
        # 4. ã€æ–°å¢ã€‘é€šçŸ¥AIæœåŠ¡
        # ========================================
        if saved_ids:
            try:
                await self._notify_ai_service(saved_ids, task_id)
                logger.info(f"âœ… AIæœåŠ¡é€šçŸ¥å·²å‘é€: {len(saved_ids)}æ¡ç»“æœ")
            except Exception as e:
                logger.error(f"âš ï¸  AIæœåŠ¡é€šçŸ¥å¤±è´¥: {e}")
                # é€šçŸ¥å¤±è´¥ä¸å½±å“ä»»åŠ¡å®Œæˆï¼ŒAIæœåŠ¡å¯ä»¥è½®è¯¢

        # ========================================
        # 5. æ›´æ–°ä»»åŠ¡ç»Ÿè®¡ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        # ========================================
        task.record_execution(
            success=result_batch.success,
            results_count=result_batch.returned_count,
            credits_used=result_batch.credits_used
        )

        # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
        interval = ScheduleInterval.from_value(task.schedule_interval)
        trigger = CronTrigger.from_crontab(interval.cron_expression)
        next_run = trigger.get_next_fire_time(None, datetime.now())
        if next_run:
            task.next_run_time = next_run

        # ä¿å­˜ä»»åŠ¡æ›´æ–°
        await repo.update(task)

        execution_time = (datetime.utcnow() - start_time).total_seconds()

        logger.info(
            f"âœ… æœç´¢ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task.name} | "
            f"ç»“æœæ•°: {result_batch.returned_count} | "
            f"è€—æ—¶: {execution_time:.2f}s | "
            f"ä¸‹æ¬¡æ‰§è¡Œ: {next_run.strftime('%Y-%m-%d %H:%M:%S') if next_run else 'N/A'}"
        )

    except Exception as e:
        logger.error(f"âŒ æœç´¢ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task_id}: {e}")

        # è®°å½•å¤±è´¥
        try:
            repo = await self._get_task_repository()
            task = await repo.get_by_id(task_id)
            if task:
                task.record_execution(success=False)
                await repo.update(task)
        except Exception as update_error:
            logger.error(f"æ›´æ–°å¤±è´¥ç»Ÿè®¡æ—¶å‡ºé”™: {update_error}")
```

**éªŒè¯ç‚¹**:
- [ ] åŸå§‹ç»“æœä¿å­˜è¿”å›IDåˆ—è¡¨
- [ ] åˆ›å»ºå¾…å¤„ç†è®°å½•æˆåŠŸ
- [ ] AIæœåŠ¡é€šçŸ¥é€»è¾‘å®Œæ•´

### Day 4: å®ç°AIæœåŠ¡é€šçŸ¥

**æ–°å¢æ–¹æ³•**: `_notify_ai_service` in `TaskSchedulerService`

```python
async def _notify_ai_service(
    self,
    raw_result_ids: List[str],
    task_id: str
) -> None:
    """
    é€šçŸ¥AIæœåŠ¡å¤„ç†æ–°çš„æœç´¢ç»“æœ

    é€šçŸ¥æ–¹å¼ï¼šHTTP POSTè¯·æ±‚åˆ°AIæœåŠ¡

    Args:
        raw_result_ids: åŸå§‹ç»“æœIDåˆ—è¡¨
        task_id: ä»»åŠ¡ID
    """
    import httpx
    import os

    ai_service_url = os.getenv("AI_SERVICE_URL", "http://localhost:8001")
    notify_endpoint = f"{ai_service_url}/api/v1/ai/process-results"

    payload = {
        "raw_result_ids": raw_result_ids,
        "task_id": task_id,
        "timestamp": datetime.utcnow().isoformat()
    }

    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.post(notify_endpoint, json=payload)

            if response.status_code == 202:  # Accepted
                logger.info(f"âœ… AIæœåŠ¡æ¥å—å¤„ç†è¯·æ±‚: {len(raw_result_ids)}æ¡")
            else:
                logger.warning(f"âš ï¸  AIæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")

    except httpx.RequestError as e:
        logger.error(f"âŒ AIæœåŠ¡é€šçŸ¥å¤±è´¥ (ç½‘ç»œé”™è¯¯): {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼ŒAIæœåŠ¡å¯ä»¥é€šè¿‡è½®è¯¢è·å–å¾…å¤„ç†ä»»åŠ¡
    except Exception as e:
        logger.error(f"âŒ AIæœåŠ¡é€šçŸ¥å¤±è´¥ (æœªçŸ¥é”™è¯¯): {e}")
```

**ç¯å¢ƒå˜é‡é…ç½®**:

```bash
# .env
AI_SERVICE_URL=http://localhost:8001  # AIæœåŠ¡åœ°å€
```

**éªŒè¯ç‚¹**:
- [ ] HTTPè¯·æ±‚æ­£å¸¸å‘é€
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] ä¸é˜»å¡ä¸»æµç¨‹

---

## Phase 3: APIå±‚é€‚é…ï¼ˆDay 5-6ï¼‰

### Day 5: ä¿®æ”¹æŸ¥è¯¢API

**æ–‡ä»¶**: `src/api/v1/endpoints/search_results_frontend.py`

**ä¿®æ”¹å†…å®¹**:

1. **æ·»åŠ æ–°çš„æŸ¥è¯¢ç«¯ç‚¹ï¼ˆé»˜è®¤è¿”å›processed_results_newï¼‰**:

```python
from src.infrastructure.database.processed_result_repositories import ProcessedResultRepository
from src.core.domain.entities.processed_result import ProcessedStatus

@router.get("/api/v1/search-tasks/{task_id}/results", summary="è·å–ä»»åŠ¡ç»“æœï¼ˆv2.0.0ï¼‰")
async def get_task_results(
    task_id: str,
    view: str = "processed",  # "processed" | "raw"
    status: Optional[str] = None,  # ProcessedStatuså€¼
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100)
):
    """
    è·å–ä»»åŠ¡çš„æœç´¢ç»“æœ

    v2.0.0 å˜æ›´ï¼š
    - é»˜è®¤è¿”å› processed_results_newï¼ˆAIå¤„ç†åçš„ç»“æœï¼‰
    - æ”¯æŒ view=raw æŸ¥çœ‹åŸå§‹ç»“æœ
    - æ”¯æŒæŒ‰ ProcessedStatus ç­›é€‰

    Args:
        task_id: ä»»åŠ¡ID
        view: è§†å›¾æ¨¡å¼ï¼ˆprocessed: AIå¤„ç†ç»“æœ, raw: åŸå§‹ç»“æœï¼‰
        status: çŠ¶æ€ç­›é€‰ï¼ˆpending/processing/completed/failed/archived/deletedï¼‰
        page: é¡µç 
        page_size: æ¯é¡µæ•°é‡
    """
    try:
        if view == "raw":
            # è¿”å›åŸå§‹ç»“æœï¼ˆå‘åå…¼å®¹ï¼‰
            repo = SearchResultRepository()
            results, total = await repo.get_by_task(task_id, page, page_size)

            return {
                "task_id": task_id,
                "view": "raw",
                "results": [result.to_summary() for result in results],
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total": total,
                    "pages": (total + page_size - 1) // page_size
                }
            }

        else:  # view == "processed" (é»˜è®¤)
            # è¿”å›AIå¤„ç†ç»“æœ
            repo = ProcessedResultRepository()

            # çŠ¶æ€ç­›é€‰
            status_filter = None
            if status:
                try:
                    status_filter = ProcessedStatus(status)
                except ValueError:
                    raise HTTPException(
                        status_code=400,
                        detail=f"æ— æ•ˆçš„çŠ¶æ€å€¼: {status}"
                    )

            results, total = await repo.get_by_task(
                task_id,
                status=status_filter,
                page=page,
                page_size=page_size
            )

            # è·å–çŠ¶æ€ç»Ÿè®¡
            status_stats = await repo.get_status_statistics(task_id)

            return {
                "task_id": task_id,
                "view": "processed",
                "statistics": status_stats,
                "results": [result.to_dict() for result in results],
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total": total,
                    "pages": (total + page_size - 1) // page_size
                }
            }

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

**éªŒè¯ç‚¹**:
- [ ] é»˜è®¤è¿”å›processed_results_new
- [ ] view=raw è¿”å›åŸå§‹ç»“æœ
- [ ] çŠ¶æ€ç­›é€‰æ­£å¸¸å·¥ä½œ
- [ ] åˆ†é¡µæ­£ç¡®

### Day 6: æ–°å¢ç”¨æˆ·æ“ä½œAPI

**æ–°å¢ç«¯ç‚¹**:

```python
@router.post(
    "/api/v1/processed-results/{result_id}/archive",
    summary="ç•™å­˜ç»“æœ"
)
async def archive_result(result_id: str, user_notes: Optional[str] = None):
    """ç”¨æˆ·æ ‡è®°ç»“æœä¸ºç•™å­˜"""
    repo = ProcessedResultRepository()
    success = await repo.update_user_action(
        result_id,
        ProcessedStatus.ARCHIVED,
        user_notes=user_notes
    )

    if not success:
        raise HTTPException(status_code=404, detail="ç»“æœä¸å­˜åœ¨")

    return {"success": True, "message": "å·²æ ‡è®°ä¸ºç•™å­˜"}


@router.post(
    "/api/v1/processed-results/{result_id}/delete",
    summary="åˆ é™¤ç»“æœï¼ˆè½¯åˆ é™¤ï¼‰"
)
async def delete_result(result_id: str):
    """ç”¨æˆ·æ ‡è®°ç»“æœä¸ºåˆ é™¤"""
    repo = ProcessedResultRepository()
    success = await repo.update_user_action(
        result_id,
        ProcessedStatus.DELETED
    )

    if not success:
        raise HTTPException(status_code=404, detail="ç»“æœä¸å­˜åœ¨")

    return {"success": True, "message": "å·²åˆ é™¤"}


@router.put(
    "/api/v1/processed-results/{result_id}/rating",
    summary="è¯„åˆ†ç»“æœ"
)
async def rate_result(
    result_id: str,
    rating: int = Query(..., ge=1, le=5),
    user_notes: Optional[str] = None
):
    """ç”¨æˆ·å¯¹ç»“æœè¿›è¡Œè¯„åˆ†"""
    repo = ProcessedResultRepository()

    # è·å–å½“å‰ç»“æœ
    result = await repo.get_by_id(result_id)
    if not result:
        raise HTTPException(status_code=404, detail="ç»“æœä¸å­˜åœ¨")

    # æ›´æ–°è¯„åˆ†
    success = await repo.update_user_action(
        result_id,
        result.status,  # ä¿æŒåŸçŠ¶æ€
        user_rating=rating,
        user_notes=user_notes
    )

    return {"success": True, "rating": rating}
```

**éªŒè¯ç‚¹**:
- [ ] ç•™å­˜APIæ­£å¸¸å·¥ä½œ
- [ ] åˆ é™¤APIæ­£å¸¸å·¥ä½œ
- [ ] è¯„åˆ†APIæ­£å¸¸å·¥ä½œ

---

## Phase 4: æ•°æ®è¿ç§»ï¼ˆDay 7ï¼‰

### åˆ›å»ºè¿ç§»è„šæœ¬

**æ–‡ä»¶**: `scripts/migrate_search_results_to_processed.py`

```python
"""
å°†ç°æœ‰ search_results è¿ç§»åˆ° processed_results_new

è¿ç§»ç­–ç•¥ï¼š
1. è¯»å–æ‰€æœ‰ search_results
2. ä¸ºæ¯æ¡è®°å½•åˆ›å»ºå¯¹åº”çš„ processed_results_new
3. åˆå§‹çŠ¶æ€è®¾ä¸º PENDING
4. ä¿ç•™åŸå§‹æ•°æ®ä¸­çš„ status æ˜ å°„
"""

import asyncio
import os
from motor.motor_asyncio import AsyncIOMotorClient
from datetime import datetime

async def migrate():
    # è¿æ¥æ•°æ®åº“
    mongodb_url = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
    db_name = os.getenv("MONGODB_DB_NAME", "intelligent_system")

    client = AsyncIOMotorClient(mongodb_url)
    db = client[db_name]

    search_results_col = db["search_results"]
    processed_results_new_col = db["processed_results_new"]

    # ç»Ÿè®¡
    total = await search_results_col.count_documents({})
    migrated = 0
    skipped = 0

    print(f"å¼€å§‹è¿ç§»ï¼Œå…± {total} æ¡è®°å½•...")

    # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹1000æ¡ï¼‰
    batch_size = 1000
    skip = 0

    while skip < total:
        # è¯»å–ä¸€æ‰¹åŸå§‹ç»“æœ
        cursor = search_results_col.find({}).skip(skip).limit(batch_size)

        batch = []
        async for doc in cursor:
            raw_id = doc["_id"]
            task_id = doc.get("task_id", "")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = await processed_results_new_col.find_one({"raw_result_id": raw_id})
            if existing:
                skipped += 1
                continue

            # åˆ›å»ºprocessed_resultæ–‡æ¡£
            processed_doc = {
                "_id": f"processed_{raw_id}",  # ç”Ÿæˆæ–°ID
                "raw_result_id": raw_id,
                "task_id": task_id,
                "translated_title": None,
                "translated_content": None,
                "summary": None,
                "key_points": [],
                "sentiment": None,
                "categories": [],
                "ai_model": None,
                "ai_processing_time_ms": 0,
                "ai_confidence_score": 0.0,
                "ai_metadata": {},
                "status": "pending",  # åˆå§‹çŠ¶æ€ä¸ºPENDING
                "user_rating": None,
                "user_notes": None,
                "created_at": doc.get("created_at", datetime.utcnow()),
                "processed_at": None,
                "updated_at": datetime.utcnow(),
                "processing_error": None,
                "retry_count": 0
            }

            batch.append(processed_doc)
            migrated += 1

        # æ‰¹é‡æ’å…¥
        if batch:
            await processed_results_new_col.insert_many(batch)
            print(f"å·²è¿ç§» {migrated}/{total} æ¡è®°å½•...")

        skip += batch_size

    print(f"âœ… è¿ç§»å®Œæˆï¼š")
    print(f"  - æ€»è®°å½•æ•°: {total}")
    print(f"  - å·²è¿ç§»: {migrated}")
    print(f"  - å·²è·³è¿‡: {skipped}")

    client.close()

if __name__ == "__main__":
    asyncio.run(migrate())
```

**æ‰§è¡Œè¿ç§»**:

```bash
# 1. å¤‡ä»½æ•°æ®åº“ï¼ˆé‡è¦ï¼ï¼‰
mongodump --uri="mongodb://localhost:27017/intelligent_system" --out=./backup_$(date +%Y%m%d)

# 2. æ‰§è¡Œè¿ç§»
python scripts/migrate_search_results_to_processed.py

# 3. éªŒè¯è¿ç§»ç»“æœ
python scripts/verify_migration.py
```

**éªŒè¯è„šæœ¬**: `scripts/verify_migration.py`

```python
"""éªŒè¯è¿ç§»ç»“æœ"""

import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
import os

async def verify():
    mongodb_url = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
    db_name = os.getenv("MONGODB_DB_NAME", "intelligent_system")

    client = AsyncIOMotorClient(mongodb_url)
    db = client[db_name]

    search_results_count = await db.search_results.count_documents({})
    processed_results_new_count = await db.processed_results_new.count_documents({})

    print(f"search_results è®°å½•æ•°: {search_results_count}")
    print(f"processed_results_new è®°å½•æ•°: {processed_results_new_count}")

    if search_results_count == processed_results_new_count:
        print("âœ… è¿ç§»éªŒè¯æˆåŠŸï¼šè®°å½•æ•°ä¸€è‡´")
    else:
        print("âŒ è¿ç§»éªŒè¯å¤±è´¥ï¼šè®°å½•æ•°ä¸ä¸€è‡´")
        print(f"   å·®å¼‚: {abs(search_results_count - processed_results_new_count)} æ¡")

    # æ£€æŸ¥å…³è”å…³ç³»
    cursor = db.processed_results_new.aggregate([
        {
            "$lookup": {
                "from": "search_results",
                "localField": "raw_result_id",
                "foreignField": "_id",
                "as": "raw"
            }
        },
        {"$match": {"raw": {"$size": 0}}},
        {"$count": "orphaned"}
    ])

    orphaned = 0
    async for doc in cursor:
        orphaned = doc.get("orphaned", 0)

    if orphaned > 0:
        print(f"âš ï¸  å‘ç° {orphaned} æ¡å­¤ç«‹è®°å½•ï¼ˆæ— å¯¹åº”åŸå§‹ç»“æœï¼‰")
    else:
        print("âœ… å…³è”å…³ç³»éªŒè¯æˆåŠŸï¼šæ— å­¤ç«‹è®°å½•")

    client.close()

if __name__ == "__main__":
    asyncio.run(verify())
```

---

## Phase 5: æµ‹è¯•å’Œæ–‡æ¡£ï¼ˆDay 8-9ï¼‰

### Day 8: å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

**åˆ›å»ºæµ‹è¯•æ–‡ä»¶**: `tests/test_processed_result.py`

```python
"""ProcessedResult å•å…ƒæµ‹è¯•"""

import pytest
from datetime import datetime
from src.core.domain.entities.processed_result import (
    ProcessedResult,
    ProcessedStatus
)

def test_create_processed_result():
    """æµ‹è¯•åˆ›å»ºProcessedResult"""
    result = ProcessedResult(
        raw_result_id="test_raw_id",
        task_id="test_task_id"
    )

    assert result.raw_result_id == "test_raw_id"
    assert result.task_id == "test_task_id"
    assert result.status == ProcessedStatus.PENDING
    assert result.retry_count == 0


def test_mark_as_processing():
    """æµ‹è¯•æ ‡è®°ä¸ºå¤„ç†ä¸­"""
    result = ProcessedResult(
        raw_result_id="test_raw_id",
        task_id="test_task_id"
    )

    result.mark_as_processing()

    assert result.status == ProcessedStatus.PROCESSING


def test_mark_as_completed():
    """æµ‹è¯•æ ‡è®°ä¸ºå®Œæˆ"""
    result = ProcessedResult(
        raw_result_id="test_raw_id",
        task_id="test_task_id"
    )

    result.mark_as_completed("gpt-4", 5000)

    assert result.status == ProcessedStatus.COMPLETED
    assert result.ai_model == "gpt-4"
    assert result.ai_processing_time_ms == 5000
    assert result.processed_at is not None


def test_mark_as_failed():
    """æµ‹è¯•æ ‡è®°ä¸ºå¤±è´¥"""
    result = ProcessedResult(
        raw_result_id="test_raw_id",
        task_id="test_task_id"
    )

    result.mark_as_failed("AIæœåŠ¡é”™è¯¯")

    assert result.status == ProcessedStatus.FAILED
    assert result.processing_error == "AIæœåŠ¡é”™è¯¯"
    assert result.retry_count == 1


def test_status_transitions():
    """æµ‹è¯•çŠ¶æ€æµè½¬"""
    result = ProcessedResult(
        raw_result_id="test_raw_id",
        task_id="test_task_id"
    )

    # PENDING -> PROCESSING
    result.mark_as_processing()
    assert result.status == ProcessedStatus.PROCESSING

    # PROCESSING -> COMPLETED
    result.mark_as_completed("gpt-4", 5000)
    assert result.status == ProcessedStatus.COMPLETED

    # COMPLETED -> ARCHIVED
    result.mark_as_archived()
    assert result.status == ProcessedStatus.ARCHIVED
```

**åˆ›å»ºRepositoryæµ‹è¯•**: `tests/test_processed_result_repository.py`

**åˆ›å»ºé›†æˆæµ‹è¯•**: `tests/integration/test_scheduled_task_flow.py`

### Day 9: æ–‡æ¡£æ›´æ–°

**æ›´æ–°æ–‡æ¡£**:

1. **API_GUIDE.md**: æ·»åŠ æ–°çš„APIç«¯ç‚¹æ–‡æ¡£
2. **DATABASE_COLLECTIONS_GUIDE.md**: æ›´æ–°é›†åˆèŒè´£è¯´æ˜
3. **SYSTEM_ARCHITECTURE.md**: æ›´æ–°æ¶æ„å›¾
4. **VERSION_MANAGEMENT.md**: è®°å½•v2.0.0å˜æ›´

---

## ğŸš¨ é£é™©ç®¡ç†

### é£é™©è¯†åˆ«

| é£é™© | å½±å“ | å¯èƒ½æ€§ | ç¼“è§£æªæ–½ |
|------|------|--------|----------|
| æ•°æ®è¿ç§»å¤±è´¥ | é«˜ | ä½ | å®Œæ•´å¤‡ä»½ + å›æ»šè„šæœ¬ |
| AIæœåŠ¡é€šçŸ¥å¤±è´¥ | ä¸­ | ä¸­ | è½®è¯¢æœºåˆ¶å…œåº• |
| å‰ç«¯å…¼å®¹æ€§é—®é¢˜ | ä¸­ | ä½ | å‘åå…¼å®¹APIè®¾è®¡ |
| æ€§èƒ½ä¸‹é™ | ä¸­ | ä½ | ç´¢å¼•ä¼˜åŒ– + ç›‘æ§ |

### å›æ»šæ–¹æ¡ˆ

**å›æ»šæ¡ä»¶**:
- æ•°æ®è¿ç§»å¤±è´¥ç‡ >5%
- APIé”™è¯¯ç‡ >10%
- æ€§èƒ½ä¸‹é™ >30%

**å›æ»šæ­¥éª¤**:
1. åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
2. æ¢å¤æ•°æ®åº“å¤‡ä»½
3. å›é€€ä»£ç åˆ°ä¸Šä¸€ç‰ˆæœ¬
4. é‡å¯æœåŠ¡

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

- [ ] å®šæ—¶ä»»åŠ¡æ­£å¸¸æ‰§è¡Œå¹¶ä¿å­˜åŸå§‹ç»“æœ
- [ ] processed_results_new è®°å½•è‡ªåŠ¨åˆ›å»º
- [ ] AIæœåŠ¡é€šçŸ¥æ­£å¸¸å‘é€
- [ ] æŸ¥è¯¢APIè¿”å›æ­£ç¡®æ•°æ®
- [ ] ç”¨æˆ·æ“ä½œAPIæ­£å¸¸å·¥ä½œ

### æ€§èƒ½éªŒæ”¶

- [ ] å®šæ—¶ä»»åŠ¡æ‰§è¡Œæ—¶é—´å¢åŠ  <10%
- [ ] APIå“åº”æ—¶é—´ <200ms
- [ ] æ•°æ®åº“æŸ¥è¯¢æ•ˆç‡æ— æ˜æ˜¾ä¸‹é™

### æ•°æ®éªŒæ”¶

- [ ] å†å²æ•°æ®è¿ç§»æˆåŠŸç‡ >99%
- [ ] å…³è”å…³ç³»å®Œæ•´æ€§ 100%
- [ ] æ— å­¤ç«‹è®°å½•

---

## ğŸ“ ç›¸å…³æ–‡æ¡£

- [æ¶æ„è®¾è®¡æ–‡æ¡£](SEARCH_RESULTS_SEPARATION_ARCHITECTURE.md)
- [UMLå›¾ç›®å½•](diagrams/)
- [APIæ–‡æ¡£æ›´æ–°](../docs/API_GUIDE.md)
- [æ•°æ®åº“é›†åˆæŒ‡å—](../docs/DATABASE_COLLECTIONS_GUIDE.md)

---

**æ–‡æ¡£ä½œè€…**: Claude Code Assistant
**æ–‡æ¡£çŠ¶æ€**: âœ… å®æ–½æŒ‡å—å®Œæˆ
**å®¡æ ¸äºº**: Backend Team
**é¢„è®¡å¼€å§‹æ—¥æœŸ**: å¾…å®š
**é¢„è®¡å®Œæˆæ—¥æœŸ**: å¼€å§‹å9ä¸ªå·¥ä½œæ—¥
