# crawl_website æ—¶é—´èŒƒå›´åŠŸèƒ½åˆ†ææŠ¥å‘Š

**æ—¥æœŸ**: 2025-11-06
**åˆ†æç›®æ ‡**: ç¡®å®š `crawl_website` ä»»åŠ¡ç±»å‹æ˜¯å¦æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤
**ç»“è®º**: âŒ **å½“å‰ä¸æ”¯æŒ**ï¼ŒFirecrawl Crawl API ä¸æä¾›æ—¶é—´è¿‡æ»¤åŠŸèƒ½

---

## æ‰§è¡Œæ‘˜è¦

ç»è¿‡å…¨é¢çš„ä»£ç åˆ†æå’Œ Firecrawl API æ–‡æ¡£è°ƒç ”ï¼Œç¡®è®¤ï¼š

1. âœ… **search_keyword æ¨¡å¼**ï¼šå·²æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆ`time_range` å‚æ•°ï¼‰
2. âŒ **crawl_website æ¨¡å¼**ï¼šä¸æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆFirecrawl Crawl API é™åˆ¶ï¼‰
3. ğŸ’¡ **å¯è¡Œæ–¹æ¡ˆ**ï¼šé€šè¿‡åç«¯å…ƒæ•°æ®è¿‡æ»¤æˆ–ä½¿ç”¨ Search API æ›¿ä»£å®ç°éœ€æ±‚

---

## è¯¦ç»†åˆ†æ

### 1. crawl_website å½“å‰å®ç°

#### 1.1 çˆ¬å–é…ç½®å‚æ•° (`crawl_config`)

**ä»£ç ä½ç½®**: `src/infrastructure/crawlers/firecrawl_adapter.py:114-178`

**æ”¯æŒçš„å‚æ•°**:
```python
{
    "limit": 100,              # æœ€å¤§é¡µé¢æ•°
    "max_depth": 3,            # çˆ¬å–æ·±åº¦
    "include_paths": [],       # åŒ…å«çš„è·¯å¾„æ¨¡å¼
    "exclude_paths": [],       # æ’é™¤çš„è·¯å¾„æ¨¡å¼
    "only_main_content": True, # åªæå–ä¸»å†…å®¹
    "wait_for": 1000,          # é¡µé¢ç­‰å¾…æ—¶é—´
    "exclude_tags": []         # æ’é™¤çš„HTMLæ ‡ç­¾
}
```

#### 1.2 Firecrawl Crawl API å‚æ•°

**å®˜æ–¹æ–‡æ¡£**: https://docs.firecrawl.dev/features/crawl

**API v2 å‚æ•°**:
- `url`: èµ·å§‹URL
- `limit`: æœ€å¤§çˆ¬å–é¡µé¢æ•°
- `max_discovery_depth`: çˆ¬å–æ·±åº¦
- `include_paths`: è·¯å¾„åŒ…å«æ¨¡å¼
- `exclude_paths`: è·¯å¾„æ’é™¤æ¨¡å¼
- `scrape_options`: çˆ¬å–é€‰é¡¹ï¼ˆæ ¼å¼ã€å†…å®¹æ¸…ç†ç­‰ï¼‰
- `poll_interval`: è½®è¯¢é—´éš”
- `timeout`: è¶…æ—¶æ—¶é—´

**âŒ ä¸æ”¯æŒçš„å‚æ•°**:
- âŒ `tbs`: æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆä»… Search API æ”¯æŒï¼‰
- âŒ `from_date` / `to_date`: æ—¥æœŸèŒƒå›´
- âŒ `maxAge` çš„ä½œç”¨æ˜¯**ç¼“å­˜æ§åˆ¶**ï¼Œä¸æ˜¯å†…å®¹è¿‡æ»¤

---

### 2. search_keyword æ—¶é—´è¿‡æ»¤å®ç°

#### 2.1 å·²æ”¯æŒçš„åŠŸèƒ½

**ä»£ç ä½ç½®**: `src/infrastructure/search/firecrawl_search_adapter.py:280-293`

**å®ç°ä»£ç **:
```python
# æ„å»ºè¯·æ±‚ä½“æ—¶ï¼Œå¦‚æœé…ç½®äº† time_range
if config.get('time_range'):
    body['tbs'] = self._convert_time_range(config['time_range'])

def _convert_time_range(self, time_range: str) -> str:
    """è½¬æ¢æ—¶é—´èŒƒå›´ä¸ºFirecrawlæ ¼å¼"""
    mapping = {
        "day": "qdr:d",      # æœ€è¿‘ä¸€å¤©
        "week": "qdr:w",     # æœ€è¿‘ä¸€å‘¨
        "month": "qdr:m",    # æœ€è¿‘ä¸€ä¸ªæœˆ
        "year": "qdr:y"      # æœ€è¿‘ä¸€å¹´
    }
    return mapping.get(time_range, "")
```

#### 2.2 ä½¿ç”¨æ–¹å¼

**API è¯·æ±‚ç¤ºä¾‹**:
```json
{
  "name": "AIæ–°é—»ç›‘æ§",
  "query": "äººå·¥æ™ºèƒ½",
  "task_type": "search_keyword",
  "search_config": {
    "limit": 10,
    "time_range": "week"    // åªæœç´¢æœ€è¿‘ä¸€å‘¨çš„å†…å®¹
  }
}
```

**Firecrawl API å‚æ•°**:
- **é¢„å®šä¹‰èŒƒå›´**: `qdr:h`, `qdr:d`, `qdr:w`, `qdr:m`, `qdr:y`
- **è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´**: `cdr:1,cd_min:MM/DD/YYYY,cd_max:MM/DD/YYYY`

---

### 3. ä¸ºä»€ä¹ˆ Crawl API ä¸æ”¯æŒæ—¶é—´è¿‡æ»¤

#### 3.1 API è®¾è®¡ç›®çš„ä¸åŒ

| API ç±»å‹ | è®¾è®¡ç›®çš„ | ä½¿ç”¨åœºæ™¯ |
|---------|---------|---------|
| **Search API** | æœç´¢å¼•æ“ç»“æœçˆ¬å– | ç›‘æ§å…³é”®è¯ã€æ–°é—»è¿½è¸ªã€èˆ†æƒ…åˆ†æ |
| **Crawl API** | ç½‘ç«™ç»“æ„çˆ¬å– | ç½‘ç«™å½’æ¡£ã€å†…å®¹è¿ç§»ã€å®Œæ•´æŠ“å– |

**Crawl API ç‰¹ç‚¹**:
- é€šè¿‡ sitemap æˆ–é€’å½’å‘ç°é¡µé¢
- æŒ‰ URL ç»“æ„è€Œéå†…å®¹å…ƒæ•°æ®çˆ¬å–
- é€‚ç”¨äºçˆ¬å–æ•´ä¸ªç½‘ç«™æˆ–ç‰¹å®šè·¯å¾„

**Search API ç‰¹ç‚¹**:
- é€šè¿‡æœç´¢å¼•æ“è·å–ç»“æœ
- å¯ä»¥æŒ‰å‘å¸ƒæ—¶é—´ã€ç›¸å…³æ€§æ’åº
- é€‚ç”¨äºè¿½è¸ªç‰¹å®šä¸»é¢˜çš„æœ€æ–°å†…å®¹

#### 3.2 æŠ€æœ¯åŸç†å·®å¼‚

```
Search API å·¥ä½œæµç¨‹:
1. æŸ¥è¯¢æœç´¢å¼•æ“ (Google/Bing) â†’ å¯åº”ç”¨æ—¶é—´è¿‡æ»¤
2. æœç´¢å¼•æ“è¿”å›ç¬¦åˆæ¡ä»¶çš„URLåˆ—è¡¨
3. çˆ¬å–æœç´¢ç»“æœé¡µé¢

Crawl API å·¥ä½œæµç¨‹:
1. ä»èµ·å§‹URLå¼€å§‹çˆ¬å–
2. æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥
3. é€’å½’çˆ¬å–ç¬¦åˆ include_paths çš„é“¾æ¥
4. âŒ æ— æ³•åœ¨çˆ¬å–å‰çŸ¥é“é¡µé¢å‘å¸ƒæ—¶é—´
```

---

### 4. å…ƒæ•°æ®å¯ç”¨æ€§åˆ†æ

#### 4.1 çˆ¬å–ç»“æœåŒ…å«çš„æ—¶é—´ä¿¡æ¯

**ä»£ç ä½ç½®**: `src/infrastructure/crawlers/firecrawl_adapter.py:164-171`

**metadata å­—æ®µ**:
```python
{
    "article:published_time": "2025-11-06T10:00:00Z",  # æ–‡ç« å‘å¸ƒæ—¶é—´
    "article:modified_time": "2025-11-06T12:00:00Z",   # ä¿®æ”¹æ—¶é—´
    "og:published_time": "2025-11-06T10:00:00Z",       # Open Graphå‘å¸ƒæ—¶é—´
    "statusCode": 200,
    "sourceURL": "https://example.com/article"
}
```

#### 4.2 å¯ç”¨æ€§è¯„ä¼°

| å­—æ®µ | å¯ç”¨æ€§ | è¯´æ˜ |
|------|--------|------|
| `article:published_time` | âš ï¸ éƒ¨åˆ†å¯ç”¨ | ä¾èµ–ç½‘ç«™å®ç° Open Graph å…ƒæ ‡ç­¾ |
| `article:modified_time` | âš ï¸ éƒ¨åˆ†å¯ç”¨ | å¯èƒ½ä¸å‡†ç¡®ï¼ˆç¼–è¾‘æ›´æ–°ä¼šæ”¹å˜ï¼‰ |
| URL è·¯å¾„æ—¥æœŸ | âš ï¸ ä¾èµ–ç½‘ç«™ | `/2025/11/article` æ ¼å¼ï¼Œéœ€æ­£åˆ™è§£æ |

**é—®é¢˜**:
1. ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½åŒ…å«ç»“æ„åŒ–çš„æ—¶é—´å…ƒæ•°æ®
2. å…ƒæ•°æ®åœ¨çˆ¬å–**å**æ‰èƒ½è·å–ï¼Œæ— æ³•å‡å°‘çˆ¬å–é‡
3. ä¼šæ¶ˆè€—ä¸å¿…è¦çš„ Firecrawl ç§¯åˆ†

---

## è§£å†³æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ1: åç«¯å…ƒæ•°æ®è¿‡æ»¤ â­ æ¨èï¼ˆç®€å•åœºæ™¯ï¼‰

**å®ç°æ–¹å¼**:
1. æ­£å¸¸çˆ¬å–ç½‘ç«™ï¼ˆä¸é™æ—¶é—´ï¼‰
2. çˆ¬å–å®Œæˆåï¼Œæ ¹æ® `article_published_time` è¿‡æ»¤ç»“æœ
3. åªä¿å­˜ç¬¦åˆæ—¶é—´èŒƒå›´çš„ç»“æœ

**ä¼˜ç‚¹**:
- âœ… å®ç°ç®€å•ï¼Œä¸éœ€è¦æ”¹åŠ¨ Firecrawl è°ƒç”¨
- âœ… å¯ä»¥ä½¿ç”¨ç²¾ç¡®çš„æ—¥æœŸè¿‡æ»¤

**ç¼ºç‚¹**:
- âŒ ä»ä¼šçˆ¬å–æ‰€æœ‰é¡µé¢ï¼ˆæµªè´¹ç§¯åˆ†ï¼‰
- âŒ æ‰§è¡Œæ—¶é—´é•¿ï¼ˆæ— æ³•æå‰ç»ˆæ­¢ï¼‰
- âŒ ä¾èµ–ç½‘ç«™çš„å…ƒæ•°æ®è´¨é‡

**ä»£ç ç¤ºä¾‹**:
```python
async def crawl_with_time_filter(url, from_date, to_date):
    # 1. çˆ¬å–ç½‘ç«™
    results = await firecrawl_adapter.crawl(url, limit=100)

    # 2. è¿‡æ»¤ç»“æœ
    filtered_results = []
    for result in results:
        pub_time = result.metadata.get('article:published_time')
        if pub_time:
            pub_date = datetime.fromisoformat(pub_time)
            if from_date <= pub_date <= to_date:
                filtered_results.append(result)

    return filtered_results
```

**é€‚ç”¨åœºæ™¯**:
- ç§¯åˆ†å……è¶³çš„æƒ…å†µ
- ç½‘ç«™è§„æ¨¡è¾ƒå°ï¼ˆ< 100é¡µï¼‰
- éœ€è¦ç²¾ç¡®çš„æ—¥æœŸèŒƒå›´æ§åˆ¶

---

### æ–¹æ¡ˆ2: ä½¿ç”¨ Search API + site: æ“ä½œç¬¦ â­â­ æ¨èï¼ˆé«˜æ•ˆåœºæ™¯ï¼‰

**å®ç°æ–¹å¼**:
1. æ”¹ç”¨ `search_keyword` æ¨¡å¼
2. ä½¿ç”¨ `site:domain.com` é™åˆ¶æœç´¢èŒƒå›´
3. é…ç½® `time_range` å‚æ•°

**ä¼˜ç‚¹**:
- âœ… åŸç”Ÿæ”¯æŒæ—¶é—´è¿‡æ»¤ï¼ˆèŠ‚çœç§¯åˆ†ï¼‰
- âœ… åªçˆ¬å–æœ€æ–°å†…å®¹
- âœ… æ‰§è¡Œé€Ÿåº¦å¿«

**ç¼ºç‚¹**:
- âŒ æ”¹å˜äº†çˆ¬å–è¡Œä¸ºï¼ˆæœç´¢ vs é€’å½’çˆ¬å–ï¼‰
- âŒ ä¾èµ–æœç´¢å¼•æ“ç´¢å¼•ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
- âŒ æ— æ³•æ§åˆ¶çˆ¬å–æ·±åº¦

**API è¯·æ±‚ç¤ºä¾‹**:
```json
{
  "name": "æŠ€æœ¯åšå®¢æœ€æ–°æ–‡ç« ",
  "query": "site:blog.example.com",
  "task_type": "search_keyword",
  "search_config": {
    "limit": 50,
    "time_range": "month",
    "only_main_content": true
  }
}
```

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦ç›‘æ§ç½‘ç«™æœ€æ–°å†…å®¹
- ç§¯åˆ†æœ‰é™ï¼Œéœ€è¦é«˜æ•ˆçˆ¬å–
- ç½‘ç«™å·²è¢«æœç´¢å¼•æ“ç´¢å¼•

---

### æ–¹æ¡ˆ3: è°ƒåº¦é¢‘ç‡æ§åˆ¶ â­â­â­ æ¨èï¼ˆæ ‡å‡†åœºæ™¯ï¼‰

**å®ç°æ–¹å¼**:
1. ä¿æŒ `crawl_website` æ¨¡å¼ä¸å˜
2. é€šè¿‡è°ƒæ•´ `schedule_interval` æ§åˆ¶æ›´æ–°é¢‘ç‡
3. ä¾èµ–å¢é‡çˆ¬å–é€»è¾‘ï¼ˆå»é‡ï¼‰

**ä¼˜ç‚¹**:
- âœ… æ— éœ€ä¿®æ”¹ç°æœ‰å®ç°
- âœ… é€‚åˆå®Œæ•´ç½‘ç«™å½’æ¡£åœºæ™¯
- âœ… ç¬¦åˆ Firecrawl Crawl API è®¾è®¡æ„å›¾

**ç¼ºç‚¹**:
- âŒ æ— æ³•ç²¾ç¡®æ§åˆ¶æ—¶é—´èŒƒå›´
- âŒ å¯èƒ½é‡å¤çˆ¬å–ç›¸åŒå†…å®¹

**é…ç½®ç¤ºä¾‹**:
```json
{
  "name": "æ¯æ—¥åšå®¢å½’æ¡£",
  "crawl_url": "https://blog.example.com",
  "task_type": "crawl_website",
  "crawl_config": {
    "limit": 50,
    "max_depth": 2
  },
  "schedule_interval": "DAILY"  // æ¯å¤©çˆ¬å–ä¸€æ¬¡ï¼Œè‡ªç„¶åªè·å–æ–°å†…å®¹
}
```

**é€‚ç”¨åœºæ™¯**:
- å®Œæ•´ç½‘ç«™å½’æ¡£éœ€æ±‚
- ç½‘ç«™æ›´æ–°é¢‘ç‡å›ºå®š
- ä¸éœ€è¦ç²¾ç¡®çš„æ—¶é—´è¿‡æ»¤

---

### æ–¹æ¡ˆ4: æ··åˆæ¨¡å¼ â­â­â­ æ¨èï¼ˆçµæ´»åœºæ™¯ï¼‰

**å®ç°æ–¹å¼**:
1. æ ¹æ®ç”¨æˆ·éœ€æ±‚æ™ºèƒ½é€‰æ‹©æ¨¡å¼
2. æœ‰æ—¶é—´èŒƒå›´è¦æ±‚ â†’ `search_keyword` + `site:`
3. å®Œæ•´å½’æ¡£éœ€æ±‚ â†’ `crawl_website`

**ä¼˜ç‚¹**:
- âœ… å…¼é¡¾æ•ˆç‡å’Œå®Œæ•´æ€§
- âœ… ç”¨æˆ·ä½“éªŒæœ€ä½³

**ç¼ºç‚¹**:
- âŒ å®ç°å¤æ‚åº¦è¾ƒé«˜
- âŒ éœ€è¦å‰ç«¯æ”¯æŒæ¨¡å¼åˆ‡æ¢

**äº§å“è®¾è®¡**:
```
åˆ›å»ºä»»åŠ¡é¡µé¢:
â”œâ”€ ä»»åŠ¡ç±»å‹: [ä¸‹æ‹‰èœå•]
â”‚  â”œâ”€ å…³é”®è¯æœç´¢ (æ¨èç”¨äºç›‘æ§)
â”‚  â”œâ”€ ç½‘ç«™çˆ¬å– (æ¨èç”¨äºå½’æ¡£)
â”‚  â””â”€ å•é¡µé¢çˆ¬å–
â”‚
â””â”€ æ—¶é—´èŒƒå›´: [æ¡ä»¶æ˜¾ç¤º]
   â””â”€ (ä»…"å…³é”®è¯æœç´¢"æ˜¾ç¤º)
       â”œâ”€ æœ€è¿‘ä¸€å¤©
       â”œâ”€ æœ€è¿‘ä¸€å‘¨
       â”œâ”€ æœ€è¿‘ä¸€ä¸ªæœˆ
       â””â”€ æœ€è¿‘ä¸€å¹´
```

---

## æ¨èæ–¹æ¡ˆæ€»ç»“

### çŸ­æœŸæ–¹æ¡ˆï¼ˆæ— éœ€å¼€å‘ï¼‰

**ä½¿ç”¨ search_keyword + site: å®ç°æ—¶é—´è¿‡æ»¤**

1. åˆ›å»ºä»»åŠ¡æ—¶é€‰æ‹© `search_keyword` æ¨¡å¼
2. åœ¨ query ä¸­ä½¿ç”¨ `site:domain.com` é™å®šåŸŸå
3. é…ç½® `time_range` å‚æ•°

**ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8000/api/v1/search-tasks \
  -H "Content-Type: application/json" \
  -d '{
    "name": "æŠ€æœ¯åšå®¢æœ€æ–°æ–‡ç« ",
    "query": "site:blog.example.com",
    "task_type": "search_keyword",
    "search_config": {
      "limit": 50,
      "time_range": "week"
    },
    "schedule_interval": "DAILY"
  }'
```

---

### ä¸­æœŸæ–¹æ¡ˆï¼ˆå»ºè®®å¼€å‘ï¼‰

**ä¸º crawl_website æ·»åŠ åç«¯æ—¶é—´è¿‡æ»¤**

**å®ç°æ­¥éª¤**:

1. **æ‰©å±• `crawl_config`**:
   ```python
   {
       "limit": 100,
       "max_depth": 2,
       "time_filter": {
           "from_date": "2025-10-01",  # å¯é€‰
           "to_date": "2025-11-01",    # å¯é€‰
           "fallback_strategy": "keep_all"  # å½“ç¼ºå°‘å…ƒæ•°æ®æ—¶çš„ç­–ç•¥
       }
   }
   ```

2. **ä¿®æ”¹çˆ¬å–æµç¨‹** (`src/services/task_scheduler.py`):
   ```python
   async def execute_crawl_task(task: SearchTask):
       # 1. çˆ¬å–ç½‘ç«™
       results = await firecrawl_adapter.crawl(...)

       # 2. åº”ç”¨æ—¶é—´è¿‡æ»¤
       if task.crawl_config.get('time_filter'):
           results = filter_by_publish_time(
               results,
               task.crawl_config['time_filter']
           )

       # 3. ä¿å­˜è¿‡æ»¤åçš„ç»“æœ
       await save_results(results)
   ```

3. **æ·»åŠ å‰ç«¯æ”¯æŒ**:
   ```typescript
   // ä»»åŠ¡åˆ›å»ºè¡¨å•
   {
     taskType: 'crawl_website',
     crawlConfig: {
       limit: 100,
       maxDepth: 2,
       timeFilter: {
         enabled: true,
         fromDate: '2025-10-01',
         toDate: '2025-11-01'
       }
     }
   }
   ```

**å·¥ä½œé‡ä¼°ç®—**: 2-3äººå¤©

---

### é•¿æœŸæ–¹æ¡ˆï¼ˆäº§å“ä¼˜åŒ–ï¼‰

**æ™ºèƒ½ä»»åŠ¡ç±»å‹æ¨è**

æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªåŠ¨æ¨èæœ€åˆé€‚çš„ä»»åŠ¡ç±»å‹ï¼š

```
ç”¨æˆ·è¾“å…¥: "æˆ‘æƒ³ç›‘æ§æŠ€æœ¯åšå®¢çš„æœ€æ–°æ–‡ç« "
ç³»ç»Ÿæ¨è: search_keyword + site: + time_range
åŸå› : éœ€è¦æ—¶é—´è¿‡æ»¤ï¼ŒSearch API æ›´é«˜æ•ˆ

ç”¨æˆ·è¾“å…¥: "å½’æ¡£æ•´ä¸ªç½‘ç«™å†…å®¹"
ç³»ç»Ÿæ¨è: crawl_website
åŸå› : å®Œæ•´çˆ¬å–éœ€æ±‚ï¼ŒCrawl API æ›´é€‚åˆ

ç”¨æˆ·è¾“å…¥: "å®šæœŸæ£€æŸ¥å®˜ç½‘é¦–é¡µå˜åŒ–"
ç³»ç»Ÿæ¨è: scrape_url
åŸå› : å•é¡µé¢ç›‘æ§ï¼ŒScrape API æœ€èŠ‚çœç§¯åˆ†
```

---

## æŠ€æœ¯æ–‡æ¡£æ›´æ–°å»ºè®®

### API æ–‡æ¡£è¡¥å……

**`POST /api/v1/search-tasks` æ–‡æ¡£**:

```markdown
### crawl_config å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| limit | integer | æœ€å¤§çˆ¬å–é¡µé¢æ•° | 100 |
| max_depth | integer | çˆ¬å–æ·±åº¦ | 3 |
| time_filter | object | âš ï¸ å®éªŒæ€§åŠŸèƒ½ï¼šåç«¯æ—¶é—´è¿‡æ»¤ | null |

âš ï¸ **æ—¶é—´è¿‡æ»¤é™åˆ¶**:
- Firecrawl Crawl API ä¸åŸç”Ÿæ”¯æŒæ—¶é—´è¿‡æ»¤
- åç«¯è¿‡æ»¤ä¼šçˆ¬å–æ‰€æœ‰é¡µé¢åå†ç­›é€‰ï¼ˆæ¶ˆè€—ç§¯åˆ†ï¼‰
- å¦‚éœ€é«˜æ•ˆæ—¶é—´è¿‡æ»¤ï¼Œå»ºè®®ä½¿ç”¨ search_keyword æ¨¡å¼

âœ… **æ¨èåšæ³•**:
- ç›‘æ§æœ€æ–°å†…å®¹ â†’ ä½¿ç”¨ `search_keyword` + `time_range`
- å®Œæ•´ç½‘ç«™å½’æ¡£ â†’ ä½¿ç”¨ `crawl_website` + è°ƒåº¦é¢‘ç‡æ§åˆ¶
```

---

## é™„å½•ï¼šæµ‹è¯•ç”¨ä¾‹

### æµ‹è¯•1: search_keyword æ—¶é—´è¿‡æ»¤

```python
# æµ‹è¯• search_keyword çš„ time_range åŠŸèƒ½
async def test_search_with_time_filter():
    task = {
        "name": "æµ‹è¯•æ—¶é—´è¿‡æ»¤",
        "query": "äººå·¥æ™ºèƒ½",
        "task_type": "search_keyword",
        "search_config": {
            "limit": 10,
            "time_range": "week"
        }
    }

    results = await create_and_execute_task(task)

    # éªŒè¯æ‰€æœ‰ç»“æœçš„å‘å¸ƒæ—¶é—´åœ¨ä¸€å‘¨å†…
    now = datetime.utcnow()
    one_week_ago = now - timedelta(days=7)

    for result in results:
        assert result.published_date >= one_week_ago
```

### æµ‹è¯•2: crawl_website å…ƒæ•°æ®éªŒè¯

```python
# éªŒè¯ crawl_website ç»“æœåŒ…å«æ—¶é—´å…ƒæ•°æ®
async def test_crawl_metadata():
    task = {
        "name": "æµ‹è¯•å…ƒæ•°æ®",
        "crawl_url": "https://blog.example.com",
        "task_type": "crawl_website",
        "crawl_config": {
            "limit": 10
        }
    }

    results = await create_and_execute_task(task)

    # ç»Ÿè®¡åŒ…å«æ—¶é—´å…ƒæ•°æ®çš„ç»“æœæ¯”ä¾‹
    with_time_metadata = 0
    for result in results:
        if result.metadata.get('article:published_time'):
            with_time_metadata += 1

    coverage = with_time_metadata / len(results) * 100
    print(f"æ—¶é—´å…ƒæ•°æ®è¦†ç›–ç‡: {coverage}%")
```

---

## æ€»ç»“

### å…³é”®è¦ç‚¹

1. âŒ **crawl_website ä¸æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤**
   - Firecrawl Crawl API çš„è®¾è®¡é™åˆ¶
   - çˆ¬å–æ—¶æ— æ³•é¢„çŸ¥é¡µé¢å‘å¸ƒæ—¶é—´

2. âœ… **search_keyword å·²æ”¯æŒæ—¶é—´è¿‡æ»¤**
   - é€šè¿‡ `time_range` å‚æ•°å®ç°
   - æ”¯æŒ: day, week, month, year

3. ğŸ’¡ **æ¨èè§£å†³æ–¹æ¡ˆ**
   - **çŸ­æœŸ**: ä½¿ç”¨ search_keyword + site: æ›¿ä»£
   - **ä¸­æœŸ**: å®ç°åç«¯å…ƒæ•°æ®è¿‡æ»¤
   - **é•¿æœŸ**: æ™ºèƒ½ä»»åŠ¡ç±»å‹æ¨è

### å†³ç­–å»ºè®®

**å¦‚æœç”¨æˆ·éœ€æ±‚æ˜¯**:
- âœ… ç›‘æ§ç½‘ç«™æœ€æ–°å†…å®¹ â†’ ä½¿ç”¨ search_keyword + time_range
- âœ… å®Œæ•´ç½‘ç«™å½’æ¡£ â†’ ä½¿ç”¨ crawl_website + è°ƒåº¦é¢‘ç‡
- âœ… ç²¾ç¡®æ—¥æœŸèŒƒå›´ â†’ å¼€å‘åç«¯è¿‡æ»¤åŠŸèƒ½

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-11-06
**ä¸‹æ¬¡å®¡é˜…**: éœ€æ±‚ç¡®è®¤åæ›´æ–°å®ç°æ–¹æ¡ˆ
