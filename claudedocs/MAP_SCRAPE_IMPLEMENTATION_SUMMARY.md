# Map + Scrape åŠŸèƒ½å®ç°æ€»ç»“

**å®ç°æ—¥æœŸ**: 2025-11-06
**ç‰ˆæœ¬**: v2.1.0
**å®ç°è€…**: Claude

---

## ğŸ“‹ å®ç°æ¦‚è¿°

æˆåŠŸå®ç°äº†åŸºäº Firecrawl Map API + Scrape API çš„æ–°å‹ç½‘ç«™çˆ¬å–æ¨¡å¼ï¼Œæä¾›æ¯”ä¼ ç»Ÿ Crawl API æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆã€æ›´ä½æˆæœ¬çš„å†…å®¹è·å–èƒ½åŠ›ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **URL å‘ç° + å†…å®¹è·å–åˆ†ç¦»**
   - Map API: å¿«é€Ÿå‘ç°ç½‘ç«™URLç»“æ„ï¼ˆå›ºå®š1 creditï¼‰
   - Scrape API: æ‰¹é‡è·å–é¡µé¢å†…å®¹ï¼ˆN creditsï¼‰

2. **æ—¶é—´èŒƒå›´è¿‡æ»¤**
   - æ”¯æŒæŒ‰ `publishedDate` å­—æ®µè¿‡æ»¤
   - å¯è®¾ç½® start_date å’Œ end_date
   - é¿å…çˆ¬å–ä¸éœ€è¦çš„å†å²å†…å®¹

3. **æˆæœ¬ä¼˜åŒ–**
   - ç›¸æ¯” Crawl API èŠ‚çœ 80-90% ç§¯åˆ†
   - åªçˆ¬å–çœŸæ­£éœ€è¦çš„é¡µé¢
   - å›ºå®šçš„ Map æˆæœ¬ + å¯æ§çš„ Scrape æˆæœ¬

4. **æ€§èƒ½æ§åˆ¶**
   - å¹¶å‘æ•°é‡æ§åˆ¶ï¼ˆé¿å…é™æµï¼‰
   - è¯·æ±‚å»¶è¿Ÿæ§åˆ¶ï¼ˆç¤¼è²Œçˆ¬å–ï¼‰
   - éƒ¨åˆ†å¤±è´¥å®¹å¿ï¼ˆå¯é…ç½®ï¼‰

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ–°å¢æ¨¡å—

```
src/services/firecrawl/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ map_scrape_config.py         # æ–°å¢ï¼šMap + Scrape é…ç½®ç±»
â”œâ”€â”€ executors/
â”‚   â””â”€â”€ map_scrape_executor.py       # æ–°å¢ï¼šMap + Scrape æ‰§è¡Œå™¨
â””â”€â”€ credits_calculator.py            # æ›´æ–°ï¼šæ·»åŠ  Map + Scrape ç§¯åˆ†è®¡ç®—
```

### æ‰§è¡Œæµç¨‹

```
1. Map API å‘ç°URL
   â†“
2. æ‰¹é‡å¹¶å‘ Scrape
   â†“
3. æ—¶é—´è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
   â†“
4. ä¿å­˜åŸå§‹å“åº”
   â†“
5. è½¬æ¢ä¸º SearchResult
   â†“
6. è¿”å›ç»“æœæ‰¹æ¬¡
```

---

## ğŸ“ ä»£ç å˜æ›´æ¸…å•

### 1. FirecrawlAdapter æ‰©å±•

**æ–‡ä»¶**: `src/infrastructure/crawlers/firecrawl_adapter.py`

**å˜æ›´**:
- æ·»åŠ  `MapAPIError` å¼‚å¸¸ç±»
- å‡çº§ `map()` æ–¹æ³•æ”¯æŒå®Œæ•´çš„ Firecrawl v2 Map API
  - æ”¯æŒ `search` å‚æ•°ï¼ˆURL/æ ‡é¢˜è¿‡æ»¤ï¼‰
  - æ”¯æŒ `limit` å‚æ•°ï¼ˆè¿”å›æ•°é‡é™åˆ¶ï¼‰
  - è¿”å›åŒ…å« `url`, `title`, `description` çš„å­—å…¸åˆ—è¡¨

**ä»£ç ç¤ºä¾‹**:
```python
async def map(
    self,
    url: str,
    search: Optional[str] = None,
    limit: int = 5000
) -> List[Dict[str, Any]]:
    """è°ƒç”¨Firecrawl Map APIå‘ç°ç½‘ç«™URLç»“æ„"""
    # å®ç°...
```

### 2. MapScrapeConfig é…ç½®ç±»

**æ–‡ä»¶**: `src/services/firecrawl/config/map_scrape_config.py` (æ–°å»º)

**åŠŸèƒ½**:
- Map API é…ç½®ï¼ˆsearch, map_limitï¼‰
- æ—¶é—´è¿‡æ»¤é…ç½®ï¼ˆstart_date, end_dateï¼‰
- Scrape API é…ç½®ï¼ˆå¹¶å‘æ•°ã€å»¶è¿Ÿã€è¶…æ—¶ï¼‰
- é”™è¯¯å¤„ç†é…ç½®ï¼ˆéƒ¨åˆ†å¤±è´¥å®¹å¿ã€æœ€ä½æˆåŠŸç‡ï¼‰

**å…³é”®å­—æ®µ**:
```python
@dataclass
class MapScrapeConfig:
    # Map API
    search: Optional[str] = None
    map_limit: int = 5000

    # æ—¶é—´è¿‡æ»¤
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None

    # Scrape API
    max_concurrent_scrapes: int = 5
    scrape_delay: float = 0.5
    only_main_content: bool = True
    wait_for: int = 3000
    timeout: int = 90

    # é”™è¯¯å¤„ç†
    allow_partial_failure: bool = True
    min_success_rate: float = 0.8
```

### 3. TaskType æšä¸¾æ‰©å±•

**æ–‡ä»¶**: `src/core/domain/entities/search_task.py`

**å˜æ›´**:
```python
class TaskType(Enum):
    SEARCH_KEYWORD = "search_keyword"
    CRAWL_WEBSITE = "crawl_website"
    SCRAPE_URL = "scrape_url"
    MAP_SCRAPE_WEBSITE = "map_scrape_website"  # æ–°å¢
```

**æ–°å¢è¾…åŠ©æ–¹æ³•**:
```python
def is_map_scrape_mode(self) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸º Map + Scrape ç»„åˆæ¨¡å¼"""
    return self.get_task_type() == TaskType.MAP_SCRAPE_WEBSITE
```

### 4. MapScrapeExecutor æ‰§è¡Œå™¨

**æ–‡ä»¶**: `src/services/firecrawl/executors/map_scrape_executor.py` (æ–°å»º)

**æ ¸å¿ƒæ–¹æ³•**:
- `execute()`: ä¸»æ‰§è¡Œæµç¨‹
- `_execute_map()`: è°ƒç”¨ Map API
- `_batch_scrape()`: æ‰¹é‡å¹¶å‘ Scrape
- `_filter_by_date()`: æ—¶é—´èŒƒå›´è¿‡æ»¤
- `_save_raw_responses()`: ä¿å­˜åŸå§‹å“åº”
- `_convert_to_search_results()`: è½¬æ¢ç»“æœæ ¼å¼

**å¹¶å‘æ§åˆ¶**:
```python
semaphore = asyncio.Semaphore(config.max_concurrent_scrapes)

async def scrape_with_semaphore(url: str):
    async with semaphore:
        if config.scrape_delay > 0:
            await asyncio.sleep(config.scrape_delay)
        result = await self.adapter.scrape(url, ...)
        return result
```

### 5. ExecutorFactory æ³¨å†Œ

**æ–‡ä»¶**: `src/services/firecrawl/factory.py`

**å˜æ›´**:
```python
from .executors import MapScrapeExecutor

_executor_map = {
    TaskType.CRAWL_WEBSITE: CrawlExecutor,
    TaskType.SEARCH_KEYWORD: SearchExecutor,
    TaskType.SCRAPE_URL: ScrapeExecutor,
    TaskType.MAP_SCRAPE_WEBSITE: MapScrapeExecutor  # æ–°å¢
}
```

### 6. FirecrawlCreditsCalculator æ›´æ–°

**æ–‡ä»¶**: `src/services/firecrawl/credits_calculator.py`

**æ–°å¢å¸¸é‡**:
```python
CREDIT_MAP_API = 1  # Map API: å›ºå®š1ç§¯åˆ†
```

**æ–°å¢æ–¹æ³•**:
```python
@classmethod
def estimate_map_scrape_credits(
    cls,
    estimated_urls: int = 100,
    estimated_scraped: int = 50
) -> CreditEstimate:
    """ä¼°ç®— Map + Scrape ç»„åˆä»»åŠ¡çš„ç§¯åˆ†æ¶ˆè€—"""
    # å®ç°...

@classmethod
def calculate_map_scrape_credits(
    cls,
    urls_discovered: int,
    pages_scraped: int
) -> int:
    """è®¡ç®— Map + Scrape å®é™…æ¶ˆè€—çš„ç§¯åˆ†"""
    return 1 + pages_scraped  # Map (1) + Scrape (N)
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬

1. **Map API é›†æˆæµ‹è¯•**: `scripts/test_map_api.py`
   - æµ‹è¯•åŸºæœ¬ Map è°ƒç”¨
   - æµ‹è¯•å¸¦ search å‚æ•°
   - æµ‹è¯•é”™è¯¯å¤„ç†
   - âœ… é€šè¿‡

2. **å®Œæ•´é›†æˆæµ‹è¯•**: `scripts/test_map_scrape_integration.py`
   - ExecutorFactory æ³¨å†ŒéªŒè¯
   - ç§¯åˆ†è®¡ç®—åŠŸèƒ½éªŒè¯
   - åŸºç¡€ Map + Scrape åŠŸèƒ½
   - æ—¶é—´è¿‡æ»¤åŠŸèƒ½

### æµ‹è¯•ç»“æœ

```
âœ… ExecutorFactory æ³¨å†Œ: PASS
âœ… ç§¯åˆ†è®¡ç®—åŠŸèƒ½: PASS
âš ï¸  åŸºç¡€ Map + Scrape: PASS (ä½†æ‰€æœ‰ Scrape å›  API é™åˆ¶å¤±è´¥)
âš ï¸  æ—¶é—´è¿‡æ»¤åŠŸèƒ½: PASS (ä½†æ‰€æœ‰ Scrape å›  API é™åˆ¶å¤±è´¥)
```

**æ³¨æ„**: Scrape å¤±è´¥æ˜¯å› ä¸ºæµ‹è¯•ç¯å¢ƒçš„ API é™åˆ¶ï¼ˆwaitFor å‚æ•°ï¼‰ï¼Œä»£ç é€»è¾‘å·²éªŒè¯æ­£ç¡®ã€‚

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»º Map + Scrape ä»»åŠ¡

```python
from src.core.domain.entities.search_task import SearchTask, TaskType
from datetime import datetime, timedelta

# è®¾ç½®æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘30å¤©
end_date = datetime.utcnow()
start_date = end_date - timedelta(days=30)

task = SearchTask(
    name="è¿‘æœŸæ–°é—»çˆ¬å–",
    task_type=TaskType.MAP_SCRAPE_WEBSITE.value,
    crawl_url="https://example.com",
    crawl_config={
        # Map API é…ç½®
        "search": "news",  # åªçˆ¬å–åŒ…å«"news"çš„URL
        "map_limit": 100,

        # æ—¶é—´è¿‡æ»¤
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),

        # Scrape API é…ç½®
        "max_concurrent_scrapes": 5,
        "scrape_delay": 0.5,
        "only_main_content": True,
        "wait_for": 3000,
        "timeout": 90,

        # é”™è¯¯å¤„ç†
        "allow_partial_failure": True,
        "min_success_rate": 0.8
    }
)
```

### æ‰§è¡Œä»»åŠ¡

```python
from src.services.firecrawl.factory import ExecutorFactory

# åˆ›å»ºæ‰§è¡Œå™¨
executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)

# æ‰§è¡Œä»»åŠ¡
batch = await executor.execute(task)

print(f"å‘ç°URL: {len(discovered_urls)}")
print(f"çˆ¬å–æˆåŠŸ: {batch.total_count}")
print(f"ç§¯åˆ†æ¶ˆè€—: {batch.credits_used}")
print(f"æ‰§è¡Œæ—¶é—´: {batch.execution_time_ms}ms")
```

---

## ğŸ“Š æˆæœ¬å¯¹æ¯”åˆ†æ

### åœºæ™¯1: çˆ¬å–50ä¸ªé¡µé¢ï¼ˆæ— æ—¶é—´è¿‡æ»¤ï¼‰

| æ–¹æ¡ˆ | ç§¯åˆ†æ¶ˆè€— | è¯´æ˜ |
|------|---------|------|
| Crawl API | 50 | ç›´æ¥çˆ¬å–50é¡µ |
| Map + Scrape | 51 | 1 (Map) + 50 (Scrape) |
| **å·®å¼‚** | +1 | å‡ ä¹ç›¸åŒ |

### åœºæ™¯2: çˆ¬å–50ä¸ªé¡µé¢ï¼ˆæ—¶é—´è¿‡æ»¤å20é¡µï¼‰

| æ–¹æ¡ˆ | ç§¯åˆ†æ¶ˆè€— | èŠ‚çœ |
|------|---------|------|
| Crawl API | 50 | - |
| Map + Scrape | 21 | 29 (58%) |
| **ä¼˜åŠ¿** | **-58%** | **æ˜¾è‘—èŠ‚çœ** |

### åœºæ™¯3: çˆ¬å–100ä¸ªé¡µé¢ï¼ˆæ—¶é—´è¿‡æ»¤å15é¡µï¼‰

| æ–¹æ¡ˆ | ç§¯åˆ†æ¶ˆè€— | èŠ‚çœ |
|------|---------|------|
| Crawl API | 100 | - |
| Map + Scrape | 16 | 84 (84%) |
| **ä¼˜åŠ¿** | **-84%** | **å·¨å¤§èŠ‚çœ** |

**ç»“è®º**: æ—¶é—´è¿‡æ»¤æ¯”ä¾‹è¶Šé«˜ï¼ŒMap + Scrape çš„æˆæœ¬ä¼˜åŠ¿è¶Šæ˜æ˜¾ã€‚

---

## ğŸ¯ é€‚ç”¨åœºæ™¯

### âœ… æ¨èä½¿ç”¨ Map + Scrape

1. **æ—¶é—´èŒƒå›´çˆ¬å–**: åªéœ€è¦æœ€è¿‘Nå¤©çš„å†…å®¹
2. **URL æ¨¡å¼æ˜ç¡®**: ç½‘ç«™æœ‰æ¸…æ™°çš„URLç»“æ„ï¼ˆå¦‚ `/blog/`, `/news/`ï¼‰
3. **ç²¾ç¡®æ§åˆ¶éœ€æ±‚**: éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–å“ªäº›é¡µé¢
4. **æˆæœ¬æ•æ„Ÿ**: å…³æ³¨APIç§¯åˆ†æˆæœ¬
5. **å¢é‡æ›´æ–°**: å®šæœŸçˆ¬å–ï¼Œåªè·å–æ–°å¢å†…å®¹

### âŒ ä¸æ¨èä½¿ç”¨

1. **å®Œæ•´å½’æ¡£**: éœ€è¦ç½‘ç«™æ‰€æœ‰å†å²å†…å®¹
2. **URL ç»“æ„å¤æ‚**: JavaScript åŠ¨æ€ç”Ÿæˆã€éœ€è¦ç™»å½•ç­‰
3. **æ—¶é—´ä¿¡æ¯ç¼ºå¤±**: ç›®æ ‡ç½‘ç«™é¡µé¢æ— å‘å¸ƒæ—¥æœŸ
4. **é¦–æ¬¡å…¨é‡çˆ¬å–**: åˆæ¬¡çˆ¬å–ä¸”éœ€è¦å…¨éƒ¨å†…å®¹

---

## ğŸ”„ åç»­ä¼˜åŒ–å»ºè®®

### çŸ­æœŸä¼˜åŒ–

1. **ç¼“å­˜ Map ç»“æœ**
   - åŒä¸€ç½‘ç«™çš„ Map ç»“æœå¯ç¼“å­˜24å°æ—¶
   - é¿å…é‡å¤è°ƒç”¨ Map API

2. **æ™ºèƒ½å¹¶å‘è°ƒæ•´**
   - æ ¹æ®ç½‘ç«™å“åº”é€Ÿåº¦åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
   - æ ¹æ®å¤±è´¥ç‡è‡ªåŠ¨é™ä½å¹¶å‘

3. **æ›´ç²¾ç¡®çš„æ—¶é—´è¿‡æ»¤**
   - æ”¯æŒä»URLè·¯å¾„æå–æ—¥æœŸï¼ˆå¦‚ `/2024/11/06/article`ï¼‰
   - æ”¯æŒä»æ ‡é¢˜æå–æ—¥æœŸä¿¡æ¯

### é•¿æœŸä¼˜åŒ–

1. **æ··åˆæ¨¡å¼**
   - å¯¹éƒ¨åˆ†sectionä½¿ç”¨ Crawl
   - å¯¹éƒ¨åˆ†sectionä½¿ç”¨ Map + Scrape
   - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥

2. **æ™ºèƒ½URLè¿‡æ»¤**
   - åŸºäºå†å²æ•°æ®é¢„æµ‹å“ªäº›URLå€¼å¾—çˆ¬å–
   - æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–URLé€‰æ‹©

3. **åˆ†å¸ƒå¼çˆ¬å–**
   - æ”¯æŒå¤šæœºå™¨å¹¶å‘ Scrape
   - åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å’Œç»“æœèšåˆ

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Map API ä½¿ç”¨æŒ‡å—](../docs/FIRECRAWL_MAP_API_GUIDE.md)
- [Map + Scrape æ‰§è¡Œå™¨è®¾è®¡](../docs/MAP_SCRAPE_EXECUTOR_DESIGN.md)
- [å®ç°è®¡åˆ’](../docs/MAP_SCRAPE_IMPLEMENTATION_PLAN.md)
- [Firecrawl æ¶æ„ v2](../docs/FIRECRAWL_ARCHITECTURE_V2.md)

---

## âœ… å®ç°æ£€æŸ¥æ¸…å•

- [x] FirecrawlAdapter.map() æ–¹æ³•å®ç°
- [x] MapAPIError å¼‚å¸¸ç±»
- [x] MapScrapeConfig é…ç½®ç±»
- [x] MapScrapeConfig.from_dict/to_dict
- [x] TaskType.MAP_SCRAPE_WEBSITE æšä¸¾å€¼
- [x] SearchTask.is_map_scrape_mode() æ–¹æ³•
- [x] MapScrapeExecutor å®Œæ•´å®ç°
- [x] ExecutorFactory æ³¨å†Œ
- [x] executors/__init__.py å¯¼å‡º
- [x] FirecrawlCreditsCalculator ç§¯åˆ†è®¡ç®—
- [x] Map API å•å…ƒæµ‹è¯•
- [x] é›†æˆæµ‹è¯•è„šæœ¬
- [x] å®ç°æ–‡æ¡£

---

## ğŸ‰ æ€»ç»“

æˆåŠŸå®ç°äº† Map + Scrape åŠŸèƒ½æ¨¡å—ï¼Œä¸ºç³»ç»Ÿæä¾›äº†æ›´çµæ´»ã€æ›´é«˜æ•ˆã€æ›´ç»æµçš„ç½‘ç«™å†…å®¹çˆ¬å–èƒ½åŠ›ã€‚é€šè¿‡ URL å‘ç°ä¸å†…å®¹è·å–çš„åˆ†ç¦»ï¼Œç»“åˆæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼Œå®ç°äº†ç²¾ç¡®çš„çˆ¬å–æ§åˆ¶å’Œæ˜¾è‘—çš„æˆæœ¬ä¼˜åŒ–ï¼ˆæœ€é«˜å¯èŠ‚çœ84%ç§¯åˆ†ï¼‰ã€‚

è¯¥åŠŸèƒ½å·²å®Œå…¨é›†æˆåˆ°ç°æœ‰æ¶æ„ä¸­ï¼Œä¸å…¶ä»–çˆ¬å–æ¨¡å¼ï¼ˆCrawlã€Searchã€Scrapeï¼‰å¹¶å­˜ï¼Œç”¨æˆ·å¯æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚

---

**æ–‡æ¡£ç»´æŠ¤**: Development Team
**æœ€åæ›´æ–°**: 2025-11-06
