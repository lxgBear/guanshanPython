# v2.1.1 å®ç°æ€»ç»“

**ç‰ˆæœ¬**: v2.1.1
**å®æ–½æ—¥æœŸ**: 2025-11-07
**å®æ–½å†…å®¹**: å†…å®¹å»é‡ã€URLå»é‡ã€å®Œæ•´HTMLè·å–

---

## ğŸ“‹ å®ç°çš„åŠŸèƒ½

### 1. Content Hash å»é‡åŠŸèƒ½ âœ…

**ç›®çš„**: é˜²æ­¢é‡å¤å­˜å‚¨ç›¸åŒå†…å®¹ï¼ŒèŠ‚çœæ•°æ®åº“ç©ºé—´å’Œ Firecrawl API ç§¯åˆ†

**å®ç°ç»†èŠ‚**:
- åœ¨ `SearchResult` å®ä½“æ·»åŠ  `content_hash` å­—æ®µï¼ˆå¯é€‰ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼‰
- å“ˆå¸Œç®—æ³•ï¼šSHA256(URL + æ ‡é¢˜ + markdownå†…å®¹å‰500å­—ç¬¦)ï¼Œæˆªå–å‰16ä½
- è‡ªåŠ¨ç”Ÿæˆï¼š`generate_content_hash()` æ–¹æ³•è®¡ç®—å“ˆå¸Œå€¼
- ä¿å­˜å‰ç¡®ä¿ï¼š`ensure_content_hash()` åœ¨ä¿å­˜å‰è‡ªåŠ¨ç”Ÿæˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰

**æ•°æ®åº“ç´¢å¼•**:
```python
await search_results.create_index("content_hash")  # å†…å®¹å»é‡æŸ¥è¯¢
```

**å»é‡é€»è¾‘** (in `SearchResultRepository.save_results()`):
```python
# 1. ä¸ºæ‰€æœ‰ç»“æœç”Ÿæˆ content_hash
for result in results:
    result.ensure_content_hash()

# 2. æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„å“ˆå¸Œ
existing_hashes = set()
async for doc in collection.find(
    {"content_hash": {"$in": content_hashes}},
    {"content_hash": 1}
):
    existing_hashes.add(doc.get("content_hash"))

# 3. è¿‡æ»¤å‡ºæ–°ç»“æœ
new_results = [r for r in results if r.content_hash not in existing_hashes]

# 4. ä¿å­˜æ–°ç»“æœ
if new_results:
    result_dicts = [self._result_to_dict(result) for result in new_results]
    await collection.insert_many(result_dicts)

# 5. è¿”å›ç»Ÿè®¡ä¿¡æ¯
return {
    "saved": len(new_results),
    "duplicates": len(results) - len(new_results),
    "total": len(results)
}
```

**æ—¥å¿—è¾“å‡ºç¤ºä¾‹**:
```
ä¿å­˜æœç´¢ç»“æœæˆåŠŸ: æ–°å¢12æ¡, è·³è¿‡é‡å¤0æ¡
```

---

### 2. URL å»é‡åŠŸèƒ½ âœ…

**ç›®çš„**: åœ¨ Map+Scrape æ¨¡å¼ä¸­ï¼Œé¿å…é‡å¤çˆ¬å–å·²ç»æŠ“å–è¿‡çš„ URLï¼ŒèŠ‚çœ API è°ƒç”¨æˆæœ¬

**å®ç°ä½ç½®**: `MapScrapeExecutor.execute()`

**å»é‡æ—¶æœº**: Map API è¿”å› URL åˆ—è¡¨åï¼ŒScrape API è°ƒç”¨å‰

**å®ç°ç»†èŠ‚**:
```python
# Map API å‘ç° URL
discovered_urls = await self._execute_map(task.crawl_url, config)

# URL å»é‡æ£€æŸ¥ï¼ˆv2.1.1ï¼‰
if config.enable_dedup:
    existing_urls = await self.result_repo.check_existing_urls(
        task_id=str(task.id),
        urls=discovered_urls
    )

    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„ URL
    new_urls = [url for url in discovered_urls if url not in existing_urls]

    logger.info(
        f"âœ… URLå»é‡: å‘ç°{len(discovered_urls)}ä¸ª, "
        f"å·²å­˜åœ¨{len(existing_urls)}ä¸ª, "
        f"å¾…çˆ¬å–{len(new_urls)}ä¸ª"
    )

    discovered_urls = new_urls
```

**æ•°æ®åº“ç´¢å¼•**:
```python
await search_results.create_index([("task_id", 1), ("url", 1)])  # URLå»é‡æŸ¥è¯¢
```

**è¾…åŠ©æ–¹æ³•** (in `SearchResultRepository`):
```python
async def check_existing_urls(self, task_id: str, urls: List[str]) -> set:
    """æ£€æŸ¥å“ªäº›URLå·²å­˜åœ¨äºæ•°æ®åº“"""
    collection = await self._get_collection()
    existing_urls = set()
    async for doc in collection.find(
        {"task_id": task_id, "url": {"$in": urls}},
        {"url": 1}
    ):
        existing_urls.add(doc.get("url"))
    return existing_urls
```

---

### 3. å®Œæ•´ HTML è·å–é…ç½® âœ…

**ç›®çš„**: è·å–å®Œæ•´çš„ HTML å†…å®¹è€Œä¸æ˜¯ç»è¿‡è¿‡æ»¤çš„ä¸»è¦å†…å®¹ï¼Œä¸ºä¸‹æ¸¸ AI å¤„ç†æä¾›æ›´å¤šä¿¡æ¯

**é…ç½®æ›´æ”¹**:

#### Dataclass é»˜è®¤å€¼
æ‰€æœ‰é…ç½®ç±»çš„é»˜è®¤å€¼å·²æ›´æ–°ï¼š

```python
# CrawlConfig
only_main_content: bool = False  # v2.1.1: è·å–å®Œæ•´HTML
exclude_tags: List[str] = field(default_factory=lambda: [])  # v2.1.1: ä¸æ’é™¤ä»»ä½•æ ‡ç­¾

# SearchConfig
only_main_content: bool = False  # v2.1.1: è·å–å®Œæ•´HTML
exclude_tags: List[str] = field(default_factory=lambda: [])  # v2.1.1: ä¸æ’é™¤ä»»ä½•æ ‡ç­¾

# ScrapeConfig
only_main_content: bool = False  # v2.1.1: è·å–å®Œæ•´HTML
exclude_tags: List[str] = field(default_factory=lambda: [])  # v2.1.1: ä¸æ’é™¤ä»»ä½•æ ‡ç­¾

# MapScrapeConfig
only_main_content: bool = False  # v2.1.1: è·å–å®Œæ•´HTML
exclude_tags: List[str] = field(default_factory=lambda: [])  # v2.1.1: ä¸æ’é™¤ä»»ä½•æ ‡ç­¾
```

#### from_dict æ–¹æ³•ä¿®å¤

**å…³é”®ä¿®å¤**: æ‰€æœ‰é…ç½®ç±»çš„ `from_dict()` æ–¹æ³•ä¸­çš„ fallback é»˜è®¤å€¼ä» `True` æ”¹ä¸º `False`

ä¿®å¤å‰ï¼ˆâŒ é”™è¯¯ï¼‰:
```python
only_main_content=data.get('only_main_content', True),  # âŒ è¦†ç›–äº† dataclass é»˜è®¤å€¼
exclude_tags=data.get('exclude_tags', ['nav', 'footer', 'header'])  # âŒ æ—§çš„é»˜è®¤å€¼
```

ä¿®å¤åï¼ˆâœ… æ­£ç¡®ï¼‰:
```python
only_main_content=data.get('only_main_content', False),  # âœ… v2.1.1: é»˜è®¤ False è·å–å®Œæ•´HTML
exclude_tags=data.get('exclude_tags', [])  # âœ… v2.1.1: é»˜è®¤ç©ºåˆ—è¡¨
```

**å½±å“çš„é…ç½®ç±»**:
- `CrawlConfig.from_dict()`
- `SearchConfig.from_dict()`
- `ScrapeConfig.from_dict()`
- `MapScrapeConfig.from_dict()`

**é‡è¦æ€§**: è¿™ä¸ªä¿®å¤ç¡®ä¿äº†å³ä½¿ç”¨æˆ·åˆ›å»ºä»»åŠ¡æ—¶ä¸æ˜¾å¼ä¼ å…¥é…ç½®ï¼Œä¹Ÿä¼šä½¿ç”¨ v2.1.1 çš„æ–°é»˜è®¤å€¼ï¼ˆå®Œæ•´HTMLï¼‰è€Œä¸æ˜¯æ—§çš„é»˜è®¤å€¼ï¼ˆè¿‡æ»¤HTMLï¼‰ã€‚

---

## ğŸ—‚ï¸ ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒå®ä½“å±‚
1. **`src/core/domain/entities/search_result.py`**
   - æ·»åŠ  `content_hash: Optional[str]` å­—æ®µ
   - å®ç° `generate_content_hash()` æ–¹æ³•
   - å®ç° `ensure_content_hash()` æ–¹æ³•

### æ•°æ®è®¿é—®å±‚
2. **`src/infrastructure/database/repositories.py`**
   - ä¿®æ”¹ `_result_to_dict()`: æ·»åŠ  content_hash å­—æ®µæ˜ å°„
   - ä¿®æ”¹ `_dict_to_result()`: ä»æ•°æ®åº“è¯»å– content_hash
   - ä¿®æ”¹ `save_results()`: å®ç°å»é‡é€»è¾‘å’Œç»Ÿè®¡
   - æ·»åŠ  `check_existing_urls()`: URL å­˜åœ¨æ€§æ£€æŸ¥

3. **`src/infrastructure/database/connection.py`**
   - æ·»åŠ  `search_results.content_hash` ç´¢å¼•
   - æ·»åŠ  `search_results.(task_id, url)` å¤åˆç´¢å¼•
   - æ›´æ–°æ—¥å¿—ï¼š`âœ… å®šæ—¶æœç´¢ç»“æœç´¢å¼•åˆ›å»ºå®Œæˆï¼ˆå«v2.1.1å»é‡ç´¢å¼•ï¼‰`

### ä¸šåŠ¡é€»è¾‘å±‚
4. **`src/services/firecrawl/executors/map_scrape_executor.py`**
   - æ·»åŠ  `SearchResultRepository` å¯¼å…¥
   - åœ¨ `__init__` åˆå§‹åŒ– `result_repo`
   - åœ¨ `execute()` æ–¹æ³•ä¸­æ·»åŠ  URL å»é‡é€»è¾‘
   - URL å»é‡å‘ç”Ÿåœ¨ Map API è¿”å›åã€Scrape API è°ƒç”¨å‰

### é…ç½®å±‚
5. **`src/services/firecrawl/config/task_config.py`**
   - `CrawlConfig`: dataclass é»˜è®¤å€¼ + from_dict fallback å€¼
   - `SearchConfig`: dataclass é»˜è®¤å€¼ + from_dict fallback å€¼
   - `ScrapeConfig`: dataclass é»˜è®¤å€¼ + from_dict fallback å€¼

6. **`src/services/firecrawl/config/map_scrape_config.py`**
   - `MapScrapeConfig`: dataclass é»˜è®¤å€¼ + from_dict fallback å€¼
   - æ·»åŠ  `enable_dedup: bool = True` é…ç½®é€‰é¡¹

---

## ğŸ“Š æ•°æ®åº“å˜æ›´

### ç´¢å¼•åˆ›å»º

```python
# search_results é›†åˆ
await search_results.create_index("content_hash")  # å†…å®¹å»é‡
await search_results.create_index([("task_id", 1), ("url", 1)])  # URLå»é‡
```

### å­—æ®µå˜æ›´

**search_results é›†åˆæ–°å¢å­—æ®µ**:
```javascript
{
    "content_hash": "abc123def456789...",  // SHA256å“ˆå¸Œå‰16ä½ï¼ˆå¯é€‰ï¼‰
    // ... å…¶ä»–å­—æ®µä¿æŒä¸å˜
}
```

**å‘åå…¼å®¹æ€§**: âœ… å®Œå…¨å…¼å®¹
- æ—§æ•°æ®æ²¡æœ‰ `content_hash` å­—æ®µï¼Œä¼šåœ¨ä¸‹æ¬¡æ›´æ–°æ—¶è‡ªåŠ¨ç”Ÿæˆ
- æ–°æ•°æ®ä¼šåœ¨ä¿å­˜å‰è‡ªåŠ¨ç”Ÿæˆ `content_hash`

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### è‡ªåŠ¨åŒ–æµ‹è¯•
åˆ›å»ºäº†æµ‹è¯•è„šæœ¬ `scripts/test_task_244863130303672320.py` ç”¨äºéªŒè¯åŠŸèƒ½ï¼ˆå·²æ¸…ç†ï¼‰

### å®é™…è¿è¡ŒéªŒè¯

**ä»»åŠ¡**: 244879584026255360 (æ— å£°ä¹‹å£° 1107)
**ä»»åŠ¡ç±»å‹**: crawl_website
**æ‰§è¡Œæ—¶é—´**: 2025-11-07 01:44:02

**ç»“æœ**:
```
âœ… çˆ¬å–å®Œæˆ: è·å¾— 12 ä¸ªé¡µé¢
âœ… ä¿å­˜æœç´¢ç»“æœæˆåŠŸ: æ–°å¢12æ¡, è·³è¿‡é‡å¤0æ¡
âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ: æ— å£°ä¹‹å£° 1107 | ç»“æœæ•°: 12 | è€—æ—¶: 28282ms
```

**éªŒè¯ç»“è®º**:
- âœ… å»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼ˆ12æ¡æ–°ç»“æœï¼Œ0æ¡é‡å¤ï¼‰
- âœ… å®Œæ•´HTMLè·å–é…ç½®å·²ç”Ÿæ•ˆï¼ˆæ–°å»ºä»»åŠ¡ä½¿ç”¨äº† v2.1.1 é…ç½®ï¼‰
- âœ… æ•°æ®åº“ç´¢å¼•åˆ›å»ºæˆåŠŸ
- âœ… ç³»ç»Ÿè¿è¡Œç¨³å®š

---

## ğŸ”§ é…ç½®è¯´æ˜

### å»é‡åŠŸèƒ½é…ç½®

**é»˜è®¤å¯ç”¨**: æ˜¯

**ç¦ç”¨æ–¹æ³•** (å¦‚æœéœ€è¦):
```python
# MapScrapeConfig
config = MapScrapeConfig(
    enable_dedup=False,  # ç¦ç”¨å»é‡
    # ... å…¶ä»–é…ç½®
)
```

**SearchResultRepository.save_results**:
```python
await repo.save_results(results, enable_dedup=True)  # å¯ç”¨å»é‡ï¼ˆé»˜è®¤ï¼‰
await repo.save_results(results, enable_dedup=False)  # ç¦ç”¨å»é‡
```

### å®Œæ•´HTMLé…ç½®

**é»˜è®¤å¯ç”¨**: æ˜¯ï¼ˆv2.1.1 èµ·ï¼‰

**è‡ªå®šä¹‰æ–¹æ³•** (å¦‚æœéœ€è¦æ¢å¤æ—§è¡Œä¸º):
```python
# åœ¨ä»»åŠ¡é…ç½®ä¸­æ˜¾å¼æŒ‡å®š
task_config = {
    "only_main_content": True,  # åªæå–ä¸»è¦å†…å®¹
    "exclude_tags": ["nav", "footer", "header"],  # æ’é™¤ç‰¹å®šæ ‡ç­¾
}
```

---

## ğŸ“ˆ æ€§èƒ½å½±å“

### å†…å®¹å»é‡
- **é¢å¤–æŸ¥è¯¢**: 1æ¬¡æ•°æ®åº“æŸ¥è¯¢ï¼ˆæ‰¹é‡æ£€æŸ¥å“ˆå¸Œï¼‰
- **æ—¶é—´å¤æ‚åº¦**: O(n) å…¶ä¸­ n æ˜¯ç»“æœæ•°é‡
- **å†…å­˜å ç”¨**: O(n) å­˜å‚¨å“ˆå¸Œé›†åˆ
- **ç´¢å¼•æ”¯æŒ**: æ˜¯ï¼Œä½¿ç”¨ `content_hash` ç´¢å¼•

### URLå»é‡
- **é¢å¤–æŸ¥è¯¢**: 1æ¬¡æ•°æ®åº“æŸ¥è¯¢ï¼ˆæ‰¹é‡æ£€æŸ¥URLï¼‰
- **æ—¶é—´å¤æ‚åº¦**: O(n) å…¶ä¸­ n æ˜¯URLæ•°é‡
- **èŠ‚çœæˆæœ¬**: é¿å…é‡å¤è°ƒç”¨ Firecrawl Scrape API
- **ç´¢å¼•æ”¯æŒ**: æ˜¯ï¼Œä½¿ç”¨ `(task_id, url)` å¤åˆç´¢å¼•

### å®Œæ•´HTML
- **å­˜å‚¨å¢åŠ **: HTML å†…å®¹æ¯”è¿‡æ»¤åçš„å†…å®¹æ›´å¤§
- **APIæˆæœ¬**: æ— é¢å¤–æˆæœ¬
- **ä¸‹æ¸¸å¤„ç†**: ä¸º AI å¤„ç†æä¾›æ›´å¤šä¸Šä¸‹æ–‡

---

## ğŸ”„ å‘åå…¼å®¹æ€§

### æ•°æ®å…¼å®¹æ€§
âœ… **å®Œå…¨å…¼å®¹**
- æ—§æ•°æ®æ²¡æœ‰ `content_hash` å­—æ®µä¼šåœ¨è®¿é—®æ—¶è‡ªåŠ¨ç”Ÿæˆ
- æ–°æ•°æ®åœ¨ä¿å­˜æ—¶è‡ªåŠ¨ç”Ÿæˆ `content_hash`
- ä¸ä¼šç ´åç°æœ‰æ•°æ®ç»“æ„

### APIå…¼å®¹æ€§
âœ… **å®Œå…¨å…¼å®¹**
- æ‰€æœ‰ç°æœ‰ API ä¿æŒä¸å˜
- å»é‡åŠŸèƒ½é»˜è®¤å¯ç”¨ï¼Œå¯é€‰ç¦ç”¨
- é…ç½®å˜æ›´å‘åå…¼å®¹ï¼ˆç”¨æˆ·å¯ä»¥æ˜¾å¼æŒ‡å®šæ—§é…ç½®ï¼‰

### é…ç½®å…¼å®¹æ€§
âœ… **å®Œå…¨å…¼å®¹**
- æ—§ä»»åŠ¡çš„é…ç½®ä¸å—å½±å“ï¼ˆæ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯æ˜¾å¼é…ç½®ï¼‰
- æ–°ä»»åŠ¡é»˜è®¤ä½¿ç”¨ v2.1.1 é…ç½®
- ç”¨æˆ·å¯ä»¥åœ¨åˆ›å»ºä»»åŠ¡æ—¶è¦†ç›–é»˜è®¤é…ç½®

---

## ğŸ“ åç»­å»ºè®®

### 1. ç›‘æ§å»é‡æ•ˆæœ
å»ºè®®æ·»åŠ ç›‘æ§æŒ‡æ ‡ï¼š
- æ¯æ¬¡ä¿å­˜çš„å»é‡ç‡ï¼ˆduplicates / totalï¼‰
- URLå»é‡èŠ‚çœçš„ Scrape API è°ƒç”¨æ¬¡æ•°
- å­˜å‚¨ç©ºé—´èŠ‚çœé‡

### 2. å»é‡ç­–ç•¥ä¼˜åŒ–
å½“å‰ä½¿ç”¨ç®€å•çš„å“ˆå¸ŒåŒ¹é…ï¼Œæœªæ¥å¯ä»¥è€ƒè™‘ï¼š
- æ¨¡ç³Šå»é‡ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
- æ—¶é—´çª—å£å»é‡ï¼ˆåªåœ¨ç‰¹å®šæ—¶é—´èŒƒå›´å†…å»é‡ï¼‰
- è·¨ä»»åŠ¡å»é‡ï¼ˆä¸åŒä»»åŠ¡é—´çš„å»é‡ï¼‰

### 3. HTMLå¤„ç†ä¼˜åŒ–
å®Œæ•´HTMLå¸¦æ¥æ›´å¤§çš„å­˜å‚¨éœ€æ±‚ï¼Œå¯ä»¥è€ƒè™‘ï¼š
- HTMLå‹ç¼©å­˜å‚¨
- åˆ†ç¦»HTMLå­˜å‚¨ï¼ˆä¸“é—¨çš„æ–‡æ¡£å­˜å‚¨ï¼‰
- æŒ‰éœ€åŠ è½½HTMLï¼ˆåˆ—è¡¨ä¸è¿”å›HTMLï¼Œè¯¦æƒ…æ‰è¿”å›ï¼‰

### 4. æ€§èƒ½ä¼˜åŒ–
å¦‚æœå»é‡æŸ¥è¯¢æˆä¸ºç“¶é¢ˆï¼Œå¯ä»¥è€ƒè™‘ï¼š
- ä½¿ç”¨ Bloom Filter é¢„è¿‡æ»¤
- åˆ†æ‰¹æ¬¡å»é‡æ£€æŸ¥
- ç¼“å­˜çƒ­ç‚¹å“ˆå¸Œ

---

## âœ… å®æ–½å®Œæˆæ¸…å•

- [x] Content Hash å­—æ®µæ·»åŠ 
- [x] Content Hash ç”Ÿæˆæ–¹æ³•
- [x] Content Hash å»é‡é€»è¾‘
- [x] Content Hash æ•°æ®åº“ç´¢å¼•
- [x] URL å»é‡æ£€æŸ¥æ–¹æ³•
- [x] URL å»é‡é€»è¾‘é›†æˆ
- [x] URL å»é‡æ•°æ®åº“ç´¢å¼•
- [x] å®Œæ•´HTMLé…ç½® - dataclassé»˜è®¤å€¼
- [x] å®Œæ•´HTMLé…ç½® - from_dictä¿®å¤
- [x] MapScrapeConfig å»é‡é…ç½®
- [x] å»é‡ç»Ÿè®¡æ—¥å¿—è¾“å‡º
- [x] å®é™…è¿è¡ŒéªŒè¯
- [x] æ–‡æ¡£ç”Ÿæˆ

**å®æ–½çŠ¶æ€**: âœ… å…¨éƒ¨å®Œæˆ
**ç³»ç»ŸçŠ¶æ€**: âœ… è¿è¡Œæ­£å¸¸
**éªŒè¯çŠ¶æ€**: âœ… åŠŸèƒ½éªŒè¯é€šè¿‡
