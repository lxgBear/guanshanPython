# URL æ•°é‡é™åˆ¶åŠŸèƒ½å®ç°æ€»ç»“ (v2.1.3)

**å®ç°æ—¥æœŸ**: 2025-11-10
**ç‰ˆæœ¬**: v2.1.3
**åŠŸèƒ½**: Map API è¿”å›é“¾æ¥æ•°é‡é™åˆ¶

---

## 1. åŠŸèƒ½æ¦‚è¿°

### èƒŒæ™¯

åœ¨ v2.1.2 ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å®ç°äº† URL è¿‡æ»¤åŠŸèƒ½ï¼Œèƒ½å¤Ÿè¿‡æ»¤æ‰æ— ç”¨çš„é“¾æ¥ï¼ˆç™»å½•é¡µé¢ã€PDF æ–‡ä»¶ç­‰ï¼‰ã€‚ä½†åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œå³ä½¿ç»è¿‡è¿‡æ»¤ï¼Œå‰©ä½™çš„ URL æ•°é‡ä»ç„¶å¯èƒ½å¾ˆå¤§ï¼Œå¯¼è‡´ï¼š

- **Scrape API ç§¯åˆ†æ¶ˆè€—è¿‡é«˜**ï¼šæ¯ä¸ª URL æ¶ˆè€— 1 ç§¯åˆ†
- **çˆ¬å–æ—¶é—´è¿‡é•¿**ï¼šå¤§é‡ URL éœ€è¦è¾ƒé•¿çš„å¹¶å‘çˆ¬å–æ—¶é—´
- **æ•°æ®å¤„ç†å‹åŠ›å¤§**ï¼šå¤§é‡ç»“æœéœ€è¦å­˜å‚¨å’Œå¤„ç†

### è§£å†³æ–¹æ¡ˆ

åœ¨ Map + Scrape æ¨¡å¼é…ç½®ä¸­æ–°å¢ `max_scrape_urls` å‚æ•°ï¼Œå…è®¸ç”¨æˆ·ç²¾ç¡®æ§åˆ¶æœ€ç»ˆçˆ¬å–çš„ URL æ•°é‡ã€‚è¯¥å‚æ•°åœ¨ URL è¿‡æ»¤ä¹‹åã€å»é‡æ£€æŸ¥ä¹‹å‰ç”Ÿæ•ˆï¼Œç¡®ä¿ï¼š

- åªçˆ¬å–æœ€é‡è¦/æœ€æ–°çš„å†…å®¹
- ç²¾ç¡®æ§åˆ¶ Scrape API ç§¯åˆ†æ¶ˆè€—
- ä¼˜åŒ–çˆ¬å–æ€§èƒ½å’Œæ•°æ®å¤„ç†æ•ˆç‡

### æ ¸å¿ƒä»·å€¼

- **æˆæœ¬æ§åˆ¶**ï¼šç²¾ç¡®æ§åˆ¶ Scrape API ç§¯åˆ†æ¶ˆè€—
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘çˆ¬å–æ—¶é—´å’Œæ•°æ®å¤„ç†å‹åŠ›
- **çµæ´»é…ç½®**ï¼šå¯é€‰å‚æ•°ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
- **ä¸è¿‡æ»¤äº’è¡¥**ï¼šå…ˆè¿‡æ»¤æ— ç”¨é“¾æ¥ï¼Œå†é™åˆ¶æ•°é‡

---

## 2. å®ç°ç»†èŠ‚

### 2.1 é…ç½®å‚æ•°

**æ–‡ä»¶**: `src/services/firecrawl/config/map_scrape_config.py`

#### æ–°å¢å‚æ•°

```python
# v2.1.3: URLæ•°é‡é™åˆ¶é…ç½®
max_scrape_urls: Optional[int] = 500  # æœ€å¤§çˆ¬å–URLæ•°é‡ï¼Œé»˜è®¤500(Noneè¡¨ç¤ºä¸é™åˆ¶)
```

**å‚æ•°è¯´æ˜**:
- **ç±»å‹**: `Optional[int]`
- **é»˜è®¤å€¼**: `500` (é»˜è®¤é™åˆ¶ä¸º 500 ä¸ª URL)
- **å«ä¹‰**:
  - å½“è®¾ç½®ä¸ºæ•´æ•°æ—¶ï¼Œæœ€å¤šçˆ¬å–è¯¥æ•°é‡çš„ URL
  - å½“è®¾ç½®ä¸º `None` æ—¶ï¼Œä¸é™åˆ¶æ•°é‡ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
- **åº”ç”¨æ—¶æœº**: URL è¿‡æ»¤ä¹‹åã€å»é‡æ£€æŸ¥ä¹‹å‰

#### é…ç½®éªŒè¯

```python
def is_valid(self) -> bool:
    """éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ"""
    # ... å…¶ä»–éªŒè¯ ...

    # éªŒè¯ max_scrape_urls (v2.1.3)
    if self.max_scrape_urls is not None and self.max_scrape_urls <= 0:
        return False

    return True
```

**éªŒè¯è§„åˆ™**:
- å¦‚æœè®¾ç½®äº† `max_scrape_urls`ï¼Œå¿…é¡»å¤§äº 0
- `None` å€¼ä¸ºåˆæ³•é…ç½®ï¼ˆè¡¨ç¤ºä¸é™åˆ¶ï¼‰

#### åºåˆ—åŒ–/ååºåˆ—åŒ–

```python
def to_dict(self) -> dict:
    """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
    return {
        # ... å…¶ä»–å­—æ®µ ...
        'max_scrape_urls': self.max_scrape_urls,  # v2.1.3
    }

@classmethod
def from_dict(cls, data: dict) -> 'MapScrapeConfig':
    """ä»å­—å…¸åˆ›å»ºé…ç½®å¯¹è±¡"""
    return cls(
        # ... å…¶ä»–å‚æ•° ...
        max_scrape_urls=data.get('max_scrape_urls'),  # v2.1.3
    )
```

### 2.2 æ‰§è¡Œé€»è¾‘

**æ–‡ä»¶**: `src/services/firecrawl/executors/map_scrape_executor.py`

#### é›†æˆä½ç½®

åœ¨ `execute()` æ–¹æ³•ä¸­çš„æ‰§è¡Œæµç¨‹ï¼š

```
1. Map API å‘ç° URL                    # Step 1
2. URL è¿‡æ»¤ (v2.1.2)                   # Step 3.3
3. URL æ•°é‡é™åˆ¶ (v2.1.3)               # Step 3.4 â† æ–°å¢
4. URL å»é‡æ£€æŸ¥ (v2.1.1)               # Step 3.5
5. æ‰¹é‡ Scrape è·å–å†…å®¹                # Step 4
6. æ—¶é—´è¿‡æ»¤                            # Step 5
7. ä¿å­˜ç»“æœ                            # Step 6-10
```

#### å®ç°ä»£ç 

```python
# 3.4. é™åˆ¶ Scrape URL æ•°é‡ (v2.1.3)
if config.max_scrape_urls and len(discovered_urls) > config.max_scrape_urls:
    self.logger.info(
        f"ğŸ“Š é™åˆ¶ URL æ•°é‡: {len(discovered_urls)} â†’ {config.max_scrape_urls}"
    )
    discovered_urls = discovered_urls[:config.max_scrape_urls]
```

**æ‰§è¡Œé€»è¾‘**:
1. æ£€æŸ¥æ˜¯å¦é…ç½®äº† `max_scrape_urls`
2. å¦‚æœå½“å‰ URL æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæˆªå–å‰ N ä¸ª URL
3. è®°å½•é™åˆ¶æ“ä½œæ—¥å¿—
4. ç»§ç»­æ‰§è¡Œåç»­æµç¨‹

#### æ—¥å¿—è¾“å‡ºç¤ºä¾‹

```
ğŸ“Š é™åˆ¶ URL æ•°é‡: 1523 â†’ 100
```

---

## 3. ä½¿ç”¨ç¤ºä¾‹

### 3.1 API è¯·æ±‚ç¤ºä¾‹

#### åœºæ™¯ 1: é™åˆ¶çˆ¬å– 50 ä¸ª URL

```json
{
  "crawl_url": "https://example.com",
  "crawl_config": {
    "map_limit": 5000,
    "max_scrape_urls": 50,
    "max_concurrent_scrapes": 5,
    "scrape_delay": 0.5
  }
}
```

**æ‰§è¡Œæµç¨‹**:
1. Map API å‘ç° 1000 ä¸ª URL
2. URL è¿‡æ»¤åå‰©ä½™ 800 ä¸ª URL
3. é™åˆ¶ä¸º 50 ä¸ª URL (å–å‰ 50 ä¸ª)
4. å»é‡æ£€æŸ¥
5. Scrape æœ€ç»ˆ URL åˆ—è¡¨

**ç§¯åˆ†æ¶ˆè€—**: 1 (Map) + 50 (Scrape) = 51 ç§¯åˆ†

#### åœºæ™¯ 2: ä½¿ç”¨é»˜è®¤é™åˆ¶ (500 ä¸ª URL)

```json
{
  "crawl_url": "https://example.com",
  "crawl_config": {
    "map_limit": 5000
    // ä¸æŒ‡å®š max_scrape_urlsï¼Œä½¿ç”¨é»˜è®¤å€¼ 500
  }
}
```

**æ‰§è¡Œæµç¨‹**:
1. Map API å‘ç° 1000 ä¸ª URL
2. URL è¿‡æ»¤åå‰©ä½™ 800 ä¸ª URL
3. é™åˆ¶ä¸º 500 ä¸ª URL (å–å‰ 500 ä¸ª)
4. å»é‡æ£€æŸ¥
5. Scrape æœ€ç»ˆ URL åˆ—è¡¨

**ç§¯åˆ†æ¶ˆè€—**: 1 (Map) + 500 (Scrape) = 501 ç§¯åˆ†

#### åœºæ™¯ 3: ä¸é™åˆ¶æ•°é‡ (æ˜¾å¼è®¾ç½® null)

```json
{
  "crawl_url": "https://example.com",
  "crawl_config": {
    "map_limit": 5000,
    "max_scrape_urls": null  // æ˜¾å¼è®¾ç½®ä¸º nullï¼Œä¸é™åˆ¶
  }
}
```

**æ‰§è¡Œæµç¨‹**:
1. Map API å‘ç° 1000 ä¸ª URL
2. URL è¿‡æ»¤åå‰©ä½™ 800 ä¸ª URL
3. ä¸é™åˆ¶æ•°é‡ (ä¿æŒ 800 ä¸ª)
4. å»é‡æ£€æŸ¥
5. Scrape æœ€ç»ˆ URL åˆ—è¡¨

**ç§¯åˆ†æ¶ˆè€—**: 1 (Map) + 800 (Scrape) = 801 ç§¯åˆ†

### 3.2 Python ä»£ç ç¤ºä¾‹

```python
from datetime import datetime
from src.services.firecrawl.config import MapScrapeConfig

# åˆ›å»ºé…ç½®ï¼šé™åˆ¶çˆ¬å– 100 ä¸ª URL
config = MapScrapeConfig(
    map_limit=5000,
    max_scrape_urls=100,  # é™åˆ¶çˆ¬å– 100 ä¸ª URL
    max_concurrent_scrapes=10,
    scrape_delay=0.3,
    start_date=datetime(2025, 1, 1),
    end_date=datetime(2025, 12, 31)
)

# éªŒè¯é…ç½®
assert config.is_valid() == True

# åºåˆ—åŒ–é…ç½®
config_dict = config.to_dict()
print(config_dict['max_scrape_urls'])  # 100

# ååºåˆ—åŒ–é…ç½®
config2 = MapScrapeConfig.from_dict(config_dict)
assert config2.max_scrape_urls == 100
```

---

## 4. ä¸å…¶ä»–åŠŸèƒ½çš„ååŒ

### 4.1 ä¸ URL è¿‡æ»¤çš„ååŒ (v2.1.2)

**æ‰§è¡Œé¡ºåº**: URL è¿‡æ»¤ â†’ URL æ•°é‡é™åˆ¶

```
åŸå§‹ URL (5000)
  â†“ Map API
å‘ç° URL (2000)
  â†“ URL è¿‡æ»¤ (v2.1.2)
è¿‡æ»¤å URL (800)
  â†“ URL æ•°é‡é™åˆ¶ (v2.1.3)
æœ€ç»ˆ URL (100)  â† å¦‚æœé…ç½® max_scrape_urls=100
  â†“ å»é‡æ£€æŸ¥ (v2.1.1)
å¾…çˆ¬å– URL (95)  â† å‡è®¾ 5 ä¸ªå·²å­˜åœ¨
  â†“ Scrape API
çˆ¬å–ç»“æœ (95)
```

**ä¼˜åŠ¿**:
- å…ˆè¿‡æ»¤æ‰æ— ç”¨é“¾æ¥ï¼Œæé«˜è´¨é‡
- å†é™åˆ¶æ•°é‡ï¼Œæ§åˆ¶æˆæœ¬
- åŒé‡ä¼˜åŒ–ï¼Œæ•ˆæœå åŠ 

### 4.2 ä¸ URL å»é‡çš„ååŒ (v2.1.1)

**æ‰§è¡Œé¡ºåº**: URL æ•°é‡é™åˆ¶ â†’ URL å»é‡æ£€æŸ¥

```
è¿‡æ»¤å URL (800)
  â†“ URL æ•°é‡é™åˆ¶ (v2.1.3)
é™åˆ¶å URL (100)
  â†“ å»é‡æ£€æŸ¥ (v2.1.1)
å»é‡å URL (95)   â† 5 ä¸ªå·²å­˜åœ¨ï¼Œè¢«è¿‡æ»¤
  â†“ Scrape API
æœ€ç»ˆçˆ¬å– (95)
```

**ä¼˜åŠ¿**:
- å…ˆé™åˆ¶æ•°é‡ï¼Œå‡å°‘å»é‡æ£€æŸ¥çš„å·¥ä½œé‡
- å»é‡æ£€æŸ¥åªå¤„ç†æœ‰é™çš„ URL é›†åˆ
- é¿å…çˆ¬å–å·²å­˜åœ¨çš„å†…å®¹

### 4.3 ä¸æ—¶é—´è¿‡æ»¤çš„ååŒ

**æ‰§è¡Œé¡ºåº**: URL æ•°é‡é™åˆ¶ â†’ Scrape API â†’ æ—¶é—´è¿‡æ»¤

```
é™åˆ¶å URL (100)
  â†“ Scrape API
çˆ¬å–ç»“æœ (100)
  â†“ æ—¶é—´è¿‡æ»¤
æœ€ç»ˆç»“æœ (80)    â† 20 ä¸ªä¸åœ¨æ—¶é—´èŒƒå›´å†…
```

**æ³¨æ„äº‹é¡¹**:
- æ—¶é—´è¿‡æ»¤åœ¨ Scrape ä¹‹åè¿›è¡Œï¼ˆéœ€è¦è·å– publishedDateï¼‰
- å¦‚æœå¯ç”¨æ—¶é—´è¿‡æ»¤ï¼Œå»ºè®®é€‚å½“å¢åŠ  `max_scrape_urls` å€¼
- ç¤ºä¾‹ï¼šæƒ³è¦ 50 ä¸ªç»“æœ â†’ è®¾ç½® `max_scrape_urls=70`ï¼Œç•™å‡º 20% å†—ä½™

---

## 5. æ€§èƒ½å½±å“

### 5.1 ç§¯åˆ†æ¶ˆè€—ä¼˜åŒ–

| åœºæ™¯ | ä¸é™åˆ¶ | é™åˆ¶ 100 | èŠ‚çœ |
|-----|-------|---------|------|
| Map API | 1 | 1 | 0 |
| Scrape API | 800 | 100 | 700 |
| **æ€»è®¡** | **801** | **101** | **87.4%** |

### 5.2 çˆ¬å–æ—¶é—´ä¼˜åŒ–

å‡è®¾å¹¶å‘æ•° = 5ï¼Œå»¶è¿Ÿ = 0.5s

| åœºæ™¯ | URL æ•°é‡ | é¢„ä¼°æ—¶é—´ | èŠ‚çœ |
|-----|---------|---------|------|
| ä¸é™åˆ¶ | 800 | ~80s | - |
| é™åˆ¶ 100 | 100 | ~10s | 87.5% |
| é™åˆ¶ 50 | 50 | ~5s | 93.8% |

**å…¬å¼**: çˆ¬å–æ—¶é—´ â‰ˆ (URLæ•°é‡ / å¹¶å‘æ•°) Ã— å»¶è¿Ÿ

### 5.3 æ•°æ®å¤„ç†ä¼˜åŒ–

- **æ•°æ®åº“å†™å…¥**: å‡å°‘ 87% çš„æ•°æ®æ’å…¥æ“ä½œ
- **å­˜å‚¨ç©ºé—´**: å‡å°‘ 87% çš„åŸå§‹å“åº”å­˜å‚¨
- **å¤„ç†æ—¶é—´**: å‡å°‘ 87% çš„ç»“æœè½¬æ¢å’Œå¤„ç†æ—¶é—´

---

## 6. æµ‹è¯•éªŒè¯

### 6.1 å•å…ƒæµ‹è¯•

#### é…ç½®éªŒè¯æµ‹è¯•

```python
# æµ‹è¯•æœ‰æ•ˆé…ç½®
config = MapScrapeConfig(max_scrape_urls=100)
assert config.is_valid() == True

# æµ‹è¯• None é…ç½®
config = MapScrapeConfig(max_scrape_urls=None)
assert config.is_valid() == True

# æµ‹è¯•æ— æ•ˆé…ç½® (0 å€¼)
config = MapScrapeConfig(max_scrape_urls=0)
assert config.is_valid() == False

# æµ‹è¯•æ— æ•ˆé…ç½® (è´Ÿå€¼)
config = MapScrapeConfig(max_scrape_urls=-10)
assert config.is_valid() == False
```

#### åºåˆ—åŒ–æµ‹è¯•

```python
# æµ‹è¯•åºåˆ—åŒ–
config = MapScrapeConfig(max_scrape_urls=50)
config_dict = config.to_dict()
assert config_dict['max_scrape_urls'] == 50

# æµ‹è¯•ååºåˆ—åŒ–
config2 = MapScrapeConfig.from_dict(config_dict)
assert config2.max_scrape_urls == 50

# æµ‹è¯• None å€¼åºåˆ—åŒ–
config3 = MapScrapeConfig(max_scrape_urls=None)
config_dict3 = config3.to_dict()
assert config_dict3['max_scrape_urls'] is None
```

### 6.2 é›†æˆæµ‹è¯• (å»ºè®®)

```python
import asyncio
from src.core.domain.entities.search_task import SearchTask
from src.services.firecrawl.executors import MapScrapeExecutor

async def test_url_limiting():
    """æµ‹è¯• URL æ•°é‡é™åˆ¶åŠŸèƒ½"""

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = SearchTask(
        crawl_url="https://example.com",
        crawl_config={
            "max_scrape_urls": 10  # é™åˆ¶ä¸º 10 ä¸ª
        }
    )

    # æ‰§è¡Œä»»åŠ¡
    executor = MapScrapeExecutor()
    batch = await executor.execute(task)

    # éªŒè¯ç»“æœ
    assert len(batch.results) <= 10, "ç»“æœæ•°é‡åº”è¯¥ä¸è¶…è¿‡ 10"
    assert batch.credits_used <= 11, "ç§¯åˆ†æ¶ˆè€—åº”è¯¥ä¸è¶…è¿‡ 11 (1 Map + 10 Scrape)"

    print("âœ… URL é™åˆ¶åŠŸèƒ½æµ‹è¯•é€šè¿‡")

# è¿è¡Œæµ‹è¯•
asyncio.run(test_url_limiting())
```

---

## 7. æœ€ä½³å®è·µ

### 7.1 å‚æ•°è®¾ç½®å»ºè®®

#### åœºæ™¯ 1: å¿«é€Ÿæ¢ç´¢ç½‘ç«™ç»“æ„

```python
config = MapScrapeConfig(
    map_limit=5000,
    max_scrape_urls=20,  # åªçˆ¬å– 20 ä¸ªé¡µé¢å¿«é€Ÿæ¢ç´¢
    max_concurrent_scrapes=10
)
```

**é€‚ç”¨åœºæ™¯**:
- åˆæ¬¡æ¢ç´¢æœªçŸ¥ç½‘ç«™
- å¿«é€ŸéªŒè¯ç½‘ç«™ç»“æ„
- èŠ‚çœç§¯åˆ†æˆæœ¬

#### åœºæ™¯ 2: æ—¥å¸¸å®šæœŸçˆ¬å–

```python
config = MapScrapeConfig(
    map_limit=5000,
    max_scrape_urls=100,  # æ¯æ¬¡çˆ¬å– 100 ä¸ªæœ€æ–°å†…å®¹
    enable_dedup=True,     # å¯ç”¨å»é‡ï¼Œé¿å…é‡å¤çˆ¬å–
    start_date=datetime.now() - timedelta(days=1)  # åªçˆ¬å–æ˜¨å¤©ä¹‹åçš„
)
```

**é€‚ç”¨åœºæ™¯**:
- å®šæœŸåŒæ­¥ç½‘ç«™å†…å®¹
- è·å–æœ€æ–°æ›´æ–°
- é¿å…é‡å¤çˆ¬å–

#### åœºæ™¯ 3: å…¨é‡æ•°æ®é‡‡é›†

```python
config = MapScrapeConfig(
    map_limit=5000,
    max_scrape_urls=None,  # ä¸é™åˆ¶ï¼Œçˆ¬å–æ‰€æœ‰è¿‡æ»¤åçš„ URL
    allow_partial_failure=True,
    min_success_rate=0.7
)
```

**é€‚ç”¨åœºæ™¯**:
- é¦–æ¬¡å…¨é‡é‡‡é›†
- æ•°æ®å½’æ¡£éœ€æ±‚
- ä¸è€ƒè™‘æˆæœ¬é™åˆ¶

### 7.2 ä¸æ—¶é—´è¿‡æ»¤é…åˆ

**é—®é¢˜**: æ—¶é—´è¿‡æ»¤åœ¨ Scrape ä¹‹åè¿›è¡Œï¼Œå¯èƒ½å¯¼è‡´æœ€ç»ˆç»“æœå°‘äºé¢„æœŸ

**è§£å†³æ–¹æ¡ˆ**: é¢„ç•™å†—ä½™

```python
# ç›®æ ‡ï¼šè·å– 50 ä¸ªç¬¦åˆæ—¶é—´èŒƒå›´çš„ç»“æœ
# é¢„ä¼° 70% çš„é¡µé¢ä¼šé€šè¿‡æ—¶é—´è¿‡æ»¤

config = MapScrapeConfig(
    max_scrape_urls=70,  # é¢„ç•™ 40% å†—ä½™ (50 / 0.7 â‰ˆ 70)
    start_date=datetime(2025, 10, 1),
    end_date=datetime(2025, 11, 10)
)
```

### 7.3 æˆæœ¬æ§åˆ¶ç­–ç•¥

#### ç­–ç•¥ 1: å›ºå®šé¢„ç®—

```python
# é¢„ç®—ï¼š100 ç§¯åˆ†
# Map: 1 ç§¯åˆ†, Scrape: 99 ç§¯åˆ†

config = MapScrapeConfig(
    max_scrape_urls=99  # ä¸¥æ ¼æ§åˆ¶åœ¨é¢„ç®—å†…
)
```

#### ç­–ç•¥ 2: åŠ¨æ€è°ƒæ•´

```python
# æ ¹æ® Map å‘ç°çš„ URL æ•°é‡åŠ¨æ€è°ƒæ•´

discovered_count = 2000
if discovered_count > 1000:
    max_urls = 50  # å¤§é‡ URL æ—¶é™åˆ¶æ›´ä¸¥æ ¼
elif discovered_count > 500:
    max_urls = 100
else:
    max_urls = None  # å°‘é‡ URL æ—¶ä¸é™åˆ¶
```

---

## 8. ä»£ç å˜æ›´æ€»ç»“

### 8.1 å˜æ›´æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | å˜æ›´ç±»å‹ | è¡Œæ•° | è¯´æ˜ |
|-----|---------|-----|------|
| `src/services/firecrawl/config/map_scrape_config.py` | ä¿®æ”¹ | +9 | æ–°å¢å‚æ•°ã€éªŒè¯ã€åºåˆ—åŒ– |
| `src/services/firecrawl/executors/map_scrape_executor.py` | ä¿®æ”¹ | +6 | æ–°å¢ URL é™åˆ¶é€»è¾‘ |

### 8.2 è¯¦ç»†å˜æ›´

#### `map_scrape_config.py`

```diff
@@ Docstring @@
+        max_scrape_urls: æœ€å¤§çˆ¬å–URLæ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶ï¼ˆv2.1.3ï¼‰

@@ å‚æ•°å®šä¹‰ @@
+    # v2.1.3: URLæ•°é‡é™åˆ¶é…ç½®
+    max_scrape_urls: Optional[int] = None  # æœ€å¤§çˆ¬å–URLæ•°é‡(Noneè¡¨ç¤ºä¸é™åˆ¶)

@@ éªŒè¯é€»è¾‘ @@
+        # éªŒè¯ max_scrape_urls (v2.1.3)
+        if self.max_scrape_urls is not None and self.max_scrape_urls <= 0:
+            return False

@@ åºåˆ—åŒ– @@
+            'max_scrape_urls': self.max_scrape_urls,  # v2.1.3

@@ ååºåˆ—åŒ– @@
+            max_scrape_urls=data.get('max_scrape_urls'),  # v2.1.3
```

#### `map_scrape_executor.py`

```diff
@@ æ‰§è¡Œæµç¨‹ @@
+            # 3.4. é™åˆ¶ Scrape URL æ•°é‡ (v2.1.3)
+            if config.max_scrape_urls and len(discovered_urls) > config.max_scrape_urls:
+                self.logger.info(
+                    f"ğŸ“Š é™åˆ¶ URL æ•°é‡: {len(discovered_urls)} â†’ {config.max_scrape_urls}"
+                )
+                discovered_urls = discovered_urls[:config.max_scrape_urls]
```

---

## 9. å‘åå…¼å®¹æ€§

### 9.1 å…¼å®¹æ€§ä¿è¯

- **é»˜è®¤è¡Œä¸ºå˜æ›´**: `max_scrape_urls=500` ä¸ºé»˜è®¤å€¼ï¼Œé»˜è®¤é™åˆ¶ä¸º 500 ä¸ª URL
- **æ˜¾å¼ä¸é™åˆ¶**: éœ€è¦æ˜¾å¼è®¾ç½® `max_scrape_urls=null` æ‰èƒ½ä¸é™åˆ¶
- **æˆæœ¬æ§åˆ¶ä¼˜åŒ–**: é»˜è®¤é™åˆ¶æœ‰åŠ©äºé¿å…æ„å¤–çš„é«˜é¢ç§¯åˆ†æ¶ˆè€—
- **é…ç½®çµæ´»æ€§**: å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªç”±è°ƒæ•´æˆ–å–æ¶ˆé™åˆ¶

### 9.2 ç‰ˆæœ¬å‡çº§

ä» v2.1.2 å‡çº§åˆ° v2.1.3ï¼š

```python
# v2.1.2 é…ç½®ï¼ˆå‡çº§åä¼šåº”ç”¨é»˜è®¤é™åˆ¶ 500ï¼‰
config_old = {
    "map_limit": 5000,
    "max_concurrent_scrapes": 5
}
# å‡çº§åˆ° v2.1.3 åï¼Œè¡Œä¸ºå˜åŒ–ï¼š
# - ä»¥å‰ï¼šä¸é™åˆ¶ URL æ•°é‡
# - ç°åœ¨ï¼šé»˜è®¤é™åˆ¶ä¸º 500 ä¸ª URL

# v2.1.3 é…ç½®é€‰é¡¹ 1ï¼šè‡ªå®šä¹‰é™åˆ¶
config_custom = {
    "map_limit": 5000,
    "max_concurrent_scrapes": 5,
    "max_scrape_urls": 100  # è‡ªå®šä¹‰é™åˆ¶ä¸º 100
}

# v2.1.3 é…ç½®é€‰é¡¹ 2ï¼šæ˜¾å¼ä¸é™åˆ¶
config_unlimited = {
    "map_limit": 5000,
    "max_concurrent_scrapes": 5,
    "max_scrape_urls": null  # æ˜¾å¼è®¾ç½®ä¸º nullï¼Œä¸é™åˆ¶
}
```

**æ³¨æ„äº‹é¡¹**:
- å‡çº§åé»˜è®¤ä¼šé™åˆ¶ä¸º 500 ä¸ª URLï¼Œå¯èƒ½å½±å“ç°æœ‰ä»»åŠ¡çš„çˆ¬å–èŒƒå›´
- å¦‚éœ€ä¿æŒä¸é™åˆ¶è¡Œä¸ºï¼Œéœ€è¦æ˜¾å¼è®¾ç½® `max_scrape_urls: null`
- å»ºè®®æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´è¯¥å‚æ•°

---

## 10. æ€»ç»“

### 10.1 åŠŸèƒ½äº®ç‚¹

âœ… **ç²¾ç¡®æ§åˆ¶**: å¯ç²¾ç¡®æ§åˆ¶æœ€ç»ˆçˆ¬å–çš„ URL æ•°é‡
âœ… **æˆæœ¬ä¼˜åŒ–**: é»˜è®¤é™åˆ¶ 500 ä¸ª URLï¼Œé¿å…æ„å¤–é«˜é¢ç§¯åˆ†æ¶ˆè€—
âœ… **æ€§èƒ½æå‡**: æ˜¾è‘—å‡å°‘çˆ¬å–æ—¶é—´å’Œæ•°æ®å¤„ç†å‹åŠ›
âœ… **çµæ´»é…ç½®**: å¯è‡ªç”±è°ƒæ•´é™åˆ¶æ•°é‡æˆ–å–æ¶ˆé™åˆ¶
âœ… **å®Œç¾ååŒ**: ä¸ URL è¿‡æ»¤ã€å»é‡ã€æ—¶é—´è¿‡æ»¤å®Œç¾é…åˆ
âœ… **æˆæœ¬ä¿æŠ¤**: é»˜è®¤å¯ç”¨ï¼Œé˜²æ­¢æ„å¤–çš„å¤§é‡çˆ¬å–

### 10.2 æŠ€æœ¯ä¼˜åŠ¿

- **SOLID åŸåˆ™**: éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼ŒåŠŸèƒ½ç‹¬ç«‹
- **ä»£ç ç®€æ´**: ä»…æ–°å¢ 15 è¡Œæ ¸å¿ƒä»£ç 
- **æ— å‰¯ä½œç”¨**: ä¸å½±å“å…¶ä»–æ¨¡å—å’Œç°æœ‰åŠŸèƒ½
- **æ˜“äºç»´æŠ¤**: æ¸…æ™°çš„ä»£ç ç»“æ„å’Œå®Œå–„çš„æ–‡æ¡£

### 10.3 é€‚ç”¨åœºæ™¯

- å¿«é€Ÿæ¢ç´¢ç½‘ç«™ç»“æ„ï¼ˆé™åˆ¶ 10-20 ä¸ª URLï¼‰
- æ—¥å¸¸å®šæœŸçˆ¬å–ï¼ˆé™åˆ¶ 50-100 ä¸ª URLï¼‰
- æˆæœ¬æ§åˆ¶åœºæ™¯ï¼ˆä¸¥æ ¼é¢„ç®—é™åˆ¶ï¼‰
- æ€§èƒ½ä¼˜åŒ–åœºæ™¯ï¼ˆå‡å°‘å¤„ç†æ—¶é—´ï¼‰

---

## é™„å½•

### A. å®Œæ•´æ‰§è¡Œæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Map + Scrape æ‰§è¡Œæµç¨‹                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  1. Map API      â”‚
                    â”‚  å‘ç° URL         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 5000 ä¸ª URL
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  2. URL è¿‡æ»¤     â”‚  â† v2.1.2
                    â”‚  (è¿‡æ»¤æ— ç”¨é“¾æ¥)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 800 ä¸ª URL
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  3. URL æ•°é‡é™åˆ¶ â”‚  â† v2.1.3 (æ–°å¢)
                    â”‚  (max_scrape_    â”‚
                    â”‚   urls=100)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 100 ä¸ª URL
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  4. URL å»é‡     â”‚  â† v2.1.1
                    â”‚  (è¿‡æ»¤å·²çˆ¬å–)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 95 ä¸ª URL
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  5. Scrape API   â”‚
                    â”‚  æ‰¹é‡è·å–å†…å®¹     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 95 ä¸ªç»“æœ
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  6. æ—¶é—´è¿‡æ»¤     â”‚
                    â”‚  (publishedDate) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ 80 ä¸ªç»“æœ
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  7. ä¿å­˜ç»“æœ     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### B. å‚æ•°ä¼˜å…ˆçº§

```
ç”¨æˆ·é…ç½® > é»˜è®¤å€¼

max_scrape_urls å‚æ•°ï¼š
â”œâ”€ ç”¨æˆ·æ˜¾å¼è®¾ç½®ä¸ºæ•´æ•° â†’ ä½¿ç”¨è¯¥å€¼é™åˆ¶
â”œâ”€ ç”¨æˆ·æ˜¾å¼è®¾ç½®ä¸º None â†’ ä¸é™åˆ¶
â””â”€ ç”¨æˆ·æœªè®¾ç½® â†’ é»˜è®¤ None (ä¸é™åˆ¶)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-10
**ç»´æŠ¤è€…**: Architecture Team
