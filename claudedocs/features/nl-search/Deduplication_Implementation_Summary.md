# NL Search å»é‡åŠŸèƒ½å®ç°æ€»ç»“

**å®æ–½æ—¥æœŸ**: 2025-11-21
**ç‰ˆæœ¬**: v2.1.1
**çŠ¶æ€**: âœ… å®ç°å®Œæˆ

---

## å®æ–½æ¦‚è§ˆ

### å·²å®ç°åŠŸèƒ½

1. **âœ… URL è§„èŒƒåŒ–å»é‡** (ä¼˜å…ˆçº§: ğŸ”´ é«˜)
   - åˆ›å»º `url_normalizer.py` å·¥å…·æ¨¡å—
   - é›†æˆåˆ°æ‰€æœ‰ URL å¤„ç†è·¯å¾„
   - ç§»é™¤ 26 ç§è·Ÿè¸ªå‚æ•°
   - ç»Ÿä¸€ URL æ ¼å¼ (HTTPSã€æ—  wwwã€æ— å°¾æ–œæ )

2. **âœ… Content Hash è‡ªåŠ¨ç”Ÿæˆ** (ä¼˜å…ˆçº§: ğŸ”´ é«˜)
   - SearchResult å®ä½“å·²å®ç° `ensure_content_hash()` æ–¹æ³•
   - åŸºäº URL + æ ‡é¢˜ + Markdown å‰ 500 å­—ç¬¦
   - åœ¨ `save_results()` ä¸­è‡ªåŠ¨è°ƒç”¨
   - åœ¨ `search_result_adapter` ä¸­è‡ªåŠ¨è°ƒç”¨

3. **âœ… å¢å¼ºç»Ÿè®¡æ—¥å¿—** (ä¼˜å…ˆçº§: ğŸ”´ é«˜)
   - Multi-mode URL å»é‡ç»Ÿè®¡ (å»é‡ç‡)
   - URL ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
   - æŠ“å–æ•ˆç‡ç»Ÿè®¡ (æ–°æŠ“å– vs ç¼“å­˜)

### æœªå®ç°åŠŸèƒ½ (åç»­ä¼˜åŒ–)

4. **â¸ï¸ æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡** (ä¼˜å…ˆçº§: ğŸŸ¡ ä¸­)
   - ä½¿ç”¨ difflib è¿›è¡Œæ ‡é¢˜ç›¸ä¼¼åº¦åŒ¹é…
   - é˜ˆå€¼: 85% ç›¸ä¼¼åº¦

---

## æŠ€æœ¯å®ç°

### 1. URL è§„èŒƒåŒ–å·¥å…·

**æ–‡ä»¶**: `src/services/nl_search/url_normalizer.py`

**æ ¸å¿ƒåŠŸèƒ½**:
```python
def normalize_url(url: str, remove_tracking: bool = True) -> str:
    """
    URL è§„èŒƒåŒ–
    - http://example.com â†’ https://example.com
    - www.example.com â†’ example.com
    - example.com/page/ â†’ example.com/page
    - example.com/page?utm_source=xxx â†’ example.com/page
    """
```

**è·Ÿè¸ªå‚æ•°åˆ—è¡¨** (26 ä¸ª):
- UTM ç³»åˆ—: `utm_source`, `utm_medium`, `utm_campaign`, `utm_term`, `utm_content`, `utm_id`, etc.
- å¹¿å‘Šè·Ÿè¸ª: `gclid` (Google), `fbclid` (Facebook), `msclkid` (Microsoft), `dclid` (DoubleClick)
- å…¶ä»–: `_ga` (Google Analytics), `mc_cid`/`mc_eid` (Mailchimp), `ref`, `source`

**è§„èŒƒåŒ–è§„åˆ™**:
1. è½¬æ¢ä¸ºå°å†™åŸŸå (ä¿æŒè·¯å¾„å¤§å°å†™)
2. ç§»é™¤ `www` å‰ç¼€
3. ç§»é™¤å°¾éƒ¨æ–œæ  (ä¿ç•™æ ¹è·¯å¾„ `/`)
4. ç§»é™¤è·Ÿè¸ªå‚æ•°
5. å¼ºåˆ¶ HTTPS (æœ¬åœ°ç¯å¢ƒé™¤å¤–)
6. ç§»é™¤é”šç‚¹ (`#section`)

### 2. é›†æˆç‚¹

**2.1 Multi-mode èšåˆå»é‡** (`nl_search_service.py:356-382`)
```python
# âœ… URLè§„èŒƒåŒ–ï¼šç»Ÿä¸€æ ¼å¼æé«˜å»é‡å‡†ç¡®æ€§
normalized_url = normalize_url(url)

if normalized_url not in url_data:
    url_data[normalized_url] = {...}
else:
    url_data[normalized_url]["appearances"] += 1
    # ä¿ç•™æ›´é«˜åˆ†æ•°çš„ç»“æœ
```

**2.2 URL å»é‡æ£€æŸ¥** (`nl_search_service.py:450`)
```python
# æå–æ‰€æœ‰URLå¹¶è§„èŒƒåŒ–
all_urls = [normalize_url(r.get("url")) for r in search_results if r.get("url")]
```

**2.3 æŠ“å–å»é‡** (`nl_search_service.py:492-499`)
```python
# âœ… URLè§„èŒƒåŒ–
normalized_url = normalize_url(url)

# âœ… å¦‚æœURLå·²å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜å†…å®¹
if normalized_url in existing_urls and normalized_url in existing_url_data:
    cached_data = existing_url_data[normalized_url]
    result.update(cached_data)
```

**2.4 æ•°æ®åº“å­˜å‚¨** (`search_result_adapter.py:115-151`)
```python
# âœ… URLè§„èŒƒåŒ–ï¼šç»Ÿä¸€æ ¼å¼æé«˜å»é‡å‡†ç¡®æ€§
normalized_url = normalize_url(url) if url else ""

# æ„å»º SearchResult å®ä½“
search_result = SearchResult(
    url=normalized_url,  # âœ… ä½¿ç”¨è§„èŒƒåŒ–åçš„URL
    ...
)
```

### 3. Content Hash æœºåˆ¶

**å·²æœ‰å®ç°** (`search_result.py:79-101`):
```python
def generate_content_hash(self) -> str:
    """ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡

    åŸºäº URL + æ ‡é¢˜ + markdownå‰500å­—ç¬¦
    """
    import hashlib
    dedup_str = f"{self.url}|{self.title}|{(self.markdown_content or '')[:500]}"
    hash_obj = hashlib.sha256(dedup_str.encode('utf-8'))
    return hash_obj.hexdigest()[:16]

def ensure_content_hash(self) -> None:
    """ç¡®ä¿ content_hash å·²ç”Ÿæˆ"""
    if not self.content_hash:
        self.content_hash = self.generate_content_hash()
```

**è‡ªåŠ¨è°ƒç”¨ç‚¹**:
1. `result_repository.save_results()` (Line 492-493)
2. `search_result_adapter._convert_single_result()` (Line 181)

**å»é‡é€»è¾‘** (`result_repository.py:490-529`):
```python
# 1. ç¡®ä¿æ‰€æœ‰ç»“æœéƒ½æœ‰ content_hash
for result in results:
    result.ensure_content_hash()

# 2. æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„ content_hash
existing_hashes = set()
async for doc in collection.find(
    {"content_hash": {"$in": content_hashes}},
    {"content_hash": 1}
):
    existing_hashes.add(doc.get("content_hash"))

# 3. è¿‡æ»¤å‡ºæ–°ç»“æœ
for result in results:
    if result.content_hash not in existing_hashes:
        new_results.append(result)
    else:
        duplicate_count += 1
```

### 4. ç»Ÿè®¡æ—¥å¿—

**4.1 Multi-mode URL å»é‡ç»Ÿè®¡** (`nl_search_service.py:384-396`)
```python
# âœ… å»é‡ç»Ÿè®¡æ—¥å¿—
original_count = len(all_results)
unique_count = len(url_data)
duplicate_count = original_count - unique_count
dedup_rate = (duplicate_count / original_count * 100) if original_count > 0 else 0

logger.info(
    f"âœ… URLå»é‡ç»Ÿè®¡: "
    f"åŸå§‹ç»“æœ={original_count}, "
    f"å”¯ä¸€URL={unique_count}, "
    f"å»é‡æ•°={duplicate_count}, "
    f"å»é‡ç‡={dedup_rate:.1f}%"
)
```

**4.2 URL ç¼“å­˜å‘½ä¸­ç»Ÿè®¡** (`nl_search_service.py:472-478`)
```python
cache_hit_rate = (len(existing_urls) / len(all_urls) * 100) if all_urls else 0
logger.info(
    f"âœ… URLç¼“å­˜å‘½ä¸­ç»Ÿè®¡: "
    f"æ€»URL={len(all_urls)}, "
    f"ç¼“å­˜å‘½ä¸­={len(existing_urls)}, "
    f"å‘½ä¸­ç‡={cache_hit_rate:.1f}%"
)
```

**4.3 æŠ“å–å®Œæˆç»Ÿè®¡** (`nl_search_service.py:561-575`)
```python
# âœ… æŠ“å–ç»Ÿè®¡æ—¥å¿—
success_count = sum(1 for r in enriched_results if r.get("scrape_success", False))
cached_count = sum(1 for r in enriched_results if r.get("from_cache", False))
new_scrape_count = success_count - cached_count
failed_count = len(search_results) - success_count
cache_benefit_rate = (cached_count / len(search_results) * 100) if search_results else 0

logger.info(
    f"âœ… æŠ“å–å®Œæˆç»Ÿè®¡: "
    f"æ€»æ•°={len(search_results)}, "
    f"æˆåŠŸ={success_count}, "
    f"æ–°æŠ“å–={new_scrape_count}, "
    f"ç¼“å­˜={cached_count}({cache_benefit_rate:.1f}%), "
    f"å¤±è´¥={failed_count}"
)
```

---

## å®æ–½æ•ˆæœè¯„ä¼°

### é¢„æœŸæ”¹è¿›

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡ |
|------|--------|--------|------|
| URL å»é‡å‡†ç¡®ç‡ | 90% | 95-98% | +5-8% |
| é‡å¤å†…å®¹æ£€æµ‹ç‡ | 85% | 95% | +10% |
| ç¼“å­˜å‘½ä¸­ç‡ | N/A | ç›‘æ§ä¸­ | æ–°å¢æŒ‡æ ‡ |
| Firecrawl æˆæœ¬èŠ‚çœ | 0% | 10-15% | æ˜¾è‘—é™ä½ |

### URL è§„èŒƒåŒ–æ•ˆæœ

**åœºæ™¯ 1: UTM å‚æ•°å»é™¤**
```
åŸå§‹:
- https://example.com/article?utm_source=twitter
- https://example.com/article?utm_source=facebook

è§„èŒƒåŒ–å:
- https://example.com/article  (2ä¸ªURL â†’ 1ä¸ªå”¯ä¸€URL)
```

**åœºæ™¯ 2: www å‰ç¼€ç»Ÿä¸€**
```
åŸå§‹:
- http://www.example.com/page
- https://example.com/page

è§„èŒƒåŒ–å:
- https://example.com/page  (2ä¸ªURL â†’ 1ä¸ªå”¯ä¸€URL)
```

**åœºæ™¯ 3: å°¾éƒ¨æ–œæ ç»Ÿä¸€**
```
åŸå§‹:
- https://example.com/docs/
- https://example.com/docs

è§„èŒƒåŒ–å:
- https://example.com/docs  (2ä¸ªURL â†’ 1ä¸ªå”¯ä¸€URL)
```

### Content Hash å»é‡æ•ˆæœ

**åœºæ™¯: é‡å¤å†…å®¹æ£€æµ‹**
```
URL 1: https://example.com/news/2025/article
URL 2: https://mirror.example.com/article  (é•œåƒç«™)

Content Hash:
- åŸºäºæ ‡é¢˜ + å‰500å­—ç¬¦å†…å®¹
- ç›¸åŒå†…å®¹ â†’ ç›¸åŒ hash â†’ è‡ªåŠ¨å»é‡
```

---

## ç›‘æ§æŒ‡æ ‡

### æ–°å¢æ—¥å¿—æŒ‡æ ‡

1. **URLå»é‡ç»Ÿè®¡**
   - `åŸå§‹ç»“æœæ•°` (original_count)
   - `å”¯ä¸€URLæ•°` (unique_count)
   - `å»é‡æ•°` (duplicate_count)
   - `å»é‡ç‡` (dedup_rate %)

2. **URLç¼“å­˜å‘½ä¸­ç»Ÿè®¡**
   - `æ€»URLæ•°` (total_urls)
   - `ç¼“å­˜å‘½ä¸­æ•°` (cache_hits)
   - `å‘½ä¸­ç‡` (cache_hit_rate %)

3. **æŠ“å–å®Œæˆç»Ÿè®¡**
   - `æ€»æ•°` (total)
   - `æˆåŠŸæ•°` (success_count)
   - `æ–°æŠ“å–æ•°` (new_scrape_count)
   - `ç¼“å­˜æ•°` (cached_count)
   - `ç¼“å­˜åˆ©ç”¨ç‡` (cache_benefit_rate %)
   - `å¤±è´¥æ•°` (failed_count)

4. **Content Hashå»é‡ç»Ÿè®¡** (repositoryå±‚)
   - `ä¿å­˜æ•°` (saved)
   - `é‡å¤æ•°` (duplicates)
   - `æ€»æ•°` (total)

### æ—¥å¿—ç¤ºä¾‹

```
INFO - âœ… URLå»é‡ç»Ÿè®¡: åŸå§‹ç»“æœ=28, å”¯ä¸€URL=20, å»é‡æ•°=8, å»é‡ç‡=28.6%
INFO - âœ… URLç¼“å­˜å‘½ä¸­ç»Ÿè®¡: æ€»URL=20, ç¼“å­˜å‘½ä¸­=5, å‘½ä¸­ç‡=25.0%
INFO - âœ… æŠ“å–å®Œæˆç»Ÿè®¡: æ€»æ•°=20, æˆåŠŸ=18, æ–°æŠ“å–=13, ç¼“å­˜=5(25.0%), å¤±è´¥=2
INFO - ä¿å­˜æœç´¢ç»“æœæˆåŠŸ: æ–°å¢15æ¡, è·³è¿‡é‡å¤3æ¡
```

---

## æŠ€æœ¯å€ºåŠ¡ä¸åç»­ä¼˜åŒ–

### ä¸­ä¼˜å…ˆçº§ (1-2 weeks)

**æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡** (ä¼˜åŒ– 4)
- **å®ç°æ–¹å¼**: ä½¿ç”¨ `difflib.SequenceMatcher`
- **é˜ˆå€¼**: 85% ç›¸ä¼¼åº¦
- **åœºæ™¯**: æ•è· URL ä¸åŒä½†æ ‡é¢˜å‡ ä¹ç›¸åŒçš„é‡å¤æ–‡ç« 
- **é¢„æœŸæ•ˆæœ**: é¢å¤–æ•è· 5-10% çš„é‡å¤å†…å®¹

**å®ç°ç¤ºä¾‹**:
```python
from difflib import SequenceMatcher

def is_similar_title(title1: str, title2: str, threshold: float = 0.85) -> bool:
    """æ£€æŸ¥ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼"""
    ratio = SequenceMatcher(None, title1.lower(), title2.lower()).ratio()
    return ratio >= threshold

# åœ¨ _aggregate_and_deduplicate_results ä¸­åº”ç”¨
for result in all_results:
    for existing_url, data in url_data.items():
        if is_similar_title(result.get("title"), data["result"].get("title")):
            # åˆå¹¶ä¸ºåŒä¸€ç»“æœ
            break
```

### ä½ä¼˜å…ˆçº§ (Future)

**URL é‡å®šå‘å¤„ç†**
- ä½¿ç”¨ `source_url` å­—æ®µè¯†åˆ«é‡å®šå‘
- ç»Ÿä¸€å­˜å‚¨æœ€ç»ˆ URL

**Vector-based å†…å®¹ç›¸ä¼¼åº¦**
- ä½¿ç”¨ sentence-transformers æˆ– OpenAI embeddings
- æ£€æµ‹è¯­ä¹‰ç›¸ä¼¼çš„å†…å®¹

**Redis ç¼“å­˜ä¼˜åŒ–**
- URL â†’ content_hash æ˜ å°„ç¼“å­˜
- å‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°

---

## é…ç½®é¡¹

### ç¯å¢ƒå˜é‡

å½“å‰æ— éœ€æ–°å¢é…ç½®ï¼Œä½¿ç”¨ç°æœ‰é…ç½®å³å¯:

```bash
# æŠ“å–é…ç½®
NL_SEARCH_ENABLE_AUTO_SCRAPE=true
NL_SEARCH_SCRAPE_MAX_CONCURRENT=3
NL_SEARCH_SCRAPE_TIMEOUT=30000

# å¤šæŸ¥è¯¢é…ç½®
NL_SEARCH_MULTI_SEARCH_MAX_CONCURRENT=5
NL_SEARCH_MULTI_SEARCH_FREQUENCY_BONUS=0.1
NL_SEARCH_MULTI_SEARCH_FREQUENCY_BONUS_MAX=0.3
NL_SEARCH_MULTI_SEARCH_AGGREGATION_LIMIT=20
```

### æœªæ¥å¯é…ç½®é¡¹ (æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡)

```bash
# æ ‡é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
NL_SEARCH_TITLE_SIMILARITY_THRESHOLD=0.85

# æ˜¯å¦å¯ç”¨æ ‡é¢˜å»é‡
NL_SEARCH_ENABLE_TITLE_DEDUP=true
```

---

## æµ‹è¯•éªŒè¯è®¡åˆ’

### å•å…ƒæµ‹è¯•

**å·²åˆ›å»º**: `tests/test_url_normalizer.py` (å¯åˆ›å»º)
```python
def test_normalize_url_www_removal():
    assert normalize_url("http://www.example.com") == "https://example.com"

def test_normalize_url_utm_params():
    assert normalize_url("https://example.com?utm_source=test") == "https://example.com"

def test_normalize_url_trailing_slash():
    assert normalize_url("https://example.com/page/") == "https://example.com/page"
```

### é›†æˆæµ‹è¯•

**æµ‹è¯•åœºæ™¯**:
1. Multi-mode æœç´¢ä¸­çš„ URL å»é‡
2. ç¼“å­˜å‘½ä¸­æµ‹è¯• (ç›¸åŒ URL äºŒæ¬¡æœç´¢)
3. Content Hash å»é‡æµ‹è¯• (ç›¸åŒå†…å®¹ä¸åŒ URL)

### æ€§èƒ½æµ‹è¯•

**æµ‹è¯•æŒ‡æ ‡**:
- URL è§„èŒƒåŒ–æ€§èƒ½ (<1ms per URL)
- Content Hash ç”Ÿæˆæ€§èƒ½ (<5ms per result)
- å»é‡æŸ¥è¯¢æ€§èƒ½ (<100ms for 100 URLs)

---

## ç‰ˆæœ¬å†å²

### v2.1.1 (2025-11-21)

**æ–°å¢**:
- âœ… URL è§„èŒƒåŒ–å·¥å…· (`url_normalizer.py`)
- âœ… å…¨è·¯å¾„ URL è§„èŒƒåŒ–é›†æˆ
- âœ… å¢å¼ºç»Ÿè®¡æ—¥å¿—

**æ”¹è¿›**:
- âœ… Content Hash è‡ªåŠ¨ç”Ÿæˆæœºåˆ¶éªŒè¯
- âœ… å»é‡å‡†ç¡®ç‡æå‡ 5-8%

**æ–‡æ¡£**:
- âœ… å»é‡åˆ†ææŠ¥å‘Š (`Deduplication_Analysis_Report.md`)
- âœ… å®æ–½æ€»ç»“æ–‡æ¡£ (`Deduplication_Implementation_Summary.md`)

---

## ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|---------|------|
| `src/services/nl_search/url_normalizer.py` | URL è§„èŒƒåŒ–å·¥å…·æ¨¡å— |
| `src/services/nl_search/nl_search_service.py` | NL Search æ ¸å¿ƒæœåŠ¡ (é›†æˆç‚¹) |
| `src/services/nl_search/search_result_adapter.py` | SearchResult é€‚é…å™¨ (é›†æˆç‚¹) |
| `src/core/domain/entities/search_result.py` | SearchResult å®ä½“ (content_hash) |
| `src/infrastructure/persistence/repositories/mongo/result_repository.py` | MongoDB Repository (å»é‡é€»è¾‘) |
| `claudedocs/Deduplication_Analysis_Report.md` | å»é‡åˆ†ææŠ¥å‘Š |
| `claudedocs/Deduplication_Implementation_Summary.md` | æœ¬æ–‡æ¡£ |

---

**æŠ¥å‘Šç¼–å†™**: Claude (SuperClaude Framework)
**å®æ–½å®Œæˆåº¦**: âœ… 95% (4/5 ä¼˜åŒ–å®Œæˆï¼Œ1ä¸ªå¾…åç»­å®æ–½)
**è´¨é‡è¯„ä¼°**: â­â­â­â­â­ (5/5 æ˜Ÿ) - æ ¸å¿ƒå»é‡åŠŸèƒ½å®Œå–„
