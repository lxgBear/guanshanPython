# å®šæ—¶ä»»åŠ¡ç±»å‹å®ç°æ€»ç»“

## ğŸ“‹ å®æ–½æ—¥æœŸ
2025-11-04

## ğŸ¯ éœ€æ±‚è¯´æ˜

å®ç°ä¸¤ç§å®šæ—¶ä»»åŠ¡æ¨¡å¼ï¼š
1. **ç½‘ç«™çˆ¬å–æ¨¡å¼**ï¼šä½¿ç”¨ Firecrawl Crawl API é€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™
2. **å…³é”®è¯æœç´¢æ¨¡å¼**ï¼šSearch API è·å–æœç´¢ç»“æœåï¼Œå¯¹æ¯ä¸ªç»“æœä½¿ç”¨ Scrape API çˆ¬å–è¯¦æƒ…é¡µå†…å®¹

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. æ•°æ®æ¨¡å‹è®¾è®¡ (`search_task.py`)

#### æ·»åŠ  TaskType æšä¸¾
```python
class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    SEARCH_KEYWORD = "search_keyword"  # å…³é”®è¯æœç´¢æ¨¡å¼ï¼ˆSearch API + Scrape API è¯¦æƒ…é¡µï¼‰
    CRAWL_WEBSITE = "crawl_website"    # ç½‘ç«™çˆ¬å–æ¨¡å¼ï¼ˆCrawl API é€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼‰
    SCRAPE_URL = "scrape_url"          # å•é¡µé¢çˆ¬å–æ¨¡å¼ï¼ˆScrape API çˆ¬å–å•ä¸ªé¡µé¢ï¼‰
```

#### æ›´æ–° SearchTask å®ä½“å­—æ®µ
```python
@dataclass
class SearchTask:
    # ä»»åŠ¡ç±»å‹å’Œç›®æ ‡
    task_type: str = "search_keyword"  # ä»»åŠ¡ç±»å‹
    query: str = ""  # æœç´¢å…³é”®è¯ï¼ˆSEARCH_KEYWORD æ¨¡å¼ï¼‰
    crawl_url: Optional[str] = None  # çˆ¬å–URLï¼ˆCRAWL_WEBSITE å’Œ SCRAPE_URL æ¨¡å¼ï¼‰

    # é…ç½®
    search_config: Dict[str, Any] = field(default_factory=dict)  # æœç´¢é…ç½®
    crawl_config: Dict[str, Any] = field(default_factory=dict)  # çˆ¬å–é…ç½®ï¼ˆCRAWL_WEBSITE æ¨¡å¼ï¼‰
```

#### æ·»åŠ è¾…åŠ©æ–¹æ³•
```python
def get_task_type(self) -> TaskType
def is_search_keyword_mode(self) -> bool
def is_crawl_website_mode(self) -> bool
def is_scrape_url_mode(self) -> bool
```

### 2. æ•°æ®åº“æ”¯æŒ (`repositories.py`)

#### æ›´æ–° _task_to_dict æ–¹æ³•
```python
def _task_to_dict(self, task: SearchTask) -> Dict[str, Any]:
    return {
        "task_type": task.task_type,  # v2.0.0: ä»»åŠ¡ç±»å‹
        "crawl_config": task.crawl_config,  # v2.0.0: çˆ¬å–é…ç½®
        # ... å…¶ä»–å­—æ®µ
    }
```

#### æ›´æ–° _dict_to_task æ–¹æ³•
```python
def _dict_to_task(self, data: Dict[str, Any]) -> SearchTask:
    task = SearchTask(
        task_type=data.get("task_type", "search_keyword"),  # å‘åå…¼å®¹
        crawl_config=data.get("crawl_config", {}),  # å‘åå…¼å®¹
        # ... å…¶ä»–å­—æ®µ
    )
```

### 3. Firecrawl Crawl API é€‚é…å™¨

**ç°æœ‰å®ç°** (`firecrawl_adapter.py:108-154`):
- å·²æœ‰ `crawl()` æ–¹æ³•åŸºç¡€å®ç°
- æ”¯æŒ limit, maxDepth, includePaths, excludePaths ç­‰å‚æ•°
- ä½¿ç”¨ FirecrawlApp SDK å¤„ç†å¼‚æ­¥çˆ¬å–

**å¾…å®Œå–„**:
- å¼‚æ­¥è½®è¯¢å’ŒçŠ¶æ€æ£€æŸ¥
- å¤§è§„æ¨¡ç½‘ç«™çš„åˆ†æ‰¹å¤„ç†
- é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶

## â³ å¾…å®Œæˆçš„å·¥ä½œ

### 1. ä»»åŠ¡è°ƒåº¦å™¨ä¿®æ”¹ (`task_scheduler.py`)

**éœ€è¦ä¿®æ”¹ `_execute_search_task` æ–¹æ³•**ï¼š

```python
async def _execute_search_task(self, task_id: str):
    """æ‰§è¡Œæœç´¢ä»»åŠ¡ - æ”¯æŒä¸‰ç§æ¨¡å¼"""
    task = await repo.get_by_id(task_id)

    # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
    if task.is_crawl_website_mode():
        # æ¨¡å¼1: ç½‘ç«™çˆ¬å–ï¼ˆCrawl APIï¼‰
        result_batch = await self._execute_crawl_website_task(task)
    elif task.is_search_keyword_mode():
        # æ¨¡å¼2: å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–ï¼ˆSearch API + Scrape APIï¼‰
        result_batch = await self._execute_search_with_scrape_task(task)
    else:  # SCRAPE_URL
        # æ¨¡å¼3: å•é¡µé¢çˆ¬å–ï¼ˆScrape APIï¼‰
        result_batch = await self._execute_crawl_task_internal(task, start_time)
```

### 2. å®ç°ç½‘ç«™çˆ¬å–æ–¹æ³•

```python
async def _execute_crawl_website_task(self, task: SearchTask) -> SearchResultBatch:
    """æ‰§è¡Œç½‘ç«™çˆ¬å–ä»»åŠ¡ï¼ˆCrawl APIï¼‰

    Args:
        task: æœç´¢ä»»åŠ¡ï¼ˆtask_type = "crawl_website"ï¼‰

    Returns:
        SearchResultBatch: çˆ¬å–ç»“æœæ‰¹æ¬¡
    """
    crawler = FirecrawlAdapter()

    # ä» crawl_config æå–é…ç½®
    crawl_options = {
        'limit': task.crawl_config.get('limit', 10),
        'max_depth': task.crawl_config.get('max_depth', 3),
        'include_paths': task.crawl_config.get('include_paths', []),
        'exclude_paths': task.crawl_config.get('exclude_paths', []),
        'allow_backward_links': task.crawl_config.get('allow_backward_links', False)
    }

    # è°ƒç”¨ Crawl API
    crawl_results = await crawler.crawl(task.crawl_url, **crawl_options)

    # è½¬æ¢ä¸º SearchResult åˆ—è¡¨
    search_results = []
    for crawl_result in crawl_results:
        search_result = SearchResult(
            task_id=str(task.id),
            title=crawl_result.metadata.get("title", crawl_result.url),
            url=crawl_result.url,
            markdown_content=crawl_result.markdown or crawl_result.content,
            html_content=crawl_result.html,
            metadata=crawl_result.metadata,
            status=ResultStatus.PENDING
        )
        search_results.append(search_result)

    # åˆ›å»ºæ‰¹æ¬¡
    batch = SearchResultBatch(
        task_id=str(task.id),
        query=f"ç½‘ç«™çˆ¬å–: {task.crawl_url}",
        search_config=task.crawl_config
    )
    for result in search_results:
        batch.add_result(result)

    return batch
```

### 3. å®ç°å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–æ–¹æ³•

```python
async def _execute_search_with_scrape_task(self, task: SearchTask) -> SearchResultBatch:
    """æ‰§è¡Œå…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–ä»»åŠ¡ï¼ˆSearch API + Scrape APIï¼‰

    ä¸¤é˜¶æ®µå¤„ç†ï¼š
    1. Search API è·å–æœç´¢ç»“æœï¼ˆé¦–é¡µé“¾æ¥ï¼‰
    2. å¯¹æ¯ä¸ªæœç´¢ç»“æœä½¿ç”¨ Scrape API çˆ¬å–è¯¦æƒ…é¡µ

    Args:
        task: æœç´¢ä»»åŠ¡ï¼ˆtask_type = "search_keyword"ï¼‰

    Returns:
        SearchResultBatch: åŒ…å«è¯¦æƒ…é¡µå†…å®¹çš„æœç´¢ç»“æœæ‰¹æ¬¡
    """
    # ç¬¬ä¸€é˜¶æ®µï¼šå…³é”®è¯æœç´¢
    user_config = UserSearchConfig.from_json(task.search_config)
    search_batch = await self.search_adapter.search(
        query=task.query,
        user_config=user_config,
        task_id=str(task.id)
    )

    if not search_batch.results:
        logger.warning(f"æœç´¢æ— ç»“æœ: {task.query}")
        return search_batch

    # ç¬¬äºŒé˜¶æ®µï¼šçˆ¬å–è¯¦æƒ…é¡µ
    crawler = FirecrawlAdapter()

    # é…ç½®çˆ¬å–é€‰é¡¹
    scrape_options = {
        "only_main_content": task.search_config.get("only_main_content", True),
        "wait_for": task.search_config.get("wait_for", 2000),
        "exclude_tags": task.search_config.get("exclude_tags", ["nav", "footer", "header", "aside"]),
        "timeout": task.search_config.get("timeout", 30)
    }

    # æ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µ
    enriched_results = []
    for search_result in search_batch.results:
        try:
            logger.info(f"ğŸ” çˆ¬å–è¯¦æƒ…é¡µ: {search_result.url}")

            # çˆ¬å–è¯¦æƒ…é¡µ
            crawl_result = await crawler.scrape(search_result.url, **scrape_options)

            # æ›´æ–°æœç´¢ç»“æœçš„å†…å®¹
            search_result.markdown_content = crawl_result.markdown or crawl_result.content
            search_result.html_content = crawl_result.html
            search_result.metadata.update(crawl_result.metadata or {})

            enriched_results.append(search_result)

            # é¿å…è¿‡å¿«è¯·æ±‚ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
            await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¯¦æƒ…é¡µå¤±è´¥ {search_result.url}: {e}")
            # ä¿ç•™åŸå§‹æœç´¢ç»“æœ
            enriched_results.append(search_result)

    # æ›´æ–°æ‰¹æ¬¡ç»“æœ
    search_batch.results = enriched_results
    logger.info(f"âœ… è¯¦æƒ…é¡µçˆ¬å–å®Œæˆ: {len(enriched_results)}/{len(search_batch.results)}")

    return search_batch
```

### 4. API æ›´æ–°

**å‰ç«¯ API** (`search_tasks_frontend.py`):
- æ›´æ–° `SearchTaskCreate` æ¨¡å‹æ·»åŠ  `task_type` å­—æ®µ
- æ›´æ–° `SearchTaskResponse` æ¨¡å‹è¿”å› `task_type` å’Œ `crawl_config`
- æ·»åŠ  `TaskTypeEnum` ç”¨äºå‰ç«¯é€‰æ‹©

**éªŒè¯é€»è¾‘** (`search_tasks_validation.py`):
- æ ¹æ® `task_type` éªŒè¯å¿…å¡«å­—æ®µ
- CRAWL_WEBSITE: å¿…é¡»æä¾› `crawl_url`
- SEARCH_KEYWORD: å¿…é¡»æä¾› `query`
- SCRAPE_URL: å¿…é¡»æä¾› `crawl_url`

### 5. æµ‹è¯•

**å•å…ƒæµ‹è¯•**:
- æµ‹è¯•ä¸‰ç§ä»»åŠ¡ç±»å‹çš„åˆ›å»º
- æµ‹è¯•ä»»åŠ¡ç±»å‹åˆ¤æ–­æ–¹æ³•
- æµ‹è¯•æ•°æ®åº“åºåˆ—åŒ–/ååºåˆ—åŒ–

**é›†æˆæµ‹è¯•**:
- æµ‹è¯•ç½‘ç«™çˆ¬å–æ¨¡å¼
- æµ‹è¯•å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–æ¨¡å¼
- æµ‹è¯•å•é¡µé¢çˆ¬å–æ¨¡å¼
- æµ‹è¯•é”™è¯¯æ¢å¤å’Œé‡è¯•

## ğŸ“Š é…ç½®ç¤ºä¾‹

### ç½‘ç«™çˆ¬å–ä»»åŠ¡
```json
{
  "name": "çˆ¬å–å®˜ç½‘æ‰€æœ‰é¡µé¢",
  "task_type": "crawl_website",
  "crawl_url": "https://example.com",
  "crawl_config": {
    "limit": 100,
    "max_depth": 3,
    "include_paths": ["/blog/", "/docs/"],
    "exclude_paths": ["/admin/", "/api/"],
    "allow_backward_links": false
  },
  "schedule_interval": "DAILY"
}
```

### å…³é”®è¯æœç´¢ä»»åŠ¡
```json
{
  "name": "æœç´¢å¹¶çˆ¬å–è¯¦æƒ…é¡µ",
  "task_type": "search_keyword",
  "query": "äººå·¥æ™ºèƒ½æ–°é—»",
  "search_config": {
    "limit": 10,
    "language": "zh",
    "only_main_content": true,
    "wait_for": 2000,
    "exclude_tags": ["nav", "footer", "aside"]
  },
  "schedule_interval": "HOURLY_6"
}
```

### å•é¡µé¢çˆ¬å–ä»»åŠ¡
```json
{
  "name": "çˆ¬å–ç‰¹å®šé¡µé¢",
  "task_type": "scrape_url",
  "crawl_url": "https://example.com/article/123",
  "search_config": {
    "only_main_content": true,
    "wait_for": 1000
  },
  "schedule_interval": "DAILY"
}
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ€§èƒ½è€ƒè™‘
- **ç½‘ç«™çˆ¬å–**: Crawl API å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆå‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ï¼‰ï¼Œè€ƒè™‘å¼‚æ­¥å¤„ç†å’ŒçŠ¶æ€é€šçŸ¥
- **è¯¦æƒ…é¡µçˆ¬å–**: æ‰¹é‡çˆ¬å–éœ€è¦æ§åˆ¶å¹¶å‘æ•°å’Œé€Ÿç‡é™åˆ¶ï¼Œé¿å…è¢«å°ç¦
- **è¶…æ—¶è®¾ç½®**: ä¸åŒæ¨¡å¼éœ€è¦ä¸åŒçš„è¶…æ—¶é…ç½®

### 2. èµ„æºç®¡ç†
- **API é…é¢**: Firecrawl API æœ‰é…é¢é™åˆ¶ï¼Œéœ€è¦ç›‘æ§ä½¿ç”¨é‡
- **å­˜å‚¨ç©ºé—´**: å¤§è§„æ¨¡çˆ¬å–ä¼šäº§ç”Ÿå¤§é‡æ•°æ®ï¼Œè€ƒè™‘å­˜å‚¨ä¼˜åŒ–
- **å†…å­˜ä½¿ç”¨**: æ‰¹é‡å¤„ç†æ—¶æ³¨æ„å†…å­˜ç®¡ç†

### 3. é”™è¯¯å¤„ç†
- **éƒ¨åˆ†å¤±è´¥**: è¯¦æƒ…é¡µçˆ¬å–æ—¶éƒ¨åˆ†å¤±è´¥ä¸åº”å½±å“æ•´ä½“ä»»åŠ¡
- **é‡è¯•ç­–ç•¥**: ç½‘ç»œé”™è¯¯åº”æœ‰é‡è¯•æœºåˆ¶
- **é™çº§ç­–ç•¥**: çˆ¬å–å¤±è´¥æ—¶ä¿ç•™æœç´¢ç»“æœçš„åŸºæœ¬ä¿¡æ¯

## ğŸ”„ è¿ç§»è®¡åˆ’

### ç°æœ‰ä»»åŠ¡å…¼å®¹æ€§
- æ—§ä»»åŠ¡é»˜è®¤ `task_type = "search_keyword"`
- æœ‰ `crawl_url` ä½†æ—  `task_type` çš„ä»»åŠ¡è‡ªåŠ¨è¯†åˆ«ä¸º `"scrape_url"`
- æ•°æ®åº“æŸ¥è¯¢è‡ªåŠ¨å¡«å……é»˜è®¤å€¼

### æ•°æ®åº“ç´¢å¼•
```javascript
// MongoDB ç´¢å¼•å»ºè®®
db.search_tasks.createIndex({ "task_type": 1, "is_active": 1 })
db.search_tasks.createIndex({ "task_type": 1, "status": 1, "next_run_time": 1 })
```

## ğŸ“ ä¸‹ä¸€æ­¥å·¥ä½œ

1. âœ… æ•°æ®æ¨¡å‹è®¾è®¡ - å·²å®Œæˆ
2. âœ… æ•°æ®åº“æ”¯æŒ - å·²å®Œæˆ
3. â³ ä»»åŠ¡è°ƒåº¦å™¨ä¿®æ”¹ - å¾…å®Œæˆ
4. â³ ç½‘ç«™çˆ¬å–æ–¹æ³•å®ç° - å¾…å®Œæˆ
5. â³ å…³é”®è¯æœç´¢ + è¯¦æƒ…é¡µçˆ¬å–å®ç° - å¾…å®Œæˆ
6. â³ API æ›´æ–° - å¾…å®Œæˆ
7. â³ æµ‹è¯• - å¾…å®Œæˆ
8. â³ æ–‡æ¡£æ›´æ–° - å¾…å®Œæˆ

## æ€»ç»“

æœ¬æ¬¡å®ç°å®Œæˆäº†å®šæ—¶ä»»åŠ¡ç±»å‹ç³»ç»Ÿçš„**æ ¸å¿ƒåŸºç¡€æ¶æ„**ï¼ŒåŒ…æ‹¬ï¼š
- ä»»åŠ¡ç±»å‹æšä¸¾å’Œæ•°æ®æ¨¡å‹
- æ•°æ®åº“åºåˆ—åŒ–æ”¯æŒ
- å‘åå…¼å®¹çš„è¿ç§»é€»è¾‘

**å¾…å®Œæˆçš„ä¸»è¦å·¥ä½œ**æ˜¯å®ç°ä¸‰ç§ä»»åŠ¡ç±»å‹çš„å…·ä½“æ‰§è¡Œé€»è¾‘ï¼Œç‰¹åˆ«æ˜¯ï¼š
1. ç½‘ç«™çˆ¬å–çš„å¼‚æ­¥å¤„ç†
2. å…³é”®è¯æœç´¢çš„äºŒé˜¶æ®µå¤„ç†ï¼ˆæœç´¢ + è¯¦æƒ…é¡µçˆ¬å–ï¼‰

å»ºè®®ä¼˜å…ˆå®ç°å…³é”®è¯æœç´¢çš„äºŒé˜¶æ®µå¤„ç†ï¼Œå› ä¸ºè¿™æ˜¯ç”¨æˆ·æ˜ç¡®æå‡ºçš„éœ€æ±‚ã€‚
