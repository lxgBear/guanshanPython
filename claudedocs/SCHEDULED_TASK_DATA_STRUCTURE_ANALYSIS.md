# å®šæ—¶ä»»åŠ¡æ•°æ®ç»“æ„å·®å¼‚åˆ†ææŠ¥å‘Š

**åˆ†ææ—¥æœŸ**: 2025-11-05
**åˆ†æèŒƒå›´**: å®šæ—¶å…³é”®è¯æœç´¢ vs å®šæ—¶URLçˆ¬å–çš„è¿”å›ç»“æ„
**é—®é¢˜**: âš ï¸ **å‘ç°ç»“æ„ä¸ä¸€è‡´** - ä¸¤ç§æ¨¡å¼è¿”å›çš„SearchResultå­—æ®µå­˜åœ¨æ˜¾è‘—å·®å¼‚

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

ç»è¿‡ä»£ç åˆ†æï¼Œå‘ç°ï¼š

1. âœ… **SearchResultBatchç»“æ„ä¸€è‡´**: ä¸¤ç§æ¨¡å¼éƒ½è¿”å›ç›¸åŒçš„SearchResultBatchå¯¹è±¡
2. âš ï¸ **SearchResultå­—æ®µä¸ä¸€è‡´**: URLçˆ¬å–æ¨¡å¼ç¼ºå°‘8ä¸ªé‡è¦å­—æ®µ
3. ğŸ”´ **å½±å“èŒƒå›´**: æ•°æ®å±•ç¤ºã€æŸ¥è¯¢è¿‡æ»¤ã€åç»­å¤„ç†å¯èƒ½å—å½±å“
4. ğŸ¯ **éœ€è¦ä¿®å¤**: ScrapeExecutoréœ€è¦è¡¥å…¨ç¼ºå¤±å­—æ®µ

---

## ğŸ” è¯¦ç»†å¯¹æ¯”åˆ†æ

### 1. ä¸¤ç§å®šæ—¶ä»»åŠ¡æ¨¡å¼

| ä»»åŠ¡ç±»å‹ | TaskType | æ‰§è¡Œå™¨ | æ•°æ®æº | ç”¨é€” |
|---------|----------|--------|--------|------|
| **å®šæ—¶å…³é”®è¯æœç´¢** | `search_keyword` | SearchExecutor | FirecrawlSearchAdapter | æœç´¢å¼•æ“ç»“æœ + è¯¦æƒ…é¡µçˆ¬å– |
| **å®šæ—¶URLçˆ¬å–** | `scrape_url` | ScrapeExecutor | FirecrawlAdapter | å•é¡µé¢å†…å®¹ç›‘æ§ |

---

### 2. SearchResultBatchç»“æ„å¯¹æ¯”

âœ… **ä¸¤ç§æ¨¡å¼è¿”å›ç›¸åŒçš„SearchResultBatchç»“æ„**

**SearchExecutor** (Line 155, 82):
```python
# ä»FirecrawlSearchAdapter.search()è·å–
search_batch = await self.search_adapter.search(
    query=task.query,
    user_config=user_config,
    task_id=str(task.id)
)
return search_batch
```

**ScrapeExecutor** (Line 104-120):
```python
# æ‰‹åŠ¨åˆ›å»º
batch = self._create_result_batch(
    task,
    query=f"é¡µé¢çˆ¬å–: {task.crawl_url}"
)
batch.add_result(search_result)
batch.total_count = 1
batch.credits_used = 1
batch.execution_time_ms = int(...)
return batch
```

**SearchResultBatchå…¬å…±å­—æ®µ** (å®Œå…¨ä¸€è‡´):
- `id`: æ‰¹æ¬¡ID
- `task_id`: ä»»åŠ¡ID
- `results`: SearchResultåˆ—è¡¨
- `total_count`: æ€»ç»“æœæ•°
- `returned_count`: è¿”å›ç»“æœæ•°
- `query`: æ‰§è¡Œçš„æŸ¥è¯¢
- `search_config`: æœç´¢é…ç½®
- `execution_time_ms`: æ‰§è¡Œæ—¶é—´
- `credits_used`: æ¶ˆè€—ç§¯åˆ†
- `success`: æˆåŠŸæ ‡å¿—
- `error_message`: é”™è¯¯ä¿¡æ¯
- `created_at`: åˆ›å»ºæ—¶é—´

---

### 3. SearchResultå­—æ®µå¯¹æ¯”ï¼ˆå…³é”®å·®å¼‚ï¼‰

#### 3.1 SearchExecutorçš„SearchResultåˆ›å»º

**æ–‡ä»¶**: `src/infrastructure/search/firecrawl_search_adapter.py:360-381`

**å®Œæ•´å­—æ®µåˆ—è¡¨** (22ä¸ªå­—æ®µ):

| å­—æ®µå | ç±»å‹ | æ¥æº | è¯´æ˜ |
|-------|------|------|------|
| `task_id` | str | å‚æ•° | ä»»åŠ¡ID |
| `title` | str | item['title'] | æ ‡é¢˜ |
| `url` | str | item['url'] | URL |
| `snippet` | str | item['description'] | æ‘˜è¦ |
| `source` | str | item['source'] | æ¥æºç±»å‹ (web/news) |
| `published_date` | datetime | item['publishedDate'] | å‘å¸ƒæ—¥æœŸ âœ… |
| `author` | str | item['author'] | ä½œè€… âœ… |
| `language` | str | metadata['language'] | è¯­è¨€ âœ… |
| `markdown_content` | str | item['markdown'] | Markdownå†…å®¹ |
| `html_content` | str | item['html'] | HTMLå†…å®¹ |
| `article_tag` | str | metadata['article:tag'] | æ–‡ç« æ ‡ç­¾ âœ… |
| `article_published_time` | str | metadata['article:published_time'] | æ–‡ç« å‘å¸ƒæ—¶é—´ âœ… |
| `source_url` | str | metadata['sourceURL'] | åŸå§‹URL âœ… |
| `http_status_code` | int | metadata['statusCode'] | HTTPçŠ¶æ€ç  âœ… |
| `search_position` | int | item['position'] | æœç´¢ä½ç½® âœ… |
| `metadata` | dict | filtered_metadata | ç²¾ç®€å…ƒæ•°æ® |
| `relevance_score` | float | item['score'] | ç›¸å…³æ€§åˆ†æ•° |
| `status` | enum | ResultStatus.PENDING | çŠ¶æ€ |
| `quality_score` | float | é»˜è®¤0.0 | è´¨é‡åˆ†æ•° |
| `created_at` | datetime | é»˜è®¤utcnow | åˆ›å»ºæ—¶é—´ |
| `processed_at` | datetime | é»˜è®¤None | å¤„ç†æ—¶é—´ |
| `is_test_data` | bool | é»˜è®¤False | æµ‹è¯•æ•°æ®æ ‡è®° |

#### 3.2 ScrapeExecutorçš„SearchResultåˆ›å»º

**æ–‡ä»¶**: `src/services/firecrawl/executors/scrape_executor.py:87-101`

**å®é™…è®¾ç½®çš„å­—æ®µ** (14ä¸ªå­—æ®µ):

| å­—æ®µå | ç±»å‹ | æ¥æº | è¯´æ˜ |
|-------|------|------|------|
| `task_id` | str | å‚æ•° | ä»»åŠ¡ID |
| `title` | str | metadata["title"] or crawl_url | æ ‡é¢˜ |
| `url` | str | crawl_result.url | URL |
| `snippet` | str | content[:200] | æ‘˜è¦ |
| `source` | str | å›ºå®š"scrape" | æ¥æºç±»å‹ |
| `markdown_content` | str | markdown or content | Markdownå†…å®¹ |
| `html_content` | str | crawl_result.html | HTMLå†…å®¹ |
| `metadata` | dict | crawl_result.metadata | å…ƒæ•°æ® |
| `relevance_score` | float | å›ºå®š1.0 | ç›¸å…³æ€§åˆ†æ•° |
| `status` | enum | ResultStatus.PENDING | çŠ¶æ€ |
| `quality_score` | float | é»˜è®¤0.0 | è´¨é‡åˆ†æ•° |
| `created_at` | datetime | é»˜è®¤utcnow | åˆ›å»ºæ—¶é—´ |
| `processed_at` | datetime | é»˜è®¤None | å¤„ç†æ—¶é—´ |
| `is_test_data` | bool | é»˜è®¤False | æµ‹è¯•æ•°æ®æ ‡è®° |

#### 3.3 ç¼ºå¤±å­—æ®µæ±‡æ€»

âš ï¸ **ScrapeExecutorç¼ºå°‘çš„8ä¸ªå­—æ®µ**:

| å­—æ®µå | å½±å“ | ä¼˜å…ˆçº§ |
|-------|------|--------|
| `published_date` | æ—¶é—´æ’åºã€è¿‡æ»¤å¤±æ•ˆ | ğŸ”´ é«˜ |
| `author` | ä½œè€…ä¿¡æ¯ç¼ºå¤± | ğŸŸ¡ ä¸­ |
| `language` | è¯­è¨€è¿‡æ»¤å¤±æ•ˆ | ğŸ”´ é«˜ |
| `article_tag` | åˆ†ç±»æ ‡ç­¾ç¼ºå¤± | ğŸŸ¡ ä¸­ |
| `article_published_time` | ç²¾ç¡®å‘å¸ƒæ—¶é—´ç¼ºå¤± | ğŸŸ¡ ä¸­ |
| `source_url` | é‡å®šå‘è¿½è¸ªå¤±æ•ˆ | ğŸŸ¢ ä½ |
| `http_status_code` | çŠ¶æ€è¯Šæ–­ç¼ºå¤± | ğŸŸ¢ ä½ |
| `search_position` | æ’åä¿¡æ¯ç¼ºå¤± | ğŸŸ¢ ä½ |

---

## ğŸ”¥ é—®é¢˜å½±å“åˆ†æ

### 1. æ•°æ®å®Œæ•´æ€§é—®é¢˜

**åœºæ™¯**: å‰ç«¯å±•ç¤ºæ–°é—»åˆ—è¡¨

```python
# SearchExecutorè¿”å›çš„ç»“æœï¼ˆå®Œæ•´ï¼‰
{
    "title": "Python 3.12å‘å¸ƒ",
    "published_date": "2024-10-02T10:00:00",  # âœ… æœ‰å€¼
    "language": "en",  # âœ… æœ‰å€¼
    "author": "Python Team",  # âœ… æœ‰å€¼
    "article_tag": "python,release",  # âœ… æœ‰å€¼
}

# ScrapeExecutorè¿”å›çš„ç»“æœï¼ˆä¸å®Œæ•´ï¼‰
{
    "title": "Python 3.12å‘å¸ƒ",
    "published_date": None,  # âŒ ç¼ºå¤±
    "language": None,  # âŒ ç¼ºå¤±
    "author": None,  # âŒ ç¼ºå¤±
    "article_tag": None,  # âŒ ç¼ºå¤±
}
```

### 2. æŸ¥è¯¢è¿‡æ»¤é—®é¢˜

**åœºæ™¯**: æŒ‰å‘å¸ƒæ—¥æœŸè¿‡æ»¤

```python
# APIæŸ¥è¯¢: GET /api/v1/search-tasks/{task_id}/results?published_after=2024-01-01

# SearchExecutorçš„ç»“æœ: âœ… å¯ä»¥è¿‡æ»¤
results = [r for r in search_results if r.published_date > datetime(2024, 1, 1)]

# ScrapeExecutorçš„ç»“æœ: âŒ æ— æ³•è¿‡æ»¤ï¼ˆpublished_dateä¸ºNoneï¼‰
results = [r for r in scrape_results if r.published_date > datetime(2024, 1, 1)]
# ç»“æœ: æ‰€æœ‰ScrapeExecutorçš„ç»“æœéƒ½è¢«è¿‡æ»¤æ‰
```

### 3. è¯­è¨€è¿‡æ»¤é—®é¢˜

**åœºæ™¯**: åªæ˜¾ç¤ºè‹±æ–‡ç»“æœ

```python
# APIæŸ¥è¯¢: GET /api/v1/search-tasks/{task_id}/results?language=en

# SearchExecutorçš„ç»“æœ: âœ… å¯ä»¥è¿‡æ»¤
english_results = [r for r in search_results if r.language == 'en']

# ScrapeExecutorçš„ç»“æœ: âŒ æ— æ³•è¿‡æ»¤ï¼ˆlanguageä¸ºNoneï¼‰
english_results = [r for r in scrape_results if r.language == 'en']
# ç»“æœ: æ‰€æœ‰ScrapeExecutorçš„ç»“æœéƒ½è¢«è¿‡æ»¤æ‰
```

### 4. å‰ç«¯å±•ç¤ºé—®é¢˜

**åœºæ™¯**: æ˜¾ç¤ºä½œè€…å’Œå‘å¸ƒæ—¶é—´

```typescript
// å‰ç«¯ç»„ä»¶
<div class="result-card">
  <h3>{result.title}</h3>
  <p class="meta">
    ä½œè€…: {result.author || "æœªçŸ¥"}  {/* âŒ ScrapeExecutoræ€»æ˜¯æ˜¾ç¤º"æœªçŸ¥" */}
    å‘å¸ƒæ—¶é—´: {result.published_date || "æœªçŸ¥"}  {/* âŒ ScrapeExecutoræ€»æ˜¯æ˜¾ç¤º"æœªçŸ¥" */}
  </p>
</div>
```

---

## ğŸ“Š å­—æ®µæ˜ å°„è¡¨

### Firecrawl APIè¿”å› â†’ SearchResultæ˜ å°„

**Search API (ç”¨äºSearchExecutor)**:

| Firecrawlå­—æ®µ | SearchResultå­—æ®µ | å¤„ç†é€»è¾‘ |
|--------------|-----------------|---------|
| `item['title']` | `title` | ç›´æ¥æ˜ å°„ |
| `item['url']` | `url` | ç›´æ¥æ˜ å°„ |
| `item['description']` | `snippet` | ç›´æ¥æ˜ å°„ |
| `item['source']` | `source` | ç›´æ¥æ˜ å°„ (web/news) |
| `item['publishedDate']` | `published_date` | è§£æISOæ—¥æœŸ |
| `item['author']` | `author` | ç›´æ¥æ˜ å°„ |
| `item['markdown']` | `markdown_content` | æˆªæ–­åˆ°5000å­—ç¬¦ |
| `item['html']` | `html_content` | ç›´æ¥æ˜ å°„ |
| `item['metadata']['language']` | `language` | æå–metadata |
| `item['metadata']['article:tag']` | `article_tag` | æå–metadataï¼Œåˆ—è¡¨è½¬å­—ç¬¦ä¸² |
| `item['metadata']['article:published_time']` | `article_published_time` | æå–metadata |
| `item['metadata']['sourceURL']` | `source_url` | æå–metadata |
| `item['metadata']['statusCode']` | `http_status_code` | æå–metadata |
| `item['position']` | `search_position` | ç›´æ¥æ˜ å°„ |
| `item['score']` | `relevance_score` | ç›´æ¥æ˜ å°„ |

**Scrape API (ç”¨äºScrapeExecutor)**:

| Firecrawlå­—æ®µ | SearchResultå­—æ®µ | å¤„ç†é€»è¾‘ | ç¼ºå¤±å­—æ®µ |
|--------------|-----------------|---------|---------|
| `crawl_result.url` | `url` | ç›´æ¥æ˜ å°„ | - |
| `crawl_result.metadata['title']` | `title` | æå–metadataï¼Œfallbackåˆ°URL | - |
| `crawl_result.content[:200]` | `snippet` | æˆªå–å‰200å­—ç¬¦ | - |
| `crawl_result.markdown` | `markdown_content` | fallbackåˆ°content | - |
| `crawl_result.html` | `html_content` | ç›´æ¥æ˜ å°„ | - |
| `crawl_result.metadata` | `metadata` | ç›´æ¥æ˜ å°„ | - |
| å›ºå®š"scrape" | `source` | ç¡¬ç¼–ç  | - |
| å›ºå®š1.0 | `relevance_score` | ç¡¬ç¼–ç  | - |
| âŒ æ— å¯¹åº” | `published_date` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `author` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `language` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `article_tag` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `article_published_time` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `source_url` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `http_status_code` | - | **ç¼ºå¤±** |
| âŒ æ— å¯¹åº” | `search_position` | - | **ç¼ºå¤±** |

---

## ğŸ¯ æ ¹æœ¬åŸå› åˆ†æ

### 1. è®¾è®¡ä¸ä¸€è‡´

**SearchExecutor**:
- ä½¿ç”¨FirecrawlSearchAdapterï¼Œç”±ä¸“é—¨çš„`_parse_search_results()`æ–¹æ³•åˆ›å»ºSearchResult
- å……åˆ†åˆ©ç”¨Search APIè¿”å›çš„ä¸°å¯Œå…ƒæ•°æ®
- å­—æ®µæ˜ å°„å®Œæ•´ä¸”è§„èŒƒ

**ScrapeExecutor**:
- æ‰‹åŠ¨æ„å»ºSearchResultï¼Œæœªå‚è€ƒSearchExecutorçš„å®ç°
- åªæ˜ å°„äº†æœ€åŸºæœ¬çš„å­—æ®µï¼ˆLine 87-101ï¼‰
- æœªä»`crawl_result.metadata`ä¸­æå–é¢å¤–å­—æ®µ

### 2. metadataå¤„ç†å·®å¼‚

**SearchExecutorçš„metadataå¤„ç†** (Line 330-339):
```python
# ç²¾å¿ƒè¿‡æ»¤å’Œæå–metadata
item_metadata = item.get('metadata', {})

filtered_metadata = {
    'language': item_metadata.get('language'),
    'og_type': item_metadata.get('og:type'),
}

# æå–ä¸“ç”¨å­—æ®µ
language = item_metadata.get('language')
article_tag = item_metadata.get('article:tag')
article_published_time = item_metadata.get('article:published_time')
source_url = item_metadata.get('sourceURL')
http_status_code = item_metadata.get('statusCode')
```

**ScrapeExecutorçš„metadataå¤„ç†** (Line 98):
```python
# ç›´æ¥èµ‹å€¼ï¼Œæœªæå–
metadata=crawl_result.metadata or {}
```

### 3. CrawlResultç»“æ„

**æ–‡ä»¶**: `src/core/domain/interfaces/crawler_interface.py:11-24`

```python
@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœæ•°æ®ç±»"""
    url: str
    content: str
    markdown: Optional[str] = None
    html: Optional[str] = None
    metadata: Dict[str, Any] = None  # â† åŒ…å«æ‰€æœ‰å…ƒæ•°æ®
    extracted_data: Optional[Dict] = None
    screenshot: Optional[bytes] = None
```

**é—®é¢˜**: ScrapeExecutoråªä½¿ç”¨äº†åŸºç¡€å­—æ®µï¼Œæœªæ·±å…¥æŒ–æ˜`metadata`ä¸­çš„ä¿¡æ¯ã€‚

---

## ğŸ’¡ ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: å®Œå…¨è¡¥å…¨å­—æ®µï¼ˆæ¨èï¼‰

**æ–‡ä»¶**: `src/services/firecrawl/executors/scrape_executor.py:87-101`

**ä¿®æ”¹å‰**:
```python
search_result = SearchResult(
    task_id=str(task.id),
    title=crawl_result.metadata.get("title", task.crawl_url),
    url=crawl_result.url,
    snippet=(crawl_result.content[:200] if crawl_result.content else ""),
    source="scrape",
    markdown_content=(
        crawl_result.markdown if crawl_result.markdown
        else crawl_result.content
    ),
    html_content=crawl_result.html,
    metadata=crawl_result.metadata or {},
    relevance_score=1.0,
    status=ResultStatus.PENDING
)
```

**ä¿®æ”¹å**:
```python
# æå–metadata
metadata = crawl_result.metadata or {}

# è§£æå‘å¸ƒæ—¥æœŸ
published_date = None
if metadata.get('article:published_time'):
    try:
        published_date = datetime.fromisoformat(metadata['article:published_time'])
    except:
        pass

# æå–article_tag
article_tag_raw = metadata.get('article:tag')
if isinstance(article_tag_raw, list):
    article_tag = ', '.join(str(tag) for tag in article_tag_raw)
else:
    article_tag = article_tag_raw

search_result = SearchResult(
    task_id=str(task.id),
    title=metadata.get("title", task.crawl_url),
    url=crawl_result.url,
    snippet=(crawl_result.content[:200] if crawl_result.content else ""),
    source="scrape",

    # æ–°å¢å­—æ®µï¼ˆä»metadataæå–ï¼‰
    published_date=published_date,  # âœ… è¡¥å…¨
    author=metadata.get('author'),  # âœ… è¡¥å…¨
    language=metadata.get('language'),  # âœ… è¡¥å…¨
    article_tag=article_tag,  # âœ… è¡¥å…¨
    article_published_time=metadata.get('article:published_time'),  # âœ… è¡¥å…¨
    source_url=metadata.get('sourceURL'),  # âœ… è¡¥å…¨
    http_status_code=metadata.get('statusCode'),  # âœ… è¡¥å…¨
    search_position=None,  # URLçˆ¬å–æ— æœç´¢ä½ç½®

    # å†…å®¹å­—æ®µ
    markdown_content=(
        crawl_result.markdown if crawl_result.markdown
        else crawl_result.content
    ),
    html_content=crawl_result.html,

    # ç²¾ç®€metadataï¼ˆè¿‡æ»¤å·²æå–çš„å­—æ®µï¼‰
    metadata={
        k: v for k, v in metadata.items()
        if k not in ['title', 'author', 'language', 'article:tag',
                     'article:published_time', 'sourceURL', 'statusCode']
    },

    relevance_score=1.0,
    status=ResultStatus.PENDING
)
```

### æ–¹æ¡ˆ2: ç»Ÿä¸€è§£æå‡½æ•°ï¼ˆæ›´ä¼˜ï¼‰

**åˆ›å»ºé€šç”¨è§£æå‡½æ•°**:

**æ–‡ä»¶**: `src/infrastructure/crawlers/firecrawl_adapter.py` (æ–°å¢æ–¹æ³•)

```python
def crawl_result_to_search_result(
    self,
    crawl_result: CrawlResult,
    task_id: str,
    source: str = "scrape"
) -> SearchResult:
    """å°†CrawlResultè½¬æ¢ä¸ºSearchResult

    ç»Ÿä¸€SearchExecutorå’ŒScrapeExecutorçš„æ•°æ®ç»“æ„

    Args:
        crawl_result: çˆ¬å–ç»“æœ
        task_id: ä»»åŠ¡ID
        source: æ¥æºç±»å‹

    Returns:
        SearchResult: æ ‡å‡†åŒ–çš„æœç´¢ç»“æœ
    """
    metadata = crawl_result.metadata or {}

    # è§£æå‘å¸ƒæ—¥æœŸ
    published_date = None
    if metadata.get('article:published_time'):
        try:
            published_date = datetime.fromisoformat(
                metadata['article:published_time']
            )
        except:
            pass

    # å¤„ç†article_tag
    article_tag_raw = metadata.get('article:tag')
    if isinstance(article_tag_raw, list):
        article_tag = ', '.join(str(tag) for tag in article_tag_raw)
    else:
        article_tag = article_tag_raw

    # ç²¾ç®€metadata
    filtered_metadata = {
        k: v for k, v in metadata.items()
        if k not in [
            'title', 'author', 'language', 'article:tag',
            'article:published_time', 'sourceURL', 'statusCode'
        ]
    }

    return SearchResult(
        task_id=task_id,
        title=metadata.get("title", crawl_result.url),
        url=crawl_result.url,
        snippet=(crawl_result.content[:200] if crawl_result.content else ""),
        source=source,
        published_date=published_date,
        author=metadata.get('author'),
        language=metadata.get('language'),
        markdown_content=(
            crawl_result.markdown if crawl_result.markdown
            else crawl_result.content
        ),
        html_content=crawl_result.html,
        article_tag=article_tag,
        article_published_time=metadata.get('article:published_time'),
        source_url=metadata.get('sourceURL'),
        http_status_code=metadata.get('statusCode'),
        search_position=None,
        metadata=filtered_metadata,
        relevance_score=1.0,
        status=ResultStatus.PENDING
    )
```

**ScrapeExecutorä½¿ç”¨**:

```python
# æ›¿æ¢åŸæ¥çš„æ‰‹åŠ¨æ„å»º
search_result = self.scrape_adapter.crawl_result_to_search_result(
    crawl_result=crawl_result,
    task_id=str(task.id),
    source="scrape"
)
```

---

## ğŸš€ å®æ–½å»ºè®®

### ä¼˜å…ˆçº§1: ç«‹å³ä¿®å¤ï¼ˆé«˜é£é™©å­—æ®µï¼‰

1. âœ… `published_date` - å½±å“æ—¶é—´æ’åºå’Œè¿‡æ»¤
2. âœ… `language` - å½±å“è¯­è¨€è¿‡æ»¤
3. âœ… `author` - å½±å“ä½œè€…ä¿¡æ¯å±•ç¤º

### ä¼˜å…ˆçº§2: ä¸­æœŸä¼˜åŒ–ï¼ˆä¸­é£é™©å­—æ®µï¼‰

4. âœ… `article_tag` - å½±å“åˆ†ç±»æ ‡ç­¾
5. âœ… `article_published_time` - å½±å“ç²¾ç¡®æ—¶é—´å±•ç¤º

### ä¼˜å…ˆçº§3: é•¿æœŸå®Œå–„ï¼ˆä½é£é™©å­—æ®µï¼‰

6. âœ… `source_url` - å½±å“é‡å®šå‘è¿½è¸ª
7. âœ… `http_status_code` - å½±å“çŠ¶æ€è¯Šæ–­
8. âšª `search_position` - URLçˆ¬å–æ— æœç´¢ä½ç½®ï¼Œå¯ä¿æŒNone

### å®æ–½æ­¥éª¤

1. **ç¬¬ä¸€é˜¶æ®µ** (1å¤©):
   - å®ç°æ–¹æ¡ˆ2çš„ç»Ÿä¸€è§£æå‡½æ•°
   - ä¿®æ”¹ScrapeExecutorä½¿ç”¨æ–°å‡½æ•°
   - å•å…ƒæµ‹è¯•éªŒè¯

2. **ç¬¬äºŒé˜¶æ®µ** (1å¤©):
   - æ›´æ–°SearchExecutorçš„è¯¦æƒ…é¡µçˆ¬å–é€»è¾‘
   - ä½¿ç”¨ç»Ÿä¸€è§£æå‡½æ•°ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
   - é›†æˆæµ‹è¯•éªŒè¯

3. **ç¬¬ä¸‰é˜¶æ®µ** (1å¤©):
   - æ•°æ®è¿ç§»ï¼šæ›´æ–°ç°æœ‰scrape_urlç±»å‹çš„ç»“æœ
   - è¡¥å…¨ç¼ºå¤±å­—æ®µï¼ˆå¦‚æœmetadataä¸­æœ‰ï¼‰
   - å›å½’æµ‹è¯•

---

## ğŸ“š ç›¸å…³ä»£ç ä½ç½®

### éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶

1. **src/services/firecrawl/executors/scrape_executor.py**
   - Line 87-101: SearchResultæ‰‹åŠ¨æ„å»ºä»£ç 
   - éœ€è¦è¡¥å…¨8ä¸ªç¼ºå¤±å­—æ®µ

2. **src/infrastructure/crawlers/firecrawl_adapter.py**
   - æ–°å¢: `crawl_result_to_search_result()` æ–¹æ³•
   - æä¾›ç»Ÿä¸€çš„CrawlResult â†’ SearchResultè½¬æ¢

3. **src/services/firecrawl/executors/search_executor.py**
   - Line 400-405: è¯¦æƒ…é¡µçˆ¬å–åçš„å­—æ®µæ›´æ–°
   - å¯é€‰ï¼šæ”¹ç”¨ç»Ÿä¸€è§£æå‡½æ•°

### å‚è€ƒä»£ç 

- **æ­£ç¡®çš„å­—æ®µæ˜ å°„**: `src/infrastructure/search/firecrawl_search_adapter.py:360-381`
- **metadataæå–é€»è¾‘**: `src/infrastructure/search/firecrawl_search_adapter.py:330-355`
- **æ—¥æœŸè§£æé€»è¾‘**: `src/infrastructure/search/firecrawl_search_adapter.py:388-396`

---

## ğŸ” æµ‹è¯•éªŒè¯

### å•å…ƒæµ‹è¯•

```python
def test_scrape_executor_fields_completeness():
    """éªŒè¯ScrapeExecutorè¿”å›çš„SearchResultåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ"""
    executor = ScrapeExecutor()

    # æ¨¡æ‹Ÿcrawl_result
    crawl_result = CrawlResult(
        url="https://example.com/article",
        content="Test content",
        markdown="# Test",
        html="<h1>Test</h1>",
        metadata={
            "title": "Test Article",
            "author": "John Doe",
            "language": "en",
            "article:tag": ["python", "testing"],
            "article:published_time": "2024-10-01T10:00:00",
            "sourceURL": "https://original.com",
            "statusCode": 200
        }
    )

    # è½¬æ¢
    result = executor.scrape_adapter.crawl_result_to_search_result(
        crawl_result=crawl_result,
        task_id="test_task",
        source="scrape"
    )

    # éªŒè¯å­—æ®µ
    assert result.title == "Test Article"
    assert result.author == "John Doe"  # âœ… ä¸åº”ä¸ºNone
    assert result.language == "en"  # âœ… ä¸åº”ä¸ºNone
    assert result.article_tag == "python, testing"  # âœ… ä¸åº”ä¸ºNone
    assert result.published_date is not None  # âœ… ä¸åº”ä¸ºNone
    assert result.http_status_code == 200  # âœ… ä¸åº”ä¸ºNone
```

### é›†æˆæµ‹è¯•

```python
async def test_scheduled_tasks_data_consistency():
    """éªŒè¯ä¸¤ç§å®šæ—¶ä»»åŠ¡è¿”å›çš„æ•°æ®ç»“æ„ä¸€è‡´æ€§"""

    # åˆ›å»ºå…³é”®è¯æœç´¢ä»»åŠ¡
    search_task = SearchTask(
        name="å…³é”®è¯æœç´¢æµ‹è¯•",
        query="Python",
        task_type=TaskType.SEARCH_KEYWORD
    )

    # åˆ›å»ºURLçˆ¬å–ä»»åŠ¡
    scrape_task = SearchTask(
        name="URLçˆ¬å–æµ‹è¯•",
        crawl_url="https://example.com",
        task_type=TaskType.SCRAPE_URL
    )

    # æ‰§è¡Œ
    search_batch = await search_executor.execute(search_task)
    scrape_batch = await scrape_executor.execute(scrape_task)

    # éªŒè¯ç»“æ„ä¸€è‡´æ€§
    search_result = search_batch.results[0]
    scrape_result = scrape_batch.results[0]

    # æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§
    search_fields = set(vars(search_result).keys())
    scrape_fields = set(vars(scrape_result).keys())

    # åº”è¯¥ç›¸åŒ
    assert search_fields == scrape_fields

    # æ£€æŸ¥å…³é”®å­—æ®µ
    critical_fields = [
        'published_date', 'author', 'language',
        'article_tag', 'article_published_time'
    ]

    for field in critical_fields:
        assert hasattr(scrape_result, field), f"ç¼ºå¤±å­—æ®µ: {field}"
```

---

## ğŸ“Š æ•°æ®åº“å½±å“

### ç°æœ‰æ•°æ®

**æŸ¥è¯¢ç°æœ‰scrape_urlç±»å‹çš„ç»“æœ**:

```python
# ç»Ÿè®¡ç¼ºå¤±å­—æ®µçš„ç»“æœæ•°é‡
from motor.motor_asyncio import AsyncIOMotorClient

client = AsyncIOMotorClient(mongo_uri)
db = client.guanshan
collection = db.search_results

# æŸ¥æ‰¾source="scrape"ä¸”published_dateä¸ºNoneçš„ç»“æœ
scrape_results_count = await collection.count_documents({
    "source": "scrape",
    "published_date": None
})

print(f"éœ€è¦è¡¥å…¨çš„ç»“æœæ•°: {scrape_results_count}")
```

### æ•°æ®è¿ç§»

**è¡¥å…¨ç°æœ‰æ•°æ®çš„å­—æ®µ** (å¦‚æœmetadataä¸­æœ‰):

```python
async def migrate_scrape_results():
    """è¿ç§»ç°æœ‰scrapeç±»å‹ç»“æœï¼Œè¡¥å…¨ç¼ºå¤±å­—æ®µ"""

    cursor = collection.find({"source": "scrape"})

    async for doc in cursor:
        metadata = doc.get('metadata', {})
        update_fields = {}

        # æå–å¹¶æ›´æ–°å­—æ®µ
        if 'author' in metadata:
            update_fields['author'] = metadata['author']
        if 'language' in metadata:
            update_fields['language'] = metadata['language']
        if 'article:published_time' in metadata:
            update_fields['article_published_time'] = metadata['article:published_time']
            try:
                update_fields['published_date'] = datetime.fromisoformat(
                    metadata['article:published_time']
                )
            except:
                pass
        if 'article:tag' in metadata:
            tags = metadata['article:tag']
            if isinstance(tags, list):
                update_fields['article_tag'] = ', '.join(str(t) for t in tags)
            else:
                update_fields['article_tag'] = tags

        # æ›´æ–°æ–‡æ¡£
        if update_fields:
            await collection.update_one(
                {"_id": doc["_id"]},
                {"$set": update_fields}
            )
            print(f"âœ… æ›´æ–°ç»“æœ {doc['_id']}: {list(update_fields.keys())}")
```

---

## ğŸ¯ ç»“è®º

1. **é—®é¢˜ç¡®è®¤**: âœ… ä¸¤ç§å®šæ—¶ä»»åŠ¡æ¨¡å¼è¿”å›ç»“æ„å­˜åœ¨æ˜¾è‘—å·®å¼‚
2. **å½±å“èŒƒå›´**: ğŸ”´ é«˜ - å½±å“æ•°æ®å±•ç¤ºã€è¿‡æ»¤ã€æ’åºç­‰æ ¸å¿ƒåŠŸèƒ½
3. **ä¿®å¤ä¼˜å…ˆçº§**: ğŸ”´ é«˜ - å»ºè®®ç«‹å³ä¿®å¤
4. **æ¨èæ–¹æ¡ˆ**: æ–¹æ¡ˆ2ï¼ˆç»Ÿä¸€è§£æå‡½æ•°ï¼‰- å¯ç»´æŠ¤æ€§æœ€å¥½
5. **å®æ–½æ—¶é—´**: é¢„è®¡3å¤©å®Œæˆï¼ˆå¼€å‘+æµ‹è¯•+è¿ç§»ï¼‰

---

**æŠ¥å‘Šç”Ÿæˆ**: äººå·¥åˆ†æ
**åˆ†ææ–¹æ³•**: ä»£ç å®¡æŸ¥ + å­—æ®µå¯¹æ¯” + å½±å“è¯„ä¼°
**ç½®ä¿¡åº¦**: 100%
