# Firecrawlé›†æˆæ¶æ„åˆ†ææŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

åŸºäºå¯¹Firecrawl Python SDKã€API v2æ–‡æ¡£å’Œé«˜çº§çˆ¬å–æŒ‡å—çš„æ·±åº¦åˆ†æï¼ŒFirecrawlå®Œç¾å¥‘åˆå…³å±±æ™ºèƒ½ç³»ç»Ÿçš„éœ€æ±‚ã€‚å…¶å¼‚æ­¥æ”¯æŒã€ç»“æ„åŒ–æ•°æ®æå–å’ŒåŠ¨æ€å†…å®¹å¤„ç†èƒ½åŠ›ä¸æˆ‘ä»¬çš„å…­è¾¹å½¢æ¶æ„å’ŒRAGç®¡é“é«˜åº¦åŒ¹é…ã€‚

### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **åŸç”Ÿå¼‚æ­¥æ”¯æŒ**: AsyncFirecrawlä¸FastAPIå®Œç¾é…åˆ
- âœ… **æ™ºèƒ½æ•°æ®æå–**: è‡ªç„¶è¯­è¨€schemaå®šä¹‰ï¼Œé€‚åˆæƒ…æŠ¥å¤„ç†
- âœ… **åŠ¨æ€å†…å®¹å¤„ç†**: Actionsç³»ç»Ÿå¤„ç†JavaScriptæ¸²æŸ“å†…å®¹
- âœ… **è§„æ¨¡åŒ–çˆ¬å–**: Map/CrawlåŠŸèƒ½æ”¯æŒå…¨ç«™æƒ…æŠ¥æ”¶é›†
- âœ… **æ ¼å¼çµæ´»æ€§**: Markdown for LLM, HTML for structure, JSON for data

## ğŸ—ï¸ é›†æˆæ¶æ„è®¾è®¡

### 1. å…­è¾¹å½¢æ¶æ„é›†æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           API Layer (FastAPI)            â”‚
â”‚         CrawlController endpoints        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Application Services              â”‚
â”‚  CrawlerService â†’ DocumentProcessor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Domain (Core Logic)              â”‚
â”‚  CrawlerInterface â†’ Document Entity      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Infrastructure (Firecrawl)          â”‚
â”‚  FirecrawlAdapter â†’ AsyncFirecrawl       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. æ ¸å¿ƒæ¥å£å®šä¹‰

```python
# src/core/domain/interfaces/crawler_interface.py
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœæ•°æ®ç±»"""
    url: str
    content: str
    markdown: Optional[str] = None
    html: Optional[str] = None
    metadata: Dict[str, Any] = None
    extracted_data: Optional[Dict] = None
    screenshot: Optional[bytes] = None

class CrawlerInterface(ABC):
    """çˆ¬è™«æ¥å£å®šä¹‰"""
    
    @abstractmethod
    async def scrape(self, url: str, **options) -> CrawlResult:
        """å•é¡µé¢çˆ¬å–"""
        pass
    
    @abstractmethod
    async def crawl(self, url: str, limit: int = 10, **options) -> List[CrawlResult]:
        """å…¨ç«™çˆ¬å–"""
        pass
    
    @abstractmethod
    async def map(self, url: str, limit: int = 100) -> List[str]:
        """ç«™ç‚¹åœ°å›¾ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    async def extract(self, url: str, schema: Dict) -> Dict:
        """ç»“æ„åŒ–æ•°æ®æå–"""
        pass
```

### 3. Firecrawlé€‚é…å™¨å®ç°

```python
# src/infrastructure/crawlers/firecrawl_adapter.py
from typing import Optional, Dict, Any, List
from firecrawl import AsyncFirecrawl
from tenacity import retry, stop_after_attempt, wait_exponential
from src.core.domain.interfaces import CrawlerInterface, CrawlResult
from src.config import settings
import structlog

logger = structlog.get_logger()

class FirecrawlAdapter(CrawlerInterface):
    """Firecrawlçˆ¬è™«é€‚é…å™¨"""
    
    def __init__(self):
        self.client = AsyncFirecrawl(api_key=settings.FIRECRAWL_API_KEY)
        self._cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def scrape(self, url: str, **options) -> CrawlResult:
        """å®ç°å•é¡µçˆ¬å–withé‡è¯•æœºåˆ¶"""
        try:
            # é…ç½®çˆ¬å–é€‰é¡¹
            scrape_options = {
                'formats': ['markdown', 'html'],
                'includeTags': options.get('include_tags', []),
                'excludeTags': options.get('exclude_tags', ['nav', 'footer']),
                'waitFor': options.get('wait_for', 1000),
                'actions': options.get('actions', [])
            }
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.client.scrape(url, **scrape_options)
            
            # è½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡
            return CrawlResult(
                url=url,
                content=result.get('content', ''),
                markdown=result.get('markdown'),
                html=result.get('html'),
                metadata=result.get('metadata', {}),
                screenshot=result.get('screenshot')
            )
            
        except Exception as e:
            logger.error(f"Firecrawl scrape failed", url=url, error=str(e))
            raise
    
    async def crawl(self, url: str, limit: int = 10, **options) -> List[CrawlResult]:
        """å®ç°å…¨ç«™çˆ¬å–"""
        try:
            crawl_options = {
                'limit': limit,
                'maxDepth': options.get('max_depth', 3),
                'includePaths': options.get('include_paths', []),
                'excludePaths': options.get('exclude_paths', []),
                'allowBackwardLinks': options.get('allow_backward', False)
            }
            
            # å¯åŠ¨çˆ¬å–ä»»åŠ¡
            job = await self.client.crawl(url, **crawl_options)
            
            # ç­‰å¾…å®Œæˆæˆ–è·å–ç»“æœ
            results = []
            if job.get('success'):
                for page in job.get('data', []):
                    results.append(CrawlResult(
                        url=page.get('url'),
                        content=page.get('content', ''),
                        markdown=page.get('markdown'),
                        metadata=page.get('metadata', {})
                    ))
            
            return results
            
        except Exception as e:
            logger.error(f"Firecrawl crawl failed", url=url, error=str(e))
            raise
    
    async def map(self, url: str, limit: int = 100) -> List[str]:
        """ç”Ÿæˆç«™ç‚¹URLåœ°å›¾"""
        try:
            result = await self.client.map(url, limit=limit)
            return result.get('urls', [])
        except Exception as e:
            logger.error(f"Firecrawl map failed", url=url, error=str(e))
            return []
    
    async def extract(self, url: str, schema: Dict) -> Dict:
        """ä½¿ç”¨è‡ªç„¶è¯­è¨€schemaæå–ç»“æ„åŒ–æ•°æ®"""
        try:
            # Firecrawlçš„extractç«¯ç‚¹æ”¯æŒè‡ªç„¶è¯­è¨€æè¿°
            result = await self.client.extract(
                url=url,
                schema=schema,
                formats=['markdown']
            )
            return result.get('data', {})
        except Exception as e:
            logger.error(f"Firecrawl extract failed", url=url, error=str(e))
            return {}
```

### 4. åº”ç”¨æœåŠ¡å±‚

```python
# src/application/services/crawler_service.py
from typing import List, Optional
from uuid import UUID
from src.core.domain.entities import Document
from src.core.domain.services import DocumentService
from src.infrastructure.crawlers import FirecrawlAdapter
from src.infrastructure.cache import CacheManager
from src.infrastructure.tasks import process_document_task
import hashlib

class CrawlerApplicationService:
    """çˆ¬è™«åº”ç”¨æœåŠ¡"""
    
    def __init__(
        self,
        document_service: DocumentService,
        crawler: FirecrawlAdapter,
        cache: CacheManager
    ):
        self.document_service = document_service
        self.crawler = crawler
        self.cache = cache
    
    async def scrape_and_process(self, url: str) -> Document:
        """çˆ¬å–å¹¶å¤„ç†å•ä¸ªURL"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"scrape:{hashlib.md5(url.encode()).hexdigest()}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # æ‰§è¡Œçˆ¬å–
        result = await self.crawler.scrape(url)
        
        # åˆ›å»ºæ–‡æ¡£
        document = await self.document_service.create_document(
            url=url,
            content=result.markdown or result.content,
            metadata={
                'html': result.html,
                'extracted': result.metadata
            }
        )
        
        # è§¦å‘å¼‚æ­¥å¤„ç†ï¼ˆåµŒå…¥ç”Ÿæˆã€RAGç´¢å¼•ï¼‰
        process_document_task.delay(str(document.id))
        
        # ç¼“å­˜ç»“æœ
        await self.cache.set(cache_key, document, ttl=3600)
        
        return document
    
    async def crawl_website(self, url: str, limit: int = 10) -> List[Document]:
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        # è·å–ç«™ç‚¹åœ°å›¾
        urls = await self.crawler.map(url, limit=limit)
        
        # æ‰¹é‡åˆ›å»ºçˆ¬å–ä»»åŠ¡
        documents = []
        for target_url in urls[:limit]:
            try:
                doc = await self.scrape_and_process(target_url)
                documents.append(doc)
            except Exception as e:
                logger.warning(f"Failed to process {target_url}: {e}")
                continue
        
        return documents
    
    async def extract_intelligence(
        self,
        url: str,
        extraction_prompt: str
    ) -> Dict:
        """æå–æƒ…æŠ¥æ•°æ®"""
        # æ„å»ºextraction schema
        schema = {
            "type": "object",
            "description": extraction_prompt,
            "properties": {
                "summary": {"type": "string"},
                "key_points": {"type": "array"},
                "entities": {"type": "array"},
                "sentiment": {"type": "string"},
                "relevance_score": {"type": "number"}
            }
        }
        
        # æ‰§è¡Œæå–
        extracted = await self.crawler.extract(url, schema)
        
        # å­˜å‚¨æå–ç»“æœ
        document = await self.document_service.create_document(
            url=url,
            content=str(extracted),
            metadata={'extraction_type': 'intelligence'}
        )
        
        return extracted
```

### 5. APIç«¯ç‚¹

```python
# src/api/v1/endpoints/crawl.py
from fastapi import APIRouter, Depends, BackgroundTasks
from src.api.deps import get_crawler_service
from src.api.v1.schemas import (
    ScrapeRequest, CrawlRequest, ExtractRequest,
    ScrapeResponse, CrawlResponse
)

router = APIRouter(prefix="/crawl", tags=["crawling"])

@router.post("/scrape", response_model=ScrapeResponse)
async def scrape_url(
    request: ScrapeRequest,
    service = Depends(get_crawler_service)
):
    """çˆ¬å–å•ä¸ªURL"""
    document = await service.scrape_and_process(request.url)
    return ScrapeResponse(
        document_id=document.id,
        url=document.url,
        content=document.content[:500],  # è¿”å›æ‘˜è¦
        status="completed"
    )

@router.post("/crawl", response_model=CrawlResponse)
async def crawl_website(
    request: CrawlRequest,
    background_tasks: BackgroundTasks,
    service = Depends(get_crawler_service)
):
    """çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼ˆå¼‚æ­¥ï¼‰"""
    # åˆ›å»ºçˆ¬å–ä»»åŠ¡
    task_id = str(uuid4())
    
    # åå°æ‰§è¡Œ
    background_tasks.add_task(
        service.crawl_website,
        request.url,
        request.limit
    )
    
    return CrawlResponse(
        task_id=task_id,
        status="processing",
        message=f"Crawling {request.url} with limit {request.limit}"
    )

@router.post("/extract")
async def extract_data(
    request: ExtractRequest,
    service = Depends(get_crawler_service)
):
    """æå–ç»“æ„åŒ–æ•°æ®"""
    result = await service.extract_intelligence(
        url=request.url,
        extraction_prompt=request.prompt
    )
    return result
```

## ğŸš€ é«˜çº§åŠŸèƒ½å®ç°

### 1. åŠ¨æ€å†…å®¹å¤„ç†

```python
async def scrape_dynamic_content(url: str):
    """å¤„ç†JavaScriptæ¸²æŸ“å†…å®¹"""
    actions = [
        {"type": "wait", "milliseconds": 2000},
        {"type": "click", "selector": "#load-more"},
        {"type": "scroll", "direction": "down", "amount": 500},
        {"type": "wait", "milliseconds": 1000}
    ]
    
    result = await crawler.scrape(
        url,
        actions=actions,
        wait_for="#dynamic-content"
    )
    return result
```

### 2. æ™ºèƒ½æ•°æ®æå–

```python
async def extract_article_intelligence(url: str):
    """æå–æ–‡ç« æƒ…æŠ¥"""
    schema = {
        "type": "object",
        "description": "Extract article intelligence for analysis",
        "properties": {
            "title": {"type": "string", "description": "Article title"},
            "author": {"type": "string", "description": "Author name"},
            "publish_date": {"type": "string", "description": "Publication date"},
            "summary": {"type": "string", "description": "3-sentence summary"},
            "key_topics": {"type": "array", "description": "Main topics discussed"},
            "sentiment": {"type": "string", "enum": ["positive", "neutral", "negative"]},
            "credibility_score": {"type": "number", "description": "0-1 credibility rating"}
        }
    }
    
    return await crawler.extract(url, schema)
```

### 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# src/infrastructure/tasks/crawl_tasks.py
from celery import group

@celery_app.task
def batch_crawl_task(urls: List[str]):
    """æ‰¹é‡çˆ¬å–ä»»åŠ¡"""
    # åˆ›å»ºä»»åŠ¡ç»„
    job_group = group(
        scrape_task.s(url) for url in urls
    )
    
    # å¹¶è¡Œæ‰§è¡Œ
    result = job_group.apply_async()
    return result.get()
```

## ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 1. æ™ºèƒ½ç¼“å­˜

```python
class CrawlCache:
    """çˆ¬å–ç¼“å­˜ç®¡ç†"""
    
    async def get_or_crawl(self, url: str, ttl: int = 3600):
        # åŸºäºå†…å®¹ç±»å‹çš„åŠ¨æ€TTL
        if "news" in url:
            ttl = 900  # æ–°é—»15åˆ†é’Ÿ
        elif "docs" in url:
            ttl = 86400  # æ–‡æ¡£24å°æ—¶
        
        cached = await redis.get(url)
        if cached:
            return cached
        
        result = await crawler.scrape(url)
        await redis.set(url, result, ttl)
        return result
```

### 2. é€‰æ‹©æ€§çˆ¬å–

```python
crawl_options = {
    'includePaths': ['/blog/*', '/news/*'],  # åªçˆ¬å–ç‰¹å®šè·¯å¾„
    'excludePaths': ['/admin/*', '/api/*'],  # æ’é™¤æ— å…³è·¯å¾„
    'includeTags': ['article', 'main'],      # åªæå–ä¸»è¦å†…å®¹
    'excludeTags': ['nav', 'footer', 'ads']  # æ’é™¤å¯¼èˆªå’Œå¹¿å‘Š
}
```

## ğŸ”’ å®‰å…¨ä¸åˆè§„

### 1. APIå¯†é’¥ç®¡ç†

```python
# ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†æœåŠ¡
import os
from cryptography.fernet import Fernet

class SecureConfig:
    @staticmethod
    def get_api_key():
        encrypted_key = os.getenv("ENCRYPTED_FIRECRAWL_KEY")
        cipher = Fernet(os.getenv("ENCRYPTION_KEY"))
        return cipher.decrypt(encrypted_key).decode()
```

### 2. é€Ÿç‡é™åˆ¶

```python
from aioredis import Redis
import asyncio

class RateLimiter:
    """APIé€Ÿç‡é™åˆ¶å™¨"""
    
    async def check_rate_limit(self, key: str, limit: int = 100):
        current = await redis.incr(key)
        if current == 1:
            await redis.expire(key, 3600)
        
        if current > limit:
            raise RateLimitExceeded("API rate limit exceeded")
        
        return current
```

## âš ï¸ é”™è¯¯å¤„ç†

```python
class FirecrawlErrorHandler:
    """Firecrawlé”™è¯¯å¤„ç†"""
    
    ERROR_MAPPING = {
        400: "InvalidRequestError",
        401: "AuthenticationError",
        429: "RateLimitError",
        500: "ServerError"
    }
    
    @classmethod
    def handle_error(cls, status_code: int, message: str):
        error_type = cls.ERROR_MAPPING.get(status_code, "UnknownError")
        
        if status_code == 429:
            # é€Ÿç‡é™åˆ¶ - ç­‰å¾…åé‡è¯•
            return {"retry": True, "wait": 60}
        elif status_code >= 500:
            # æœåŠ¡å™¨é”™è¯¯ - æŒ‡æ•°é€€é¿
            return {"retry": True, "backoff": True}
        else:
            # å®¢æˆ·ç«¯é”™è¯¯ - ä¸é‡è¯•
            return {"retry": False, "error": error_type}
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

```python
# src/utils/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# FirecrawlæŒ‡æ ‡
crawl_requests_total = Counter(
    'firecrawl_requests_total',
    'Total Firecrawl API requests',
    ['endpoint', 'status']
)

crawl_duration = Histogram(
    'firecrawl_duration_seconds',
    'Firecrawl request duration',
    ['endpoint']
)

crawl_queue_size = Gauge(
    'firecrawl_queue_size',
    'Current crawl queue size'
)

api_quota_remaining = Gauge(
    'firecrawl_api_quota',
    'Remaining API quota'
)
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

```python
# tests/test_firecrawl_integration.py
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_scrape_with_retry():
    """æµ‹è¯•é‡è¯•æœºåˆ¶"""
    with patch('firecrawl.AsyncFirecrawl') as mock_client:
        mock_client.scrape = AsyncMock()
        mock_client.scrape.side_effect = [
            Exception("Network error"),
            {"content": "Success"}
        ]
        
        adapter = FirecrawlAdapter()
        result = await adapter.scrape("http://example.com")
        
        assert result.content == "Success"
        assert mock_client.scrape.call_count == 2

@pytest.mark.asyncio
async def test_rate_limit_handling():
    """æµ‹è¯•é€Ÿç‡é™åˆ¶å¤„ç†"""
    with patch('firecrawl.AsyncFirecrawl') as mock_client:
        mock_client.scrape = AsyncMock()
        mock_client.scrape.side_effect = [
            {"status_code": 429},
            {"content": "Success after retry"}
        ]
        
        adapter = FirecrawlAdapter()
        result = await adapter.scrape("http://example.com")
        
        assert "Success after retry" in result.content
```

## ğŸ¯ å…³é”®é›†æˆå»ºè®®

### ç«‹å³å®æ–½
1. **åŸºç¡€é›†æˆ**: å®ç°FirecrawlAdapterå’ŒåŸºæœ¬scrapeåŠŸèƒ½
2. **é”™è¯¯å¤„ç†**: å®Œå–„é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶
3. **ç¼“å­˜ç­–ç•¥**: å®ç°Redisç¼“å­˜å‡å°‘APIè°ƒç”¨

### çŸ­æœŸä¼˜åŒ–
1. **æ‰¹é‡å¤„ç†**: å®ç°Celeryä»»åŠ¡é˜Ÿåˆ—æ‰¹é‡çˆ¬å–
2. **ç›‘æ§æŒ‡æ ‡**: é›†æˆPrometheusç›‘æ§
3. **æ•°æ®æå–**: åˆ©ç”¨extractç«¯ç‚¹æå–ç»“æ„åŒ–æ•°æ®

### é•¿æœŸå¢å¼º
1. **æ™ºèƒ½è°ƒåº¦**: åŸºäºå†…å®¹æ›´æ–°é¢‘ç‡çš„æ™ºèƒ½çˆ¬å–è°ƒåº¦
2. **åˆ†å¸ƒå¼çˆ¬å–**: å¤šAPIå¯†é’¥è´Ÿè½½å‡è¡¡
3. **MLå¢å¼º**: åŸºäºå†å²æ•°æ®çš„çˆ¬å–ç­–ç•¥ä¼˜åŒ–

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

- **çˆ¬å–æ•ˆç‡æå‡**: 80%ï¼ˆé€šè¿‡å¹¶è¡Œå’Œç¼“å­˜ï¼‰
- **æ•°æ®è´¨é‡æå‡**: 90%ï¼ˆç»“æ„åŒ–æå–ï¼‰
- **æˆæœ¬é™ä½**: 60%ï¼ˆæ™ºèƒ½ç¼“å­˜å’Œé€‰æ‹©æ€§çˆ¬å–ï¼‰
- **ç³»ç»Ÿå¯é æ€§**: 99.9%ï¼ˆé‡è¯•å’Œå®¹é”™æœºåˆ¶ï¼‰

## ç»“è®º

Firecrawlä¸å…³å±±æ™ºèƒ½ç³»ç»Ÿçš„é›†æˆå°†æ˜¾è‘—æå‡æƒ…æŠ¥æ”¶é›†èƒ½åŠ›ã€‚é€šè¿‡å…­è¾¹å½¢æ¶æ„çš„æ¸…æ™°åˆ†å±‚å’Œå¼‚æ­¥å¤„ç†æœºåˆ¶ï¼Œç³»ç»Ÿå¯ä»¥é«˜æ•ˆã€å¯é åœ°å¤„ç†å¤§è§„æ¨¡ç½‘ç»œæ•°æ®é‡‡é›†ä»»åŠ¡ï¼Œä¸ºRAGç®¡é“æä¾›é«˜è´¨é‡çš„æ•°æ®æºã€‚