# Map API URL è¿‡æ»¤æ–¹æ¡ˆè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-10
**ç›®çš„**: è§£å†³ Firecrawl Map API è¿”å›æ— ç”¨é“¾æ¥çš„é—®é¢˜

---

## é—®é¢˜èƒŒæ™¯

### æ ¸å¿ƒé—®é¢˜

Firecrawl Map API åœ¨å‘ç°ç½‘ç«™URLç»“æ„æ—¶ï¼Œä¼šè¿”å›**å¤§é‡æ— ç”¨é“¾æ¥**ï¼ŒåŒ…æ‹¬ï¼š

1. **åŠŸèƒ½æ€§é¡µé¢**: ç™»å½•ã€æ³¨å†Œã€å…³äºæˆ‘ä»¬ã€è”ç³»æ–¹å¼
2. **éå†…å®¹æ–‡ä»¶**: PDFã€å›¾ç‰‡ã€å‹ç¼©åŒ…ã€é…ç½®æ–‡ä»¶
3. **é‡å¤æ¨¡å¼**: åˆ†é¡µé“¾æ¥ã€å‚æ•°å˜ä½“ï¼ˆ`?page=1`, `?page=2`ï¼‰
4. **ç³»ç»Ÿé¡µé¢**: æœç´¢é¡µã€åˆ†ç±»é¡µã€æ ‡ç­¾é¡µã€å¯¼èˆªé¡µ
5. **å¤–éƒ¨é“¾æ¥**: è·³è½¬åˆ°å…¶ä»–åŸŸåçš„é“¾æ¥

### å½±å“åˆ†æ

**æˆæœ¬å½±å“**:
- Map API: å›ºå®š 1 credit
- Scrape API: æ¯ä¸ªURL 1 credit
- **ç¤ºä¾‹**: 1000ä¸ªURL â†’ 40%æ— ç”¨ â†’ æµªè´¹ 400 credits

**æ•ˆç‡å½±å“**:
- çˆ¬å–æ— ç”¨é¡µé¢æµªè´¹æ—¶é—´
- å¢åŠ AIå¤„ç†è´Ÿæ‹…
- é™ä½ç»“æœç›¸å…³æ€§

**ç”¨æˆ·ä½“éªŒå½±å“**:
- æœç´¢ç»“æœä¸­æ··å…¥å¤§é‡æ— å…³å†…å®¹
- éœ€è¦æ‰‹åŠ¨ç­›é€‰
- é™ä½äº§å“ä»·å€¼

---

## è§£å†³æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆçŸ©é˜µ

| æ–¹æ¡ˆ | è¿‡æ»¤ç‡ | å®æ–½æˆæœ¬ | çµæ´»æ€§ | æ¨èåº¦ |
|------|--------|----------|--------|--------|
| **æ–¹æ¡ˆ1: Map API searchå‚æ•°** | 30-50% | â­ æä½ | â­â­ ä½ | â­â­â­ |
| **æ–¹æ¡ˆ2: URLæ¨¡å¼è¿‡æ»¤** | 60-80% | â­â­ ä½ | â­â­â­ ä¸­ | â­â­â­â­â­ |
| **æ–¹æ¡ˆ3: é…ç½®åŒ–è¿‡æ»¤è§„åˆ™** | 70-90% | â­â­â­ ä¸­ | â­â­â­â­â­ é«˜ | â­â­â­â­ |
| **æ–¹æ¡ˆ4: åŸºäºåŸŸåçš„è¿‡æ»¤** | 20-40% | â­ æä½ | â­â­ ä½ | â­â­â­ |

### æ¨èæ–¹æ¡ˆ

**ç«‹å³å®æ–½**: **æ–¹æ¡ˆ2ï¼ˆURLæ¨¡å¼è¿‡æ»¤ï¼‰** + æ–¹æ¡ˆ4ï¼ˆåŸŸåè¿‡æ»¤ï¼‰

**é€‰æ‹©ç†ç”±**:
- âœ… 1-2å°æ—¶å¿«é€Ÿå®ç°
- âœ… 60-80% æ— ç”¨é“¾æ¥è¿‡æ»¤ç‡
- âœ… ä¸å½±å“ç°æœ‰åŠŸèƒ½
- âœ… å¯åç»­æ‰©å±•ä¸ºé…ç½®åŒ–

---

## æ–¹æ¡ˆ2: URLæ¨¡å¼è¿‡æ»¤è¯¦ç»†è®¾è®¡

### æ¶æ„è®¾è®¡

```
Map API è¿”å›URLs (1000ä¸ª)
    â†“
[æ­¥éª¤1: URLè§„èŒƒåŒ–]
  - ç§»é™¤fragment (#section)
  - ç»Ÿä¸€å°¾éƒ¨æ–œæ 
  - URL decode
    â†“ (995ä¸ª)
[æ­¥éª¤2: è·¯å¾„å…³é”®è¯è¿‡æ»¤]
  - é»‘åå•åŒ¹é…: login, about, contactç­‰
    â†“ (850ä¸ª, -145)
[æ­¥éª¤3: æ–‡ä»¶ç±»å‹è¿‡æ»¤]
  - æ‰©å±•åæ£€æŸ¥: .pdf, .jpg, .zipç­‰
    â†“ (780ä¸ª, -70)
[æ­¥éª¤4: åŸŸåèŒƒå›´è¿‡æ»¤]
  - æ’é™¤å¤–éƒ¨åŸŸå
    â†“ (720ä¸ª, -60)
[æ­¥éª¤5: URLå»é‡ä¼˜åŒ–]
  - å‚æ•°ç®€åŒ–ã€è·Ÿè¸ªå‚æ•°ç§»é™¤
    â†“ (650ä¸ª, -70)
è¿‡æ»¤åçš„URLs â†’ Scrape API
```

**æ€»è¿‡æ»¤ç‡**: 35% (1000 â†’ 650)

---

## æ¨¡å—åŒ–æ¶æ„è®¾è®¡

### è®¾è®¡åŸåˆ™

**1. å•ä¸€èŒè´£åŸåˆ™ (SRP)**
- æ¯ä¸ªè¿‡æ»¤å™¨åªè´Ÿè´£ä¸€ç§è¿‡æ»¤é€»è¾‘
- URLè§„èŒƒåŒ–ç‹¬ç«‹äºè¿‡æ»¤é€»è¾‘
- ç»Ÿè®¡å’Œæ—¥å¿—ç‹¬ç«‹äºä¸šåŠ¡é€»è¾‘

**2. å¼€æ”¾å°é—­åŸåˆ™ (OCP)**
- å¯¹æ‰©å±•å¼€æ”¾ï¼šæ–°å¢è¿‡æ»¤å™¨æ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç 
- å¯¹ä¿®æ”¹å°é—­ï¼šæ ¸å¿ƒè¿‡æ»¤æµç¨‹ç¨³å®šä¸å˜

**3. æ¥å£éš”ç¦»åŸåˆ™ (ISP)**
- å®šä¹‰æ¸…æ™°çš„è¿‡æ»¤å™¨æ¥å£
- æ¯ä¸ªè¿‡æ»¤å™¨åªä¾èµ–éœ€è¦çš„æ¥å£

**4. ä¾èµ–å€’ç½®åŸåˆ™ (DIP)**
- ä¾èµ–æŠ½è±¡ï¼ˆè¿‡æ»¤å™¨æ¥å£ï¼‰è€Œéå…·ä½“å®ç°
- é€šè¿‡ä¾èµ–æ³¨å…¥ç®¡ç†è¿‡æ»¤å™¨

---

### æ ¸å¿ƒæ¨¡å—åˆ’åˆ†

```
URLè¿‡æ»¤ç³»ç»Ÿ
â”œâ”€â”€ 1. è¿‡æ»¤å™¨æ¥å£å±‚ (URLFilter Interface)
â”‚   â””â”€â”€ å®šä¹‰ç»Ÿä¸€çš„è¿‡æ»¤å™¨æ¥å£
â”‚
â”œâ”€â”€ 2. è¿‡æ»¤å™¨å®ç°å±‚ (Filter Implementations)
â”‚   â”œâ”€â”€ URLNormalizer - URLè§„èŒƒåŒ–
â”‚   â”œâ”€â”€ PathKeywordFilter - è·¯å¾„å…³é”®è¯è¿‡æ»¤
â”‚   â”œâ”€â”€ FileTypeFilter - æ–‡ä»¶ç±»å‹è¿‡æ»¤
â”‚   â”œâ”€â”€ DomainFilter - åŸŸåèŒƒå›´è¿‡æ»¤
â”‚   â””â”€â”€ URLDeduplicator - URLå»é‡
â”‚
â”œâ”€â”€ 3. é…ç½®ç®¡ç†å±‚ (Configuration Management)
â”‚   â”œâ”€â”€ FilterConfig - è¿‡æ»¤å™¨é…ç½®åŸºç±»
â”‚   â”œâ”€â”€ PathKeywordConfig - è·¯å¾„å…³é”®è¯é…ç½®
â”‚   â”œâ”€â”€ FileTypeConfig - æ–‡ä»¶ç±»å‹é…ç½®
â”‚   â””â”€â”€ ConfigLoader - é…ç½®åŠ è½½å™¨
â”‚
â”œâ”€â”€ 4. è¿‡æ»¤å™¨ç®¡é“å±‚ (Filter Pipeline)
â”‚   â”œâ”€â”€ FilterChain - è¿‡æ»¤å™¨é“¾
â”‚   â”œâ”€â”€ FilterRegistry - è¿‡æ»¤å™¨æ³¨å†Œè¡¨
â”‚   â””â”€â”€ PipelineBuilder - ç®¡é“æ„å»ºå™¨
â”‚
â”œâ”€â”€ 5. ç»Ÿè®¡åˆ†æå±‚ (Statistics & Analytics)
â”‚   â”œâ”€â”€ FilterStatistics - è¿‡æ»¤ç»Ÿè®¡
â”‚   â””â”€â”€ FilterLogger - è¿‡æ»¤æ—¥å¿—
â”‚
â””â”€â”€ 6. é›†æˆé€‚é…å±‚ (Integration Adapter)
    â””â”€â”€ MapScrapeExecutoré›†æˆç‚¹
```

---

### æ¨¡å—æ¥å£è®¾è®¡

#### 1. è¿‡æ»¤å™¨æ¥å£ (URLFilter Interface)

```python
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

@dataclass
class FilterContext:
    """è¿‡æ»¤ä¸Šä¸‹æ–‡ - ä¼ é€’è¿‡æ»¤æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    base_url: str  # åŸºç¡€URLï¼ˆç”¨äºåŸŸåè¿‡æ»¤ï¼‰
    task_id: str   # ä»»åŠ¡ID
    config: Dict[str, Any]  # é…ç½®ä¿¡æ¯

class URLFilter(ABC):
    """URLè¿‡æ»¤å™¨æŠ½è±¡åŸºç±»

    æ‰€æœ‰è¿‡æ»¤å™¨å¿…é¡»å®ç°æ­¤æ¥å£ï¼Œç¡®ä¿ç»Ÿä¸€çš„è¿‡æ»¤è¡Œä¸º
    """

    @abstractmethod
    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        """æ‰§è¡Œè¿‡æ»¤

        Args:
            urls: å¾…è¿‡æ»¤çš„URLåˆ—è¡¨
            context: è¿‡æ»¤ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            List[str]: è¿‡æ»¤åçš„URLåˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_filter_name(self) -> str:
        """è·å–è¿‡æ»¤å™¨åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œç»Ÿè®¡ï¼‰"""
        pass

    @property
    def enabled(self) -> bool:
        """è¿‡æ»¤å™¨æ˜¯å¦å¯ç”¨"""
        return True
```

**è®¾è®¡è¦ç‚¹**:
- âœ… ç»Ÿä¸€çš„æ¥å£å®šä¹‰
- âœ… ä¸Šä¸‹æ–‡ä¼ é€’æ”¯æŒ
- âœ… å¯ç”¨/ç¦ç”¨æ§åˆ¶
- âœ… ä¾¿äºæµ‹è¯•å’Œæ‰©å±•

---

#### 2. è¿‡æ»¤å™¨å®ç°ç¤ºä¾‹

**PathKeywordFilter - è·¯å¾„å…³é”®è¯è¿‡æ»¤å™¨**

```python
from typing import List, Optional, Set
from .interface import URLFilter, FilterContext
from urllib.parse import urlparse

class PathKeywordFilter(URLFilter):
    """è·¯å¾„å…³é”®è¯è¿‡æ»¤å™¨

    æ ¹æ®é»‘åå•å…³é”®è¯è¿‡æ»¤URLè·¯å¾„
    """

    def __init__(self, blacklist: Optional[List[str]] = None, enabled: bool = True):
        """åˆå§‹åŒ–

        Args:
            blacklist: é»‘åå•å…³é”®è¯åˆ—è¡¨
            enabled: æ˜¯å¦å¯ç”¨
        """
        self._blacklist: Set[str] = set(blacklist or self._get_default_blacklist())
        self._enabled = enabled

    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        """æ‰§è¡Œè·¯å¾„å…³é”®è¯è¿‡æ»¤"""
        if not self._enabled:
            return urls

        filtered = []
        for url in urls:
            path = urlparse(url).path.lower()
            # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
            if not any(keyword in path for keyword in self._blacklist):
                filtered.append(url)

        return filtered

    def get_filter_name(self) -> str:
        return "PathKeywordFilter"

    @property
    def enabled(self) -> bool:
        return self._enabled

    @staticmethod
    def _get_default_blacklist() -> List[str]:
        """è·å–é»˜è®¤é»‘åå•"""
        return [
            'login', 'register', 'about', 'contact',
            'privacy', 'terms', 'search', 'category', 'tag'
        ]

    def add_keyword(self, keyword: str) -> None:
        """åŠ¨æ€æ·»åŠ å…³é”®è¯"""
        self._blacklist.add(keyword.lower())

    def remove_keyword(self, keyword: str) -> None:
        """åŠ¨æ€ç§»é™¤å…³é”®è¯"""
        self._blacklist.discard(keyword.lower())
```

**æ¨¡å—åŒ–ä¼˜åŠ¿**:
- âœ… ç‹¬ç«‹å¯æµ‹è¯•
- âœ… å¯é…ç½®åŒ–
- âœ… æ”¯æŒåŠ¨æ€è°ƒæ•´
- âœ… æ¸…æ™°çš„èŒè´£è¾¹ç•Œ

---

#### 3. è¿‡æ»¤å™¨ç®¡é“ (Filter Pipeline)

```python
from typing import List, Optional, Dict, Any
from .interface import URLFilter, FilterContext

class FilterChain:
    """è¿‡æ»¤å™¨é“¾ - è´£ä»»é“¾æ¨¡å¼

    æŒ‰é¡ºåºæ‰§è¡Œå¤šä¸ªè¿‡æ»¤å™¨ï¼Œæ”¯æŒç»Ÿè®¡å’Œæ—¥å¿—
    """

    def __init__(self):
        self._filters: List[URLFilter] = []
        self._statistics: Dict[str, Dict[str, int]] = {}

    def add_filter(self, filter: URLFilter) -> 'FilterChain':
        """æ·»åŠ è¿‡æ»¤å™¨ï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰"""
        self._filters.append(filter)
        return self

    def execute(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        """æ‰§è¡Œè¿‡æ»¤å™¨é“¾

        Args:
            urls: åŸå§‹URLåˆ—è¡¨
            context: è¿‡æ»¤ä¸Šä¸‹æ–‡

        Returns:
            List[str]: è¿‡æ»¤åçš„URLåˆ—è¡¨
        """
        current_urls = urls
        self._statistics = {}

        for filter in self._filters:
            if not filter.enabled:
                continue

            before_count = len(current_urls)
            current_urls = filter.filter(current_urls, context)
            after_count = len(current_urls)

            # è®°å½•ç»Ÿè®¡
            self._statistics[filter.get_filter_name()] = {
                "before": before_count,
                "after": after_count,
                "filtered": before_count - after_count
            }

        return current_urls

    def get_statistics(self) -> Dict[str, Dict[str, int]]:
        """è·å–è¿‡æ»¤ç»Ÿè®¡"""
        return self._statistics

    def clear(self) -> None:
        """æ¸…ç©ºè¿‡æ»¤å™¨é“¾"""
        self._filters.clear()
        self._statistics.clear()
```

**è®¾è®¡æ¨¡å¼åº”ç”¨**:
- âœ… **è´£ä»»é“¾æ¨¡å¼**: å¤šä¸ªè¿‡æ»¤å™¨ä¸²è”
- âœ… **æµå¼API**: æ”¯æŒé“¾å¼è°ƒç”¨
- âœ… **ç»Ÿè®¡æ”¶é›†**: è‡ªåŠ¨è®°å½•è¿‡æ»¤æ•ˆæœ

---

#### 4. è¿‡æ»¤å™¨æ³¨å†Œè¡¨ (Filter Registry)

```python
from typing import Dict, Type, Optional
from .interface import URLFilter

class FilterRegistry:
    """è¿‡æ»¤å™¨æ³¨å†Œè¡¨ - å•ä¾‹æ¨¡å¼

    ç®¡ç†æ‰€æœ‰å¯ç”¨çš„è¿‡æ»¤å™¨ç±»å‹
    """

    _instance: Optional['FilterRegistry'] = None
    _filters: Dict[str, Type[URLFilter]] = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def register(cls, name: str, filter_class: Type[URLFilter]) -> None:
        """æ³¨å†Œè¿‡æ»¤å™¨

        Args:
            name: è¿‡æ»¤å™¨åç§°
            filter_class: è¿‡æ»¤å™¨ç±»
        """
        cls._filters[name] = filter_class

    @classmethod
    def create(cls, name: str, **kwargs) -> URLFilter:
        """åˆ›å»ºè¿‡æ»¤å™¨å®ä¾‹

        Args:
            name: è¿‡æ»¤å™¨åç§°
            **kwargs: è¿‡æ»¤å™¨åˆå§‹åŒ–å‚æ•°

        Returns:
            URLFilter: è¿‡æ»¤å™¨å®ä¾‹
        """
        if name not in cls._filters:
            raise ValueError(f"Unknown filter: {name}")

        return cls._filters[name](**kwargs)

    @classmethod
    def list_filters(cls) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„è¿‡æ»¤å™¨"""
        return list(cls._filters.keys())
```

**è®¾è®¡æ¨¡å¼åº”ç”¨**:
- âœ… **å•ä¾‹æ¨¡å¼**: å…¨å±€å”¯ä¸€æ³¨å†Œè¡¨
- âœ… **å·¥å‚æ¨¡å¼**: åŠ¨æ€åˆ›å»ºè¿‡æ»¤å™¨
- âœ… **æ³¨å†Œæœºåˆ¶**: æ’ä»¶å¼æ‰©å±•

---

#### 5. ç®¡é“æ„å»ºå™¨ (Pipeline Builder)

```python
from typing import List, Dict, Any, Optional
from .interface import URLFilter, FilterContext
from .chain import FilterChain
from .registry import FilterRegistry

class PipelineBuilder:
    """ç®¡é“æ„å»ºå™¨ - å»ºé€ è€…æ¨¡å¼

    æ ¹æ®é…ç½®æ„å»ºè¿‡æ»¤å™¨ç®¡é“
    """

    def __init__(self):
        self._chain = FilterChain()
        self._registry = FilterRegistry()

    def add_normalizer(self) -> 'PipelineBuilder':
        """æ·»åŠ URLè§„èŒƒåŒ–å™¨"""
        filter = self._registry.create('normalizer')
        self._chain.add_filter(filter)
        return self

    def add_path_filter(self, blacklist: Optional[List[str]] = None) -> 'PipelineBuilder':
        """æ·»åŠ è·¯å¾„å…³é”®è¯è¿‡æ»¤å™¨"""
        filter = self._registry.create('path_keyword', blacklist=blacklist)
        self._chain.add_filter(filter)
        return self

    def add_file_type_filter(self, blacklist: Optional[List[str]] = None) -> 'PipelineBuilder':
        """æ·»åŠ æ–‡ä»¶ç±»å‹è¿‡æ»¤å™¨"""
        filter = self._registry.create('file_type', blacklist=blacklist)
        self._chain.add_filter(filter)
        return self

    def add_domain_filter(self, base_url: str) -> 'PipelineBuilder':
        """æ·»åŠ åŸŸåè¿‡æ»¤å™¨"""
        filter = self._registry.create('domain', base_url=base_url)
        self._chain.add_filter(filter)
        return self

    def add_deduplicator(self) -> 'PipelineBuilder':
        """æ·»åŠ URLå»é‡å™¨"""
        filter = self._registry.create('deduplicator')
        self._chain.add_filter(filter)
        return self

    def build(self) -> FilterChain:
        """æ„å»ºè¿‡æ»¤å™¨é“¾"""
        return self._chain

    @classmethod
    def build_default_pipeline(cls, base_url: str) -> FilterChain:
        """æ„å»ºé»˜è®¤è¿‡æ»¤ç®¡é“"""
        return (cls()
                .add_normalizer()
                .add_path_filter()
                .add_file_type_filter()
                .add_domain_filter(base_url)
                .add_deduplicator()
                .build())
```

**è®¾è®¡æ¨¡å¼åº”ç”¨**:
- âœ… **å»ºé€ è€…æ¨¡å¼**: æµç•…çš„æ„å»ºAPI
- âœ… **é¢„è®¾é…ç½®**: å¿«é€Ÿæ„å»ºé»˜è®¤ç®¡é“
- âœ… **çµæ´»ç»„åˆ**: è‡ªç”±ç»„åˆè¿‡æ»¤å™¨

---

### æ¨¡å—ä¾èµ–å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MapScrapeExecutor (é›†æˆå±‚)         â”‚
â”‚   è´Ÿè´£: è°ƒç”¨è¿‡æ»¤ç®¡é“ï¼Œé›†æˆåˆ°æ‰§è¡Œæµç¨‹     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ depends on
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PipelineBuilder (æ„å»ºå±‚)           â”‚
â”‚   è´Ÿè´£: æ„å»ºè¿‡æ»¤ç®¡é“ï¼Œç®¡ç†è¿‡æ»¤å™¨ç»„åˆ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ uses
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FilterChain (ç®¡é“å±‚)               â”‚
â”‚   è´Ÿè´£: æ‰§è¡Œè¿‡æ»¤å™¨é“¾ï¼Œæ”¶é›†ç»Ÿè®¡          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ contains
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      URLFilter Implementations          â”‚
â”‚   PathKeywordFilter, FileTypeFilter,    â”‚
â”‚   DomainFilter, URLDeduplicator...      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ depends on
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FilterConfig (é…ç½®å±‚)              â”‚
â”‚   è´Ÿè´£: æä¾›è¿‡æ»¤å™¨é…ç½®                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¾èµ–åŸåˆ™**:
- âœ… å•å‘ä¾èµ–ï¼šä¸Šå±‚ä¾èµ–ä¸‹å±‚
- âœ… æ¥å£ä¾èµ–ï¼šä¾èµ–æŠ½è±¡è€Œéå®ç°
- âœ… æ— å¾ªç¯ä¾èµ–ï¼šæ¸…æ™°çš„å±‚æ¬¡ç»“æ„

---

### æ‰©å±•æœºåˆ¶è®¾è®¡

#### 1. æ·»åŠ æ–°è¿‡æ»¤å™¨ï¼ˆ3æ­¥éª¤ï¼‰

**æ­¥éª¤1: å®ç°è¿‡æ»¤å™¨æ¥å£**

```python
class CustomFilter(URLFilter):
    """è‡ªå®šä¹‰è¿‡æ»¤å™¨"""

    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        # å®ç°è‡ªå®šä¹‰è¿‡æ»¤é€»è¾‘
        return filtered_urls

    def get_filter_name(self) -> str:
        return "CustomFilter"
```

**æ­¥éª¤2: æ³¨å†Œè¿‡æ»¤å™¨**

```python
# åœ¨æ¨¡å—åˆå§‹åŒ–æ—¶æ³¨å†Œ
FilterRegistry.register('custom', CustomFilter)
```

**æ­¥éª¤3: ä½¿ç”¨è¿‡æ»¤å™¨**

```python
# æ–¹å¼1: é€šè¿‡PipelineBuilder
pipeline = (PipelineBuilder()
            .add_normalizer()
            .add_path_filter()
            .add_filter('custom')  # æ·»åŠ è‡ªå®šä¹‰è¿‡æ»¤å™¨
            .build())

# æ–¹å¼2: ç›´æ¥æ·»åŠ åˆ°FilterChain
chain = FilterChain()
chain.add_filter(CustomFilter())
```

**æ‰©å±•æˆæœ¬**: âœ… æä½ï¼ˆæ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç ï¼‰

---

#### 2. é…ç½®åŒ–æ‰©å±•

**åœºæ™¯**: ä¸åŒç½‘ç«™ä½¿ç”¨ä¸åŒçš„è¿‡æ»¤è§„åˆ™

```python
# é…ç½®æ–‡ä»¶: filter_presets.yaml
presets:
  news_site:
    normalizer:
      enabled: true
    path_keyword:
      enabled: true
      blacklist: ['login', 'register', 'subscribe']
    file_type:
      enabled: true
      blacklist: ['.pdf', '.jpg']

  blog_site:
    normalizer:
      enabled: true
    path_keyword:
      enabled: true
      blacklist: ['login', 'register', 'author']

# Pythonä»£ç 
class ConfigurablePipelineBuilder:
    """å¯é…ç½®çš„ç®¡é“æ„å»ºå™¨"""

    @classmethod
    def build_from_config(cls, preset_name: str) -> FilterChain:
        """ä»é…ç½®æ„å»ºç®¡é“"""
        config = load_config(preset_name)
        builder = PipelineBuilder()

        for filter_name, filter_config in config['filters'].items():
            if filter_config.get('enabled', True):
                builder.add_filter(filter_name, **filter_config)

        return builder.build()

# ä½¿ç”¨
pipeline = ConfigurablePipelineBuilder.build_from_config('news_site')
```

**æ‰©å±•ä¼˜åŠ¿**:
- âœ… é…ç½®é©±åŠ¨
- âœ… æ— éœ€ç¼–è¯‘
- âœ… å¿«é€Ÿåˆ‡æ¢

---

#### 3. æ’ä»¶æœºåˆ¶

**åœºæ™¯**: ç¬¬ä¸‰æ–¹å¼€å‘è€…æ·»åŠ è‡ªå®šä¹‰è¿‡æ»¤å™¨

```python
# æ’ä»¶ç›®å½•ç»“æ„
plugins/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ spam_filter.py  # åƒåœ¾URLè¿‡æ»¤å™¨
â””â”€â”€ seo_filter.py   # SEOä¼˜åŒ–URLè¿‡æ»¤å™¨

# è‡ªåŠ¨åŠ è½½æ’ä»¶
class PluginLoader:
    """æ’ä»¶åŠ è½½å™¨"""

    @staticmethod
    def load_plugins(plugin_dir: str):
        """è‡ªåŠ¨åŠ è½½æ’ä»¶ç›®å½•ä¸­çš„æ‰€æœ‰è¿‡æ»¤å™¨"""
        for file in os.listdir(plugin_dir):
            if file.endswith('.py') and file != '__init__.py':
                module = importlib.import_module(f'plugins.{file[:-3]}')

                # è‡ªåŠ¨æ³¨å†Œå®ç°äº†URLFilterçš„ç±»
                for name, obj in inspect.getmembers(module, inspect.isclass):
                    if issubclass(obj, URLFilter) and obj != URLFilter:
                        FilterRegistry.register(name.lower(), obj)

# ä½¿ç”¨
PluginLoader.load_plugins('plugins')
```

**æ’ä»¶å¼€å‘**:
```python
# plugins/spam_filter.py
class SpamFilter(URLFilter):
    """åƒåœ¾URLè¿‡æ»¤å™¨"""

    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        # ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¯†åˆ«åƒåœ¾URL
        return [url for url in urls if not self._is_spam(url)]

    def _is_spam(self, url: str) -> bool:
        # åƒåœ¾URLè¯†åˆ«é€»è¾‘
        pass
```

**æ’ä»¶ä¼˜åŠ¿**:
- âœ… çƒ­æ’æ‹”
- âœ… ç¬¬ä¸‰æ–¹æ‰©å±•
- âœ… ä¸ä¾µå…¥æ ¸å¿ƒä»£ç 

---

### é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

#### MapScrapeExecutoré›†æˆç‚¹

```python
class MapScrapeExecutor(TaskExecutor):
    """Map + Scrape ç»„åˆä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(self):
        super().__init__()
        self.adapter = FirecrawlAdapter()
        self.result_repo = SearchResultRepository()

        # ğŸ†• åˆå§‹åŒ–è¿‡æ»¤ç®¡é“
        self._filter_pipeline: Optional[FilterChain] = None

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡Œ Map + Scrape ä»»åŠ¡"""
        # ... ç°æœ‰ä»£ç  ...

        # 3. æ‰§è¡Œ Map API å‘ç° URL
        discovered_urls = await self._execute_map(task.crawl_url, config)

        # ğŸ†• 3.1 URLè¿‡æ»¤ï¼ˆæ¨¡å—åŒ–å®ç°ï¼‰
        discovered_urls = await self._filter_urls(discovered_urls, task, config)

        # 4. æ‰¹é‡ Scrape è·å–å†…å®¹
        scrape_results = await self._batch_scrape(discovered_urls, config)

        # ... ç°æœ‰ä»£ç  ...

    async def _filter_urls(
        self,
        urls: List[str],
        task: SearchTask,
        config: MapScrapeConfig
    ) -> List[str]:
        """URLè¿‡æ»¤ï¼ˆæ¨¡å—åŒ–å®ç°ï¼‰

        ä½¿ç”¨è¿‡æ»¤ç®¡é“æ‰§è¡Œå¤šå±‚è¿‡æ»¤
        """
        self.logger.info(f"ğŸ” å¼€å§‹URLè¿‡æ»¤: {len(urls)} ä¸ªåŸå§‹é“¾æ¥")

        # æ„å»ºè¿‡æ»¤ç®¡é“
        pipeline = PipelineBuilder.build_default_pipeline(task.crawl_url)

        # åˆ›å»ºè¿‡æ»¤ä¸Šä¸‹æ–‡
        context = FilterContext(
            base_url=task.crawl_url,
            task_id=str(task.id),
            config=config.to_dict()
        )

        # æ‰§è¡Œè¿‡æ»¤
        filtered_urls = pipeline.execute(urls, context)

        # è¾“å‡ºç»Ÿè®¡
        stats = pipeline.get_statistics()
        self._log_filter_statistics(stats, len(urls), len(filtered_urls))

        return filtered_urls

    def _log_filter_statistics(
        self,
        stats: Dict[str, Dict[str, int]],
        original_count: int,
        final_count: int
    ):
        """è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡æ—¥å¿—"""
        self.logger.info(f"ğŸ“Š URLè¿‡æ»¤ç»Ÿè®¡:")

        for filter_name, stat in stats.items():
            self.logger.info(
                f"  â”œâ”€ {filter_name}: {stat['before']} â†’ {stat['after']} "
                f"(-{stat['filtered']})"
            )

        filter_rate = (original_count - final_count) / original_count * 100
        self.logger.info(
            f"âœ… è¿‡æ»¤å®Œæˆ: ä¿ç•™ {final_count} ä¸ªæœ‰æ•ˆé“¾æ¥ "
            f"(è¿‡æ»¤ç‡: {filter_rate:.1f}%)"
        )
```

**é›†æˆä¼˜åŠ¿**:
- âœ… æœ€å°ä¾µå…¥ï¼šåªä¿®æ”¹ä¸€ä¸ªæ–¹æ³•
- âœ… å‘åå…¼å®¹ï¼šä¸å½±å“ç°æœ‰åŠŸèƒ½
- âœ… æ˜“äºæµ‹è¯•ï¼šå¯ç‹¬ç«‹æµ‹è¯•è¿‡æ»¤é€»è¾‘

---

### æ¨¡å—åŒ–ä¼˜åŠ¿æ€»ç»“

#### 1. å¼€å‘æ•ˆç‡

| åœºæ™¯ | ä¼ ç»Ÿæ–¹å¼ | æ¨¡å—åŒ–æ–¹å¼ | æ•ˆç‡æå‡ |
|------|---------|-----------|---------|
| æ·»åŠ æ–°è¿‡æ»¤å™¨ | ä¿®æ”¹æ‰§è¡Œå™¨ä»£ç  | å®ç°æ¥å£+æ³¨å†Œ | 60% |
| è°ƒæ•´è¿‡æ»¤é¡ºåº | é‡æ„ä»£ç  | è°ƒæ•´Builderé¡ºåº | 80% |
| è°ƒè¯•å•ä¸ªè¿‡æ»¤å™¨ | æ³¨é‡Šå…¶ä»–è¿‡æ»¤å™¨ | è®¾ç½®enabled=false | 90% |
| å•å…ƒæµ‹è¯• | ä¾èµ–å®Œæ•´ç¯å¢ƒ | ç‹¬ç«‹æµ‹è¯• | 70% |

#### 2. ä»£ç è´¨é‡

- âœ… **å¯è¯»æ€§**: æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œï¼Œæ˜“äºç†è§£
- âœ… **å¯ç»´æŠ¤æ€§**: ä¿®æ”¹å±€éƒ¨åŒ–ï¼Œå½±å“èŒƒå›´å°
- âœ… **å¯æµ‹è¯•æ€§**: æ¯ä¸ªæ¨¡å—å¯ç‹¬ç«‹æµ‹è¯•
- âœ… **å¯æ‰©å±•æ€§**: æ–°å¢åŠŸèƒ½æ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç 

#### 3. å›¢é˜Ÿåä½œ

- âœ… **å¹¶è¡Œå¼€å‘**: ä¸åŒå¼€å‘è€…å¼€å‘ä¸åŒè¿‡æ»¤å™¨
- âœ… **ä»£ç å¤ç”¨**: è¿‡æ»¤å™¨å¯åœ¨å…¶ä»–é¡¹ç›®ä¸­å¤ç”¨
- âœ… **èŒè´£æ¸…æ™°**: æ¯ä¸ªæ¨¡å—æœ‰æ˜ç¡®çš„owner

#### 4. é•¿æœŸæ¼”è¿›

- âœ… **æ’ä»¶åŒ–**: æ”¯æŒç¬¬ä¸‰æ–¹æ‰©å±•
- âœ… **é…ç½®åŒ–**: æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€é…ç½®
- âœ… **æ™ºèƒ½åŒ–**: æ˜“äºé›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹

---

## å®æ–½æ­¥éª¤è¯¦è§£

### æ­¥éª¤1: URLè§„èŒƒåŒ–é¢„å¤„ç†

**ç›®çš„**: ç»Ÿä¸€URLæ ¼å¼ï¼Œä¾¿äºåç»­åŒ¹é…

**å¤„ç†é€»è¾‘**:
1. **ç§»é™¤Fragment**: `https://example.com/page#section` â†’ `https://example.com/page`
2. **ç»Ÿä¸€å°¾éƒ¨æ–œæ **: `https://example.com/page/` â†’ `https://example.com/page`
3. **URL Decode**: `%E4%B8%AD%E6%96%87` â†’ è§£ç ä¸ºä¸­æ–‡
4. **è½¬å°å†™æ¯”è¾ƒ**: ä¿ç•™åŸURLç”¨äºçˆ¬å–ï¼Œè½¬å°å†™ç”¨äºåŒ¹é…

**å®ç°ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_normalize_url(url: str) -> str`

**ä¼ªä»£ç **:
```python
def _normalize_url(url):
    # ç§»é™¤fragment
    url = url.split('#')[0]

    # ç»Ÿä¸€å°¾éƒ¨æ–œæ 
    if url.endswith('/'):
        url = url[:-1]

    # URL decode
    url = urllib.parse.unquote(url)

    return url
```

---

### æ­¥éª¤2: è·¯å¾„å…³é”®è¯é»‘åå•è¿‡æ»¤

**ç›®çš„**: æ’é™¤åŠŸèƒ½æ€§é¡µé¢å’Œç³»ç»Ÿé¡µé¢

#### é»‘åå•è®¾è®¡ï¼ˆåˆ†ç±»ç®¡ç†ï¼‰

**A. ç”¨æˆ·åŠŸèƒ½ç±»** (ä¼˜å…ˆçº§: é«˜):
```
login, signin, sign-in, log-in
register, signup, sign-up
logout, signout, sign-out
forgot-password, reset-password, password-reset
account, my-account, profile, user
dashboard, admin, settings
```

**B. ç½‘ç«™ä¿¡æ¯ç±»** (ä¼˜å…ˆçº§: é«˜):
```
about, about-us, about-me
contact, contact-us, contact-me
privacy, privacy-policy, privacy-statement
terms, terms-of-service, terms-and-conditions
disclaimer, legal, cookies, cookie-policy
```

**C. å¯¼èˆªåŠŸèƒ½ç±»** (ä¼˜å…ˆçº§: ä¸­):
```
search, site-search, search-results
sitemap, site-map, html-sitemap
category, categories, cat
tag, tags, topics
archive, archives, calendar
```

**D. æŠ€æœ¯é¡µé¢ç±»** (ä¼˜å…ˆçº§: ä¸­):
```
rss, feed, atom
api, api-docs, swagger
admin, wp-admin, backend
wp-content, wp-includes, wp-json (WordPress)
static, assets, resources
```

**E. ç¤¾äº¤åŠŸèƒ½ç±»** (ä¼˜å…ˆçº§: ä½):
```
share, social, follow
subscribe, newsletter, subscription
comment, comments, feedback
```

#### åŒ¹é…ç­–ç•¥

**ç­–ç•¥1: è·¯å¾„æ®µå®Œæ•´åŒ¹é…**
- åŒ¹é…: `https://example.com/about` âœ…
- åŒ¹é…: `https://example.com/en/about` âœ…
- åŒ¹é…: `https://example.com/about/team` âœ…
- ä¸åŒ¹é…: `https://example.com/news/about-economy` âŒ

**ç­–ç•¥2: è·¯å¾„å…³é”®è¯åŒ…å«**
- åŒ¹é…ä»»ä½•è·¯å¾„æ®µåŒ…å«é»‘åå•å…³é”®è¯
- ç¤ºä¾‹: `/en/contact-us/form` â†’ åŒ…å« `contact` â†’ è¿‡æ»¤

**å®ç°ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_filter_by_path_keywords(urls: List[str]) -> List[str]`

**ä¼ªä»£ç **:
```python
def _filter_by_path_keywords(urls):
    PATH_BLACKLIST = [
        'login', 'register', 'about', 'contact',
        'privacy', 'terms', 'search', 'category', 'tag'
    ]

    filtered = []
    for url in urls:
        path = urlparse(url).path.lower()
        # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
        if not any(keyword in path for keyword in PATH_BLACKLIST):
            filtered.append(url)

    return filtered
```

---

### æ­¥éª¤3: æ–‡ä»¶ç±»å‹æ‰©å±•åè¿‡æ»¤

**ç›®çš„**: æ’é™¤éHTMLå†…å®¹æ–‡ä»¶

#### æ–‡ä»¶ç±»å‹é»‘åå•ï¼ˆæŒ‰ç±»åˆ«ï¼‰

**A. æ–‡æ¡£ç±»** (ä¼˜å…ˆçº§: é«˜):
```
.pdf, .doc, .docx, .xls, .xlsx, .ppt, .pptx
.txt, .rtf, .odt, .ods, .odp
```

**B. å›¾ç‰‡ç±»** (ä¼˜å…ˆçº§: é«˜):
```
.jpg, .jpeg, .png, .gif, .bmp, .svg, .webp
.ico, .tiff, .tif
```

**C. å‹ç¼©åŒ…ç±»** (ä¼˜å…ˆçº§: é«˜):
```
.zip, .rar, .7z, .tar, .gz, .bz2
.tar.gz, .tgz
```

**D. å¤šåª’ä½“ç±»** (ä¼˜å…ˆçº§: ä¸­):
```
.mp3, .mp4, .avi, .mov, .wmv, .flv
.wav, .m4a, .ogg, .webm
```

**E. æŠ€æœ¯æ–‡ä»¶ç±»** (ä¼˜å…ˆçº§: ä¸­):
```
.xml, .json, .csv, .yaml, .yml
.css, .js, .map, .min.js, .min.css
.rss, .atom, .feed
```

**F. å¯æ‰§è¡Œæ–‡ä»¶ç±»** (ä¼˜å…ˆçº§: é«˜):
```
.exe, .dmg, .pkg, .deb, .rpm
.apk, .ipa, .msi
```

#### åŒ¹é…ç­–ç•¥

**ç­–ç•¥1: URLæœ«å°¾æ‰©å±•ååŒ¹é…**
- æå–URLæœ€åçš„æ–‡ä»¶æ‰©å±•å
- å¤„ç†å¸¦å‚æ•°çš„URL: `file.pdf?download=1` â†’ æå– `.pdf`
- å¤§å°å†™ä¸æ•æ„Ÿ: `.PDF` = `.pdf`

**ç­–ç•¥2: å¤šçº§æ‰©å±•åæ”¯æŒ**
- æ”¯æŒ: `.tar.gz`, `.min.js`, `.bundle.css`

**å®ç°ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_filter_by_file_type(urls: List[str]) -> List[str]`

**ä¼ªä»£ç **:
```python
def _filter_by_file_type(urls):
    FILE_BLACKLIST = [
        '.pdf', '.jpg', '.png', '.zip',
        '.mp3', '.mp4', '.xml', '.css', '.js'
    ]

    filtered = []
    for url in urls:
        # æå–æ‰©å±•åï¼ˆç§»é™¤å‚æ•°ï¼‰
        path = urlparse(url).path.lower()
        # æ£€æŸ¥æ˜¯å¦ä»¥é»‘åå•æ‰©å±•åç»“å°¾
        if not any(path.endswith(ext) for ext in FILE_BLACKLIST):
            filtered.append(url)

    return filtered
```

---

### æ­¥éª¤4: åŸŸåèŒƒå›´é™åˆ¶è¿‡æ»¤

**ç›®çš„**: åªä¿ç•™ç›®æ ‡ç½‘ç«™åŸŸåçš„é“¾æ¥

#### åŸŸåæå–å’ŒåŒ¹é…

**åŸºç¡€åŸŸåæå–**:
```
è¾“å…¥ task.crawl_url: https://www.example.com/news
åŸºç¡€åŸŸå: www.example.com
```

**åŒ¹é…è§„åˆ™**:

**ä¿ç•™ï¼ˆåŒåŸŸåï¼‰**:
- `https://www.example.com/blog/post` âœ…
- `https://www.example.com/en/news` âœ…

**æ’é™¤ï¼ˆå¤–éƒ¨åŸŸåï¼‰**:
- `https://external.com/link` âŒ
- `https://facebook.com/share` âŒ

**å­åŸŸåå¤„ç†ï¼ˆå¯é€‰ï¼‰**:

**ä¸¥æ ¼æ¨¡å¼** (æ¨è):
- `www.example.com` â‰  `blog.example.com`
- åªä¿ç•™å®Œå…¨ç›¸åŒçš„åŸŸå

**å®½æ¾æ¨¡å¼**:
- `*.example.com` éƒ½ä¿ç•™
- é€‚ç”¨äºå¤šå­åŸŸåç½‘ç«™ï¼ˆå¦‚å¤§å‹åª’ä½“ï¼‰

#### ç‰¹æ®Šæƒ…å†µå¤„ç†

**CDNåŸŸå**:
- é—®é¢˜: `cdn.example.com` å¯èƒ½å­˜å‚¨é™æ€èµ„æº
- è§£å†³: é…åˆæ–‡ä»¶ç±»å‹è¿‡æ»¤è‡ªåŠ¨æ’é™¤

**å›½é™…åŒ–åŸŸå**:
- é—®é¢˜: `example.com` vs `example.cn`
- è§£å†³: ä¸¥æ ¼åŒ¹é…ï¼Œé¿å…è·¨ç«™çˆ¬å–

**å®ç°ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_filter_external_urls(urls: List[str], base_url: str) -> List[str]`

**ä¼ªä»£ç **:
```python
def _filter_external_urls(urls, base_url):
    base_domain = urlparse(base_url).netloc

    filtered = []
    for url in urls:
        url_domain = urlparse(url).netloc
        # ä¸¥æ ¼æ¨¡å¼ï¼šå®Œå…¨åŒ¹é…
        if url_domain == base_domain:
            filtered.append(url)

    return filtered
```

---

### æ­¥éª¤5: URLå»é‡ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

**ç›®çš„**: ç§»é™¤å‚æ•°å˜ä½“é€ æˆçš„é‡å¤é“¾æ¥

#### å»é‡ç­–ç•¥

**A. å‚æ•°ç®€åŒ–è¯†åˆ«**

**åœºæ™¯**: åˆ†é¡µé“¾æ¥
```
åŸå§‹:
  https://example.com/news?page=1
  https://example.com/news?page=2
  https://example.com/news?page=3
  ... (å…±20ä¸ª)

é—®é¢˜: å†…å®¹é«˜åº¦é‡å¤ï¼Œåªæ˜¯åˆ†é¡µä¸åŒ

è§£å†³: è¯†åˆ«ä¸ºåŒä¸€æ¨¡å¼ï¼Œä¿ç•™ç¬¬1é¡µ
  https://example.com/news?page=1
```

**B. è·Ÿè¸ªå‚æ•°ç§»é™¤**

**å¸¸è§è·Ÿè¸ªå‚æ•°**:
```
utm_source, utm_medium, utm_campaign, utm_content, utm_term
ref, source, from, via
fbclid, gclid, msclkid
_ga, _gid
```

**æ¸…ç†ç¤ºä¾‹**:
```
åŸå§‹: https://example.com/article?utm_source=twitter&fbclid=xxx
æ¸…ç†: https://example.com/article
```

**C. æœç´¢å‚æ•°è¯†åˆ«**

**åœºæ™¯**: æœç´¢ç»“æœé¡µ
```
https://example.com/search?q=keyword
https://example.com/search?q=another
```

**è§£å†³**: å®Œå…¨è·³è¿‡æœç´¢ç»“æœé¡µï¼ˆå†…å®¹åŠ¨æ€ç”Ÿæˆï¼Œä»·å€¼ä½ï¼‰

#### ä¿ç•™ç­–ç•¥

**åˆ†é¡µé“¾æ¥å¤„ç†**:
- **é€‰é¡¹1**: åªä¿ç•™ç¬¬ä¸€é¡µï¼ˆ`?page=1` æˆ–æ— å‚æ•°ï¼‰
- **é€‰é¡¹2**: åªä¿ç•™æœ€æ–°é¡µï¼ˆå¯¹æ–°é—»ç½‘ç«™ï¼‰
- **æ¨è**: é€‰é¡¹1ï¼ˆç¬¬ä¸€é¡µé€šå¸¸åŒ…å«æœ€é‡è¦å†…å®¹ï¼‰

**æ’åºå‚æ•°å¤„ç†**:
```
https://example.com/products?sort=price
https://example.com/products?sort=popular
```
- **è§£å†³**: ä¿ç•™é»˜è®¤æ’åºï¼ˆæ— å‚æ•°ç‰ˆæœ¬ï¼‰

**å®ç°ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_deduplicate_urls(urls: List[str]) -> List[str]`

**ä¼ªä»£ç **:
```python
def _deduplicate_urls(urls):
    TRACKING_PARAMS = [
        'utm_source', 'utm_medium', 'utm_campaign',
        'ref', 'source', 'fbclid', 'gclid'
    ]

    seen_paths = set()
    filtered = []

    for url in urls:
        parsed = urlparse(url)

        # ç§»é™¤è·Ÿè¸ªå‚æ•°
        params = parse_qs(parsed.query)
        clean_params = {
            k: v for k, v in params.items()
            if k not in TRACKING_PARAMS
        }

        # æ„å»ºæ¸…ç†åçš„URL
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if clean_params:
            clean_url += f"?{urlencode(clean_params, doseq=True)}"

        # å»é‡æ£€æŸ¥
        if clean_url not in seen_paths:
            seen_paths.add(clean_url)
            filtered.append(url)  # ä¿ç•™åŸå§‹URL

    return filtered
```

---

## æ‰§è¡Œæµç¨‹é›†æˆ

### å½“å‰æ‰§è¡Œæµç¨‹

**ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py` Line 142-173

```
Line 142: # 3. æ‰§è¡Œ Map API å‘ç° URL
Line 144: discovered_urls = await self._execute_map(task.crawl_url, config)

Line 146: if not discovered_urls:
Line 147:     return self._create_empty_batch(task)

Line 150: self.logger.info(f"âœ… å‘ç° {len(discovered_urls)} ä¸ªURL")

Line 152: # 3.5. URLå»é‡æ£€æŸ¥ï¼ˆv2.1.1ï¼‰
Line 153: if config.enable_dedup:
Line 154:     existing_urls = await self.result_repo.check_existing_urls(...)
Line 168:     discovered_urls = new_urls

Line 174: # 4. æ‰¹é‡ Scrape è·å–å†…å®¹
Line 176: scrape_results = await self._batch_scrape(discovered_urls, config)
```

### ä¿®æ”¹åæ‰§è¡Œæµç¨‹

**æ’å…¥ä½ç½®**: Line 150ä¹‹åï¼ŒLine 152ä¹‹å‰

```python
Line 142: # 3. æ‰§è¡Œ Map API å‘ç° URL
Line 144: discovered_urls = await self._execute_map(task.crawl_url, config)

Line 146: if not discovered_urls:
Line 147:     return self._create_empty_batch(task)

Line 150: self.logger.info(f"âœ… å‘ç° {len(discovered_urls)} ä¸ªURL")

# ğŸ†• 3.1 URLè¿‡æ»¤ï¼ˆå¤šå±‚æ¬¡ï¼‰
self.logger.info(f"ğŸ” å¼€å§‹URLè¿‡æ»¤: {len(discovered_urls)} ä¸ªåŸå§‹é“¾æ¥")

# æ­¥éª¤1: URLè§„èŒƒåŒ–
discovered_urls = [self._normalize_url(url) for url in discovered_urls]

# æ­¥éª¤2: è·¯å¾„å…³é”®è¯è¿‡æ»¤
before_path = len(discovered_urls)
discovered_urls = self._filter_by_path_keywords(discovered_urls)
self.logger.info(
    f"  â”œâ”€ è·¯å¾„å…³é”®è¯è¿‡æ»¤: {before_path} â†’ {len(discovered_urls)} "
    f"(-{before_path - len(discovered_urls)})"
)

# æ­¥éª¤3: æ–‡ä»¶ç±»å‹è¿‡æ»¤
before_file = len(discovered_urls)
discovered_urls = self._filter_by_file_type(discovered_urls)
self.logger.info(
    f"  â”œâ”€ æ–‡ä»¶ç±»å‹è¿‡æ»¤: {before_file} â†’ {len(discovered_urls)} "
    f"(-{before_file - len(discovered_urls)})"
)

# æ­¥éª¤4: åŸŸåèŒƒå›´è¿‡æ»¤
before_domain = len(discovered_urls)
discovered_urls = self._filter_external_urls(discovered_urls, task.crawl_url)
self.logger.info(
    f"  â”œâ”€ åŸŸåèŒƒå›´è¿‡æ»¤: {before_domain} â†’ {len(discovered_urls)} "
    f"(-{before_domain - len(discovered_urls)})"
)

# æ­¥éª¤5: URLå»é‡ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
before_dedup = len(discovered_urls)
discovered_urls = self._deduplicate_urls(discovered_urls)
self.logger.info(
    f"  â””â”€ URLå»é‡ä¼˜åŒ–: {before_dedup} â†’ {len(discovered_urls)} "
    f"(-{before_dedup - len(discovered_urls)})"
)

self.logger.info(f"âœ… è¿‡æ»¤å®Œæˆ: ä¿ç•™ {len(discovered_urls)} ä¸ªæœ‰æ•ˆé“¾æ¥")

if not discovered_urls:
    self.logger.warning(f"âš ï¸  è¿‡æ»¤åæ— æœ‰æ•ˆURL")
    return self._create_empty_batch(task)

Line 152: # 3.5. URLå»é‡æ£€æŸ¥ï¼ˆv2.1.1ï¼‰
Line 153: if config.enable_dedup:
...
```

---

## è¿‡æ»¤ç»Ÿè®¡å’Œæ—¥å¿—è®¾è®¡

### ç»Ÿè®¡ç»´åº¦

```python
è¿‡æ»¤ç»Ÿè®¡å­—å…¸ = {
    "åŸå§‹URLæ•°é‡": 1000,
    "è§„èŒƒåŒ–å": 995,           # -5 (æ ¼å¼é—®é¢˜URL)
    "è·¯å¾„å…³é”®è¯è¿‡æ»¤å": 850,   # -145 (åŠŸèƒ½é¡µé¢)
    "æ–‡ä»¶ç±»å‹è¿‡æ»¤å": 780,     # -70 (éHTMLæ–‡ä»¶)
    "åŸŸåè¿‡æ»¤å": 720,         # -60 (å¤–éƒ¨é“¾æ¥)
    "å»é‡ä¼˜åŒ–å": 650,         # -70 (å‚æ•°å˜ä½“)
    "æœ€ç»ˆä¿ç•™": 650,
    "æ€»è¿‡æ»¤æ•°": 350,
    "æ€»è¿‡æ»¤ç‡": "35%"
}
```

### æ—¥å¿—è¾“å‡ºæ ¼å¼

**æ ‡å‡†æ ¼å¼**:
```
ğŸ” å¼€å§‹URLè¿‡æ»¤: 1000ä¸ªåŸå§‹é“¾æ¥
  â”œâ”€ è·¯å¾„å…³é”®è¯è¿‡æ»¤: 1000 â†’ 850 (-145)
  â”œâ”€ æ–‡ä»¶ç±»å‹è¿‡æ»¤: 850 â†’ 780 (-70)
  â”œâ”€ åŸŸåèŒƒå›´è¿‡æ»¤: 780 â†’ 720 (-60)
  â””â”€ URLå»é‡ä¼˜åŒ–: 720 â†’ 650 (-70)
âœ… è¿‡æ»¤å®Œæˆ: ä¿ç•™650ä¸ªæœ‰æ•ˆé“¾æ¥ (è¿‡æ»¤ç‡: 35%)
```

**è¯¦ç»†æ¨¡å¼**ï¼ˆDebugçº§åˆ«ï¼‰:
```
ğŸ” URLè¿‡æ»¤è¯¦æƒ…:
  è·¯å¾„å…³é”®è¯è¿‡æ»¤:
    âŒ https://example.com/login (åŒ¹é…: login)
    âŒ https://example.com/about-us (åŒ¹é…: about)
    âŒ https://example.com/contact (åŒ¹é…: contact)
    ... (å…±145ä¸ª)

  æ–‡ä»¶ç±»å‹è¿‡æ»¤:
    âŒ https://example.com/report.pdf (ç±»å‹: .pdf)
    âŒ https://example.com/image.jpg (ç±»å‹: .jpg)
    ... (å…±70ä¸ª)

  åŸŸåèŒƒå›´è¿‡æ»¤:
    âŒ https://external.com/link (å¤–éƒ¨åŸŸå)
    ... (å…±60ä¸ª)
```

### å®ç°ä½ç½®

**æ—¥å¿—æ–¹æ³•**: `src/services/firecrawl/executors/map_scrape_executor.py`

**æ–°å¢æ–¹æ³•**: `_log_filter_statistics(stats: Dict[str, Any]) -> None`

---

## é»‘åå•é…ç½®ç®¡ç†

### æ–¹å¼1: ç¡¬ç¼–ç ï¼ˆå¿«é€Ÿå®ç°ï¼‰âœ… æ¨èé˜¶æ®µ1

**å®ç°æ–¹å¼**:
```python
class MapScrapeExecutor(TaskExecutor):
    def _filter_by_path_keywords(self, urls):
        PATH_BLACKLIST = [
            'login', 'register', 'about', 'contact',
            'privacy', 'terms', 'search', 'category'
        ]
        # è¿‡æ»¤é€»è¾‘...
```

**ä¼˜ç‚¹**:
- âœ… å®ç°ç®€å•ï¼Œç«‹å³å¯ç”¨
- âœ… æ— éœ€é¢å¤–é…ç½®æ–‡ä»¶
- âœ… æ€§èƒ½æœ€ä¼˜

**ç¼ºç‚¹**:
- âŒ ä¸çµæ´»ï¼Œä¿®æ”¹éœ€è¦é‡æ–°éƒ¨ç½²
- âŒ æ— æ³•é’ˆå¯¹ä¸åŒç½‘ç«™å®šåˆ¶

**é€‚ç”¨åœºæ™¯**: å¿«é€ŸéªŒè¯æ–¹æ¡ˆå¯è¡Œæ€§

---

### æ–¹å¼2: ç±»çº§åˆ«å¸¸é‡

**å®ç°æ–¹å¼**:
```python
class MapScrapeExecutor(TaskExecutor):
    # é»‘åå•é…ç½®ï¼ˆç±»çº§åˆ«å¸¸é‡ï¼‰
    PATH_BLACKLIST = [
        'login', 'register', 'about', 'contact',
        'privacy', 'terms', 'search', 'category', 'tag'
    ]

    FILE_BLACKLIST = [
        '.pdf', '.jpg', '.png', '.zip',
        '.mp3', '.mp4', '.xml', '.css', '.js'
    ]

    def _filter_by_path_keywords(self, urls):
        # ä½¿ç”¨ self.PATH_BLACKLIST
```

**ä¼˜ç‚¹**:
- âœ… é›†ä¸­ç®¡ç†ï¼Œä¾¿äºç»´æŠ¤
- âœ… å¯åœ¨å­ç±»ä¸­è¦†ç›–ï¼ˆæ‰©å±•æ€§ï¼‰
- âœ… æ¸…æ™°çš„ä»£ç ç»„ç»‡

**ç¼ºç‚¹**:
- âŒ ä»éœ€é‡æ–°éƒ¨ç½²ä¿®æ”¹
- âŒ ä¸æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°

**é€‚ç”¨åœºæ™¯**: é˜¶æ®µ1åˆ°é˜¶æ®µ2çš„è¿‡æ¸¡

---

### æ–¹å¼3: é…ç½®æ–‡ä»¶ï¼ˆæœªæ¥æ‰©å±•ï¼‰

**æ–‡ä»¶ç»“æ„**:
```
src/services/firecrawl/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ map_scrape_config.py
â”‚   â””â”€â”€ url_filter_config.py  # ğŸ†• æ–°å¢
```

**é…ç½®æ–‡ä»¶å†…å®¹** (`url_filter_config.py`):
```python
"""URLè¿‡æ»¤é…ç½®"""

# è·¯å¾„å…³é”®è¯é»‘åå•ï¼ˆåˆ†ç±»ï¼‰
PATH_BLACKLIST = {
    "user_functions": [
        'login', 'signin', 'register', 'signup',
        'logout', 'account', 'profile', 'dashboard'
    ],
    "site_info": [
        'about', 'contact', 'privacy', 'terms',
        'disclaimer', 'cookies'
    ],
    "navigation": [
        'search', 'sitemap', 'category', 'tag', 'archive'
    ],
    "technical": [
        'rss', 'feed', 'api', 'admin',
        'wp-admin', 'wp-content'
    ]
}

# æ–‡ä»¶ç±»å‹é»‘åå•ï¼ˆåˆ†ç±»ï¼‰
FILE_BLACKLIST = {
    "documents": ['.pdf', '.doc', '.docx', '.xls', '.xlsx'],
    "images": ['.jpg', '.jpeg', '.png', '.gif', '.svg'],
    "archives": ['.zip', '.rar', '.7z', '.tar', '.gz'],
    "media": ['.mp3', '.mp4', '.avi', '.mov'],
    "technical": ['.xml', '.json', '.css', '.js', '.rss']
}

# è·Ÿè¸ªå‚æ•°é»‘åå•
TRACKING_PARAMS = [
    'utm_source', 'utm_medium', 'utm_campaign',
    'ref', 'source', 'from', 'via',
    'fbclid', 'gclid', 'msclkid',
    '_ga', '_gid'
]

# é¢„è®¾è§„åˆ™æ¨¡æ¿ï¼ˆæŒ‰ç½‘ç«™ç±»å‹ï¼‰
PRESET_RULES = {
    "news_site": {
        "path_whitelist": ['/news/', '/article/', '/post/'],
        "additional_blacklist": ['subscription', 'paywall']
    },
    "blog_site": {
        "path_whitelist": ['/blog/', '/post/', '/entry/'],
        "additional_blacklist": []
    },
    "ecommerce": {
        "path_whitelist": ['/product/', '/shop/', '/item/'],
        "additional_blacklist": ['cart', 'checkout', 'wishlist']
    }
}
```

**ä½¿ç”¨æ–¹å¼**:
```python
from ..config.url_filter_config import PATH_BLACKLIST, FILE_BLACKLIST

class MapScrapeExecutor(TaskExecutor):
    def _filter_by_path_keywords(self, urls):
        # åˆå¹¶æ‰€æœ‰åˆ†ç±»çš„é»‘åå•
        blacklist = []
        for category, keywords in PATH_BLACKLIST.items():
            blacklist.extend(keywords)

        # è¿‡æ»¤é€»è¾‘...
```

**ä¼˜ç‚¹**:
- âœ… é›†ä¸­é…ç½®ç®¡ç†
- âœ… æ”¯æŒåˆ†ç±»å’Œæ³¨é‡Š
- âœ… æ˜“äºæ‰©å±•å’Œç»´æŠ¤
- âœ… å¯æŒ‰ç½‘ç«™ç±»å‹æä¾›é¢„è®¾

**ç¼ºç‚¹**:
- âŒ ä»éœ€é‡æ–°éƒ¨ç½²
- âŒ å¢åŠ æ–‡ä»¶å¤æ‚åº¦

**é€‚ç”¨åœºæ™¯**: é˜¶æ®µ2å®Œæˆåçš„ä¼˜åŒ–

---

### æ–¹å¼4: æ•°æ®åº“é…ç½®ï¼ˆé«˜çº§æ‰©å±•ï¼‰

**æ•°æ®åº“è¡¨è®¾è®¡**:
```sql
CREATE TABLE url_filter_rules (
    id SERIAL PRIMARY KEY,
    rule_type VARCHAR(50),  -- 'path_keyword', 'file_type', 'tracking_param'
    rule_value VARCHAR(200),
    category VARCHAR(50),
    priority INT,
    enabled BOOLEAN,
    created_at TIMESTAMP
);

-- ç¤ºä¾‹æ•°æ®
INSERT INTO url_filter_rules VALUES
(1, 'path_keyword', 'login', 'user_functions', 100, true, NOW()),
(2, 'path_keyword', 'about', 'site_info', 90, true, NOW()),
(3, 'file_type', '.pdf', 'documents', 100, true, NOW());
```

**ä¼˜ç‚¹**:
- âœ… æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°
- âœ… å¯é€šè¿‡ç®¡ç†ç•Œé¢é…ç½®
- âœ… æ”¯æŒA/Bæµ‹è¯•ä¸åŒè§„åˆ™
- âœ… è®°å½•è§„åˆ™å˜æ›´å†å²

**ç¼ºç‚¹**:
- âŒ å®ç°å¤æ‚åº¦é«˜
- âŒ å¢åŠ æ•°æ®åº“æŸ¥è¯¢å¼€é”€
- âŒ éœ€è¦ç¼“å­˜æœºåˆ¶

**é€‚ç”¨åœºæ™¯**: äº§å“æˆç†ŸæœŸï¼Œéœ€è¦é¢‘ç¹è°ƒä¼˜è§„åˆ™

---

## æµ‹è¯•éªŒè¯ç­–ç•¥

### å•å…ƒæµ‹è¯•è®¾è®¡

**æµ‹è¯•æ–‡ä»¶**: `tests/services/firecrawl/test_url_filter.py`

#### æµ‹è¯•ç”¨ä¾‹1: è·¯å¾„å…³é”®è¯è¿‡æ»¤

```python
def test_filter_by_path_keywords():
    """æµ‹è¯•è·¯å¾„å…³é”®è¯è¿‡æ»¤"""
    executor = MapScrapeExecutor()

    # è¾“å…¥URLs
    input_urls = [
        "https://example.com/news/article-1",     # ä¿ç•™
        "https://example.com/about-us",           # è¿‡æ»¤ (about)
        "https://example.com/blog/post",          # ä¿ç•™
        "https://example.com/login",              # è¿‡æ»¤ (login)
        "https://example.com/contact",            # è¿‡æ»¤ (contact)
        "https://example.com/en/privacy-policy",  # è¿‡æ»¤ (privacy)
    ]

    # æ‰§è¡Œè¿‡æ»¤
    result = executor._filter_by_path_keywords(input_urls)

    # éªŒè¯ç»“æœ
    assert len(result) == 2
    assert "https://example.com/news/article-1" in result
    assert "https://example.com/blog/post" in result
    assert "https://example.com/about-us" not in result
```

#### æµ‹è¯•ç”¨ä¾‹2: æ–‡ä»¶ç±»å‹è¿‡æ»¤

```python
def test_filter_by_file_type():
    """æµ‹è¯•æ–‡ä»¶ç±»å‹è¿‡æ»¤"""
    executor = MapScrapeExecutor()

    # è¾“å…¥URLs
    input_urls = [
        "https://example.com/report.pdf",         # è¿‡æ»¤ (.pdf)
        "https://example.com/image.jpg",          # è¿‡æ»¤ (.jpg)
        "https://example.com/article",            # ä¿ç•™
        "https://example.com/data.zip",           # è¿‡æ»¤ (.zip)
        "https://example.com/page.html",          # ä¿ç•™
        "https://example.com/doc.pdf?v=2",        # è¿‡æ»¤ (.pdf)
    ]

    # æ‰§è¡Œè¿‡æ»¤
    result = executor._filter_by_file_type(input_urls)

    # éªŒè¯ç»“æœ
    assert len(result) == 2
    assert "https://example.com/article" in result
    assert "https://example.com/page.html" in result
```

#### æµ‹è¯•ç”¨ä¾‹3: åŸŸåè¿‡æ»¤

```python
def test_filter_external_urls():
    """æµ‹è¯•å¤–éƒ¨åŸŸåè¿‡æ»¤"""
    executor = MapScrapeExecutor()

    base_url = "https://www.example.com/news"

    # è¾“å…¥URLs
    input_urls = [
        "https://www.example.com/article-1",      # ä¿ç•™ (åŒåŸŸå)
        "https://www.example.com/blog/post",      # ä¿ç•™ (åŒåŸŸå)
        "https://external.com/link",              # è¿‡æ»¤ (å¤–éƒ¨)
        "https://another.org/page",               # è¿‡æ»¤ (å¤–éƒ¨)
        "https://blog.example.com/post",          # è¿‡æ»¤ (å­åŸŸåä¸åŒ)
    ]

    # æ‰§è¡Œè¿‡æ»¤
    result = executor._filter_external_urls(input_urls, base_url)

    # éªŒè¯ç»“æœ
    assert len(result) == 2
    assert all("www.example.com" in url for url in result)
```

#### æµ‹è¯•ç”¨ä¾‹4: URLå»é‡

```python
def test_deduplicate_urls():
    """æµ‹è¯•URLå»é‡"""
    executor = MapScrapeExecutor()

    # è¾“å…¥URLsï¼ˆåŒ…å«è·Ÿè¸ªå‚æ•°å’Œé‡å¤ï¼‰
    input_urls = [
        "https://example.com/article",
        "https://example.com/article?utm_source=twitter",
        "https://example.com/article?ref=facebook",
        "https://example.com/news?page=1",
        "https://example.com/news?page=2",
    ]

    # æ‰§è¡Œå»é‡
    result = executor._deduplicate_urls(input_urls)

    # éªŒè¯ç»“æœ
    assert len(result) == 2  # article å’Œ news (ä¿ç•™page=1)
```

#### æµ‹è¯•ç”¨ä¾‹5: è¾¹ç•Œæƒ…å†µ

```python
def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    executor = MapScrapeExecutor()

    # ç©ºåˆ—è¡¨
    assert executor._filter_by_path_keywords([]) == []

    # ç‰¹æ®Šå­—ç¬¦URL
    urls_with_special = [
        "https://example.com/æ–‡ç« /æ–°é—»",
        "https://example.com/page?param=å€¼"
    ]
    result = executor._filter_by_path_keywords(urls_with_special)
    assert len(result) == 2

    # éå¸¸é•¿çš„URL
    long_url = "https://example.com/" + "a" * 1000
    result = executor._filter_by_path_keywords([long_url])
    assert len(result) == 1
```

---

### é›†æˆæµ‹è¯•è®¾è®¡

**æµ‹è¯•æ–‡ä»¶**: `tests/services/firecrawl/test_map_scrape_integration.py`

#### æµ‹è¯•ç”¨ä¾‹: å®Œæ•´è¿‡æ»¤æµç¨‹

```python
@pytest.mark.asyncio
async def test_complete_url_filtering():
    """æµ‹è¯•å®Œæ•´çš„URLè¿‡æ»¤æµç¨‹"""

    # æ¨¡æ‹ŸMap APIè¿”å›çš„URLs
    mock_map_urls = [
        # æœ‰æ•ˆURLs (åº”ä¿ç•™)
        "https://example.com/news/article-1",
        "https://example.com/news/article-2",
        "https://example.com/blog/post-1",

        # åŠŸèƒ½é¡µé¢ (åº”è¿‡æ»¤)
        "https://example.com/login",
        "https://example.com/about",
        "https://example.com/contact",

        # æ–‡ä»¶ç±»å‹ (åº”è¿‡æ»¤)
        "https://example.com/report.pdf",
        "https://example.com/image.jpg",

        # å¤–éƒ¨é“¾æ¥ (åº”è¿‡æ»¤)
        "https://external.com/link",

        # è·Ÿè¸ªå‚æ•° (åº”å»é‡)
        "https://example.com/news/article-1?utm_source=twitter",
    ]

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = SearchTask(
        id="test_task_123",
        task_type=TaskType.CRAWL_WEBSITE,
        crawl_url="https://example.com/news",
        crawl_config={}
    )

    # æ‰§è¡Œè¿‡æ»¤æµç¨‹
    executor = MapScrapeExecutor()
    # ... æ¨¡æ‹Ÿæ‰§è¡Œ ...

    # éªŒè¯ç»“æœ
    # æœŸæœ›ä¿ç•™3ä¸ªæœ‰æ•ˆURLs
    assert final_urls_count == 3
    assert "https://example.com/login" not in final_urls
```

---

### çœŸå®æ•°æ®æµ‹è¯•

**æµ‹è¯•æ­¥éª¤**:

1. **é€‰æ‹©æµ‹è¯•ç½‘ç«™**: é€‰æ‹©3-5ä¸ªä¸åŒç±»å‹çš„æ–°é—»ç½‘ç«™
   - å¤§å‹æ–°é—»é—¨æˆ·ï¼ˆå¦‚ BBC, CNNï¼‰
   - åœ°åŒºæ€§æ–°é—»ç½‘ç«™
   - ä¸“ä¸šé¢†åŸŸæ–°é—»ç½‘ç«™

2. **æ‰§è¡ŒMap API**: è·å–çœŸå®è¿”å›çš„URLs

3. **åº”ç”¨è¿‡æ»¤å™¨**: è®°å½•æ¯ä¸€æ­¥çš„è¿‡æ»¤æ•ˆæœ

4. **äººå·¥éªŒè¯**: æŠ½æ ·æ£€æŸ¥è¿‡æ»¤ç»“æœçš„å‡†ç¡®æ€§
   - è¯¯æ€ç‡ï¼ˆæœ‰æ•ˆURLè¢«é”™è¯¯è¿‡æ»¤ï¼‰
   - æ¼æ£€ç‡ï¼ˆæ— ç”¨URLæœªè¢«è¿‡æ»¤ï¼‰

5. **è°ƒä¼˜è§„åˆ™**: æ ¹æ®éªŒè¯ç»“æœè°ƒæ•´é»‘åå•

**æµ‹è¯•è®°å½•æ¨¡æ¿**:
```
ç½‘ç«™: https://example-news.com
Map APIè¿”å›: 856ä¸ªURL

è¿‡æ»¤ç»“æœ:
â”œâ”€ è·¯å¾„å…³é”®è¯è¿‡æ»¤: 856 â†’ 720 (-136)
â”‚   â””â”€ è¿‡æ»¤ç¤ºä¾‹: /login, /about, /subscribe
â”œâ”€ æ–‡ä»¶ç±»å‹è¿‡æ»¤: 720 â†’ 680 (-40)
â”‚   â””â”€ è¿‡æ»¤ç¤ºä¾‹: report.pdf, logo.png
â”œâ”€ åŸŸåè¿‡æ»¤: 680 â†’ 650 (-30)
â”‚   â””â”€ è¿‡æ»¤ç¤ºä¾‹: facebook.com/share, twitter.com
â””â”€ å»é‡ä¼˜åŒ–: 650 â†’ 580 (-70)
    â””â”€ è¿‡æ»¤ç¤ºä¾‹: ?utm_source=xxx, ?page=2-10

æœ€ç»ˆä¿ç•™: 580ä¸ªURL (è¿‡æ»¤ç‡: 32%)

äººå·¥æŠ½æ ·éªŒè¯ (50ä¸ªURL):
â”œâ”€ æœ‰æ•ˆURL: 48ä¸ª (96%)
â”œâ”€ è¯¯æ€: 1ä¸ª (2%) - /news-about-economy (åŒ…å«about)
â””â”€ æ¼æ£€: 1ä¸ª (2%) - /newsletter (åº”è¯¥è¿‡æ»¤)

å»ºè®®è°ƒæ•´:
1. æ”¹è¿›"about"åŒ¹é…è§„åˆ™ï¼Œé¿å…è¯¯æ€"/news-about-xxx"
2. æ·»åŠ "newsletter"åˆ°é»‘åå•
```

---

## æ€§èƒ½åˆ†æ

### æ—¶é—´å¤æ‚åº¦

**å„æ­¥éª¤å¤æ‚åº¦**:
- **URLè§„èŒƒåŒ–**: O(n) - nä¸ªURLï¼Œæ¯ä¸ªç®€å•å­—ç¬¦ä¸²æ“ä½œ
- **è·¯å¾„è¿‡æ»¤**: O(n Ã— m) - nä¸ªURL, mä¸ªé»‘åå•å…³é”®è¯ï¼ˆmé€šå¸¸<50ï¼‰
- **æ–‡ä»¶ç±»å‹è¿‡æ»¤**: O(n Ã— k) - nä¸ªURL, kä¸ªæ–‡ä»¶æ‰©å±•åï¼ˆké€šå¸¸<30ï¼‰
- **åŸŸåè¿‡æ»¤**: O(n) - URLè§£æå¼€é”€
- **å»é‡ä¼˜åŒ–**: O(n) - ä½¿ç”¨setå»é‡

**æ€»ä½“å¤æ‚åº¦**: O(n Ã— max(m, k))

**å®é™…æ€§èƒ½**ï¼ˆ1000ä¸ªURLï¼‰:
- è·¯å¾„è¿‡æ»¤: ~5-10ms
- æ–‡ä»¶ç±»å‹è¿‡æ»¤: ~3-5ms
- åŸŸåè¿‡æ»¤: ~5-8ms
- å»é‡ä¼˜åŒ–: ~10-15ms
- **æ€»è€—æ—¶**: ~25-40ms âœ… å¯æ¥å—

---

### å†…å­˜å ç”¨

**æ•°æ®è§„æ¨¡ä¼°ç®—**:
```
5000ä¸ªURL Ã— å¹³å‡100å­—ç¬¦ Ã— 2å­—èŠ‚/å­—ç¬¦ = ~1MB
é»‘åå•é…ç½®: ~10KB
ä¸­é—´ç»“æœé›†: ~500KB

æ€»å†…å­˜å ç”¨: ~1.5MB
```

**å½±å“è¯„ä¼°**: âœ… å¯å¿½ç•¥ä¸è®¡ï¼ˆç›¸æ¯”Scrape APIçš„ç½‘ç»œä¼ è¾“ï¼‰

---

### ä¼˜åŒ–å»ºè®®

#### ä¼˜åŒ–1: ä½¿ç”¨é›†åˆï¼ˆsetï¼‰æŸ¥æ‰¾

**ä¼˜åŒ–å‰**:
```python
# åˆ—è¡¨æŸ¥æ‰¾ O(m)
if any(keyword in path for keyword in PATH_BLACKLIST):
    ...
```

**ä¼˜åŒ–å**:
```python
# é›†åˆæŸ¥æ‰¾ O(1)
PATH_BLACKLIST_SET = set(PATH_BLACKLIST)
if any(keyword in path for keyword in PATH_BLACKLIST_SET):
    ...
```

**æ•ˆæœ**: å¯¹äºå¤§é»‘åå•ï¼ˆ>100é¡¹ï¼‰æ˜¾è‘—æå‡

---

#### ä¼˜åŒ–2: é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼

**ä¼˜åŒ–å‰**:
```python
import re
for url in urls:
    if re.search(r'(login|register|about)', url):
        ...
```

**ä¼˜åŒ–å**:
```python
import re
PATTERN = re.compile(r'(login|register|about)')
for url in urls:
    if PATTERN.search(url):
        ...
```

**æ•ˆæœ**: æ­£åˆ™åŒ¹é…æ€§èƒ½æå‡30-50%

---

#### ä¼˜åŒ–3: æ‰¹é‡å¤„ç†å‡å°‘æ—¥å¿—

**ä¼˜åŒ–å‰**:
```python
for url in urls:
    if should_filter(url):
        logger.debug(f"Filtered: {url}")
```

**ä¼˜åŒ–å**:
```python
filtered_urls = []
for url in urls:
    if should_filter(url):
        filtered_urls.append(url)

# æ‰¹é‡æ—¥å¿—
if filtered_urls and logger.isEnabledFor(logging.DEBUG):
    logger.debug(f"Filtered {len(filtered_urls)} URLs")
```

**æ•ˆæœ**: å‡å°‘æ—¥å¿—I/Oå¼€é”€

---

## å®æ–½è®¡åˆ’

### é˜¶æ®µ1: æ ¸å¿ƒè¿‡æ»¤åŠŸèƒ½ï¼ˆ1-2å°æ—¶ï¼‰

**å·¥ä½œå†…å®¹**:
1. âœ… å®ç° `_filter_by_path_keywords()` - 30åˆ†é’Ÿ
2. âœ… å®ç° `_filter_by_file_type()` - 20åˆ†é’Ÿ
3. âœ… å®ç° `_filter_external_urls()` - 20åˆ†é’Ÿ
4. âœ… é›†æˆåˆ°æ‰§è¡Œæµç¨‹ (Line 150å) - 20åˆ†é’Ÿ
5. âœ… æ·»åŠ æ—¥å¿—è¾“å‡º - 10åˆ†é’Ÿ

**å®Œæˆæ ‡å‡†**:
- ä¸‰ä¸ªè¿‡æ»¤æ–¹æ³•æ­£å¸¸å·¥ä½œ
- æ—¥å¿—æ­£ç¡®è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡
- ä¸å½±å“ç°æœ‰åŠŸèƒ½

**éªŒè¯æ–¹æ³•**:
- åˆ›å»ºæµ‹è¯•ä»»åŠ¡
- æ£€æŸ¥æ—¥å¿—è¾“å‡º
- å¯¹æ¯”è¿‡æ»¤å‰åURLæ•°é‡

---

### é˜¶æ®µ2: ä¼˜åŒ–å¢å¼ºï¼ˆ1å°æ—¶ï¼‰

**å·¥ä½œå†…å®¹**:
1. âœ… æ·»åŠ  `_normalize_url()` - 20åˆ†é’Ÿ
2. âœ… æ·»åŠ  `_deduplicate_urls()` - 30åˆ†é’Ÿ
3. âœ… ä¼˜åŒ–é»‘åå•åˆ†ç±»ç®¡ç† - 10åˆ†é’Ÿ

**å®Œæˆæ ‡å‡†**:
- URLå»é‡åŠŸèƒ½æ­£å¸¸
- é»‘åå•æŒ‰ç±»åˆ«ç»„ç»‡
- ä»£ç å¯è¯»æ€§æå‡

**éªŒè¯æ–¹æ³•**:
- éªŒè¯è·Ÿè¸ªå‚æ•°è¢«æ­£ç¡®ç§»é™¤
- éªŒè¯åˆ†é¡µURLè¢«å»é‡

---

### é˜¶æ®µ3: æµ‹è¯•å’Œè°ƒä¼˜ï¼ˆ1å°æ—¶ï¼‰

**å·¥ä½œå†…å®¹**:
1. âœ… ç¼–å†™å•å…ƒæµ‹è¯• - 30åˆ†é’Ÿ
2. âœ… çœŸå®æ•°æ®æµ‹è¯• - 20åˆ†é’Ÿ
3. âœ… æ ¹æ®ç»“æœè°ƒä¼˜é»‘åå• - 10åˆ†é’Ÿ

**å®Œæˆæ ‡å‡†**:
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡ >80%
- çœŸå®æµ‹è¯•è¿‡æ»¤ç‡è¾¾åˆ°ç›®æ ‡
- è¯¯æ€ç‡ <5%

**éªŒè¯æ–¹æ³•**:
- è¿è¡Œå•å…ƒæµ‹è¯•
- äººå·¥æŠ½æ ·éªŒè¯
- è®°å½•æµ‹è¯•æŠ¥å‘Š

---

### æ€»å·¥ä½œé‡

**é¢„ä¼°**: 3-4å°æ—¶ï¼ˆå®Œæ•´å®ç° + æµ‹è¯•ï¼‰

**å®é™…å¯èƒ½**: 4-6å°æ—¶ï¼ˆåŒ…å«è°ƒè¯•å’Œè°ƒä¼˜ï¼‰

---

## é¢„æœŸæ•ˆæœ

### è¿‡æ»¤ç‡é¢„ä¼°

**åŸºäºå…¸å‹æ–°é—»ç½‘ç«™**:

| ç½‘ç«™ç±»å‹ | åŸå§‹URLs | åŠŸèƒ½é¡µé¢ | æ–‡ä»¶ | å¤–éƒ¨é“¾æ¥ | å»é‡ | æœ€ç»ˆä¿ç•™ | è¿‡æ»¤ç‡ |
|---------|---------|---------|------|---------|------|---------|-------|
| å¤§å‹æ–°é—»é—¨æˆ· | 5000 | -800 (16%) | -300 (6%) | -200 (4%) | -500 (10%) | 3200 | 36% |
| åœ°åŒºæ–°é—»ç½‘ç«™ | 2000 | -400 (20%) | -150 (7.5%) | -100 (5%) | -150 (7.5%) | 1200 | 40% |
| ä¸“ä¸šæ–°é—»ç½‘ç«™ | 1000 | -200 (20%) | -80 (8%) | -50 (5%) | -70 (7%) | 600 | 40% |

**æ€»ä½“é¢„ä¼°**: 35-45% è¿‡æ»¤ç‡

**é«˜è´¨é‡ç½‘ç«™**: 60-80%ï¼ˆåŠŸèƒ½é¡µé¢å’Œå¯¼èˆªç»“æ„å¤æ‚ï¼‰

---

### æ•ˆç‡æå‡

**æˆæœ¬èŠ‚çº¦**:
```
åŸå§‹: 1000ä¸ªURL Ã— 1 credit = 1000 credits
è¿‡æ»¤å: 600ä¸ªURL Ã— 1 credit = 600 credits
èŠ‚çº¦: 400 credits (40%)
```

**æ—¶é—´èŠ‚çº¦**:
```
åŸå§‹: 1000ä¸ªURL Ã— å¹³å‡5ç§’ = 83åˆ†é’Ÿ
è¿‡æ»¤å: 600ä¸ªURL Ã— å¹³å‡5ç§’ = 50åˆ†é’Ÿ
èŠ‚çº¦: 33åˆ†é’Ÿ (40%)
```

**ç»“æœè´¨é‡æå‡**:
- å‡å°‘æ— å…³å†…å®¹å¹²æ‰°
- æé«˜AIå¤„ç†å‡†ç¡®åº¦
- æ”¹å–„ç”¨æˆ·ä½“éªŒ

---

## åç»­æ‰©å±•æ–¹å‘

### æ‰©å±•1: æ™ºèƒ½é»‘åå•å­¦ä¹ 

**åŠŸèƒ½æè¿°**:
- ç”¨æˆ·å¯æ ‡è®°"æ— ç”¨URL"
- ç³»ç»Ÿè‡ªåŠ¨æå–URLæ¨¡å¼
- åŠ¨æ€æ›´æ–°é»‘åå•è§„åˆ™

**å®ç°æ€è·¯**:
1. æ·»åŠ ç”¨æˆ·åé¦ˆæ¥å£
2. æ”¶é›†æ ‡è®°çš„URL
3. ä½¿ç”¨æœºå™¨å­¦ä¹ æå–æ¨¡å¼
4. è‡ªåŠ¨ç”Ÿæˆé»‘åå•è§„åˆ™

**ä»·å€¼**:
- æŒç»­ä¼˜åŒ–è¿‡æ»¤å‡†ç¡®åº¦
- é€‚åº”ä¸åŒç½‘ç«™ç»“æ„
- å‡å°‘äººå·¥ç»´æŠ¤æˆæœ¬

---

### æ‰©å±•2: ç½‘ç«™ç±»å‹è‡ªåŠ¨è¯†åˆ«

**åŠŸèƒ½æè¿°**:
- è‡ªåŠ¨è¯†åˆ«ç½‘ç«™ç±»å‹ï¼ˆæ–°é—»/åšå®¢/ç”µå•†ï¼‰
- åº”ç”¨å¯¹åº”çš„é¢„è®¾è§„åˆ™
- ä¼˜åŒ–è¿‡æ»¤æ•ˆæœ

**å®ç°æ€è·¯**:
1. åˆ†æç½‘ç«™ç»“æ„å’ŒURLæ¨¡å¼
2. è¯†åˆ«ç½‘ç«™ç±»å‹ç‰¹å¾
3. åº”ç”¨å¯¹åº”é¢„è®¾è§„åˆ™
4. æ”¯æŒè‡ªå®šä¹‰è§„åˆ™è¦†ç›–

**é¢„è®¾è§„åˆ™ç¤ºä¾‹**:
```python
PRESET_RULES = {
    "news_site": {
        "path_whitelist": ['/news/', '/article/', '/story/'],
        "additional_blacklist": ['subscription', 'paywall']
    },
    "blog_site": {
        "path_whitelist": ['/blog/', '/post/', '/entry/'],
        "additional_blacklist": ['author', 'feed']
    }
}
```

---

### æ‰©å±•3: é…ç½®åŒ–ç®¡ç†ç•Œé¢

**åŠŸèƒ½æè¿°**:
- Webç®¡ç†ç•Œé¢é…ç½®é»‘åå•
- æ”¯æŒè§„åˆ™å¯ç”¨/ç¦ç”¨
- å®æ—¶ç”Ÿæ•ˆï¼Œæ— éœ€é‡å¯

**å®ç°è¦ç‚¹**:
1. é»‘åå•å­˜å‚¨åœ¨æ•°æ®åº“
2. æä¾›RESTful APIç®¡ç†
3. å‰ç«¯ç®¡ç†ç•Œé¢
4. è§„åˆ™ç‰ˆæœ¬æ§åˆ¶

**ç•Œé¢è®¾è®¡**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ URLè¿‡æ»¤è§„åˆ™ç®¡ç†                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è§„åˆ™ç±»å‹: [è·¯å¾„å…³é”®è¯ â–¼]             â”‚
â”‚                                     â”‚
â”‚ å½“å‰è§„åˆ™ (15ä¸ª):                    â”‚
â”‚ â˜‘ login        [ç¼–è¾‘] [åˆ é™¤]       â”‚
â”‚ â˜‘ register     [ç¼–è¾‘] [åˆ é™¤]       â”‚
â”‚ â˜ about        [ç¼–è¾‘] [åˆ é™¤]       â”‚
â”‚ â˜‘ contact      [ç¼–è¾‘] [åˆ é™¤]       â”‚
â”‚                                     â”‚
â”‚ [+ æ·»åŠ æ–°è§„åˆ™]                      â”‚
â”‚                                     â”‚
â”‚ [æµ‹è¯•è§„åˆ™] [ä¿å­˜] [é‡ç½®]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ‰©å±•4: A/Bæµ‹è¯•ä¸åŒè§„åˆ™

**åŠŸèƒ½æè¿°**:
- åŒæ—¶è¿è¡Œå¤šå¥—è¿‡æ»¤è§„åˆ™
- å¯¹æ¯”è¿‡æ»¤æ•ˆæœ
- é€‰æ‹©æœ€ä¼˜è§„åˆ™

**å®ç°æ€è·¯**:
1. å®šä¹‰å¤šå¥—è§„åˆ™é…ç½®
2. éšæœºåˆ†é…ä»»åŠ¡åˆ°ä¸åŒè§„åˆ™
3. æ”¶é›†è¿‡æ»¤æ•ˆæœæŒ‡æ ‡
4. ç»Ÿè®¡åˆ†æé€‰æ‹©æœ€ä¼˜

**æŒ‡æ ‡å¯¹æ¯”**:
```
è§„åˆ™A vs è§„åˆ™B:
â”œâ”€ è¿‡æ»¤ç‡: 35% vs 42%
â”œâ”€ ScrapeæˆåŠŸç‡: 85% vs 88%
â”œâ”€ ç»“æœç›¸å…³æ€§: 4.2/5 vs 4.5/5
â””â”€ æ¨è: è§„åˆ™B
```

---

## é£é™©å’Œæ³¨æ„äº‹é¡¹

### é£é™©1: è¯¯æ€æœ‰æ•ˆURL âš ï¸

**åœºæ™¯**: è·¯å¾„ä¸­åŒ…å«é»‘åå•å…³é”®è¯ä½†å®é™…æ˜¯æœ‰æ•ˆå†…å®¹

**ç¤ºä¾‹**:
- `/news/about-economy` (åŒ…å«"about"ä½†æ˜¯æ–°é—»å†…å®¹)
- `/blog/contact-tracing-technology` (åŒ…å«"contact"ä½†æ˜¯æŠ€æœ¯æ–‡ç« )

**ç¼“è§£æªæ–½**:
1. ä½¿ç”¨å®Œæ•´è·¯å¾„æ®µåŒ¹é…ï¼Œè€Œéç®€å•å­ä¸²åŒ¹é…
2. æ·»åŠ ç™½åå•æœºåˆ¶ï¼ˆä¼˜å…ˆçº§é«˜äºé»‘åå•ï¼‰
3. äººå·¥æŠ½æ ·éªŒè¯ï¼ŒæŒç»­ä¼˜åŒ–è§„åˆ™
4. æ”¯æŒç”¨æˆ·åé¦ˆè¯¯æ€URL

---

### é£é™©2: æ¼æ£€æ— ç”¨URL âš ï¸

**åœºæ™¯**: æ–°ç±»å‹çš„æ— ç”¨URLæœªè¢«é»‘åå•è¦†ç›–

**ç¤ºä¾‹**:
- `/subscribe-newsletter` (æœªåŒ…å«åœ¨é»‘åå•ä¸­)
- `/email-signup` (æ–°çš„æ³¨å†Œé¡µé¢æ¨¡å¼)

**ç¼“è§£æªæ–½**:
1. æŒç»­æ”¶é›†ç”¨æˆ·åé¦ˆ
2. å®šæœŸåˆ†ææ¼æ£€URLæ¨¡å¼
3. åŠ¨æ€æ›´æ–°é»‘åå•
4. æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼è§„åˆ™

---

### é£é™©3: æ€§èƒ½å½±å“

**åœºæ™¯**: å¤§é‡URLè¿‡æ»¤å¯¼è‡´å»¶è¿Ÿ

**ç¤ºä¾‹**: 5000ä¸ªURL Ã— å¤æ‚æ­£åˆ™åŒ¹é… = 500ms+

**ç¼“è§£æªæ–½**:
1. ä½¿ç”¨ç®€å•å­—ç¬¦ä¸²åŒ¹é…ä¼˜å…ˆ
2. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
3. æ‰¹é‡å¤„ç†å‡å°‘å¼€é”€
4. ç›‘æ§è¿‡æ»¤è€—æ—¶

---

### é£é™©4: ç»´æŠ¤æˆæœ¬

**åœºæ™¯**: é»‘åå•éœ€è¦æŒç»­ç»´æŠ¤å’Œæ›´æ–°

**ç¼“è§£æªæ–½**:
1. åˆ†ç±»ç®¡ç†é»‘åå•
2. æ·»åŠ è¯¦ç»†æ³¨é‡Šè¯´æ˜
3. ç‰ˆæœ¬æ§åˆ¶è§„åˆ™å˜æ›´
4. è‡ªåŠ¨åŒ–æµ‹è¯•éªŒè¯

---

## æ€»ç»“

### æ ¸å¿ƒä»·å€¼

**æ–¹æ¡ˆ2: URLæ¨¡å¼è¿‡æ»¤**æä¾›äº†ï¼š

1. **é«˜æ•ˆè¿‡æ»¤**: 60-80% æ— ç”¨é“¾æ¥å»é™¤
2. **å¿«é€Ÿå®æ–½**: 3-4å°æ—¶å®Œæ•´å®ç°
3. **ä½æˆæœ¬**: æ— éœ€å¤–éƒ¨æœåŠ¡ï¼Œçº¯ä»£ç å®ç°
4. **å¯æ‰©å±•**: æ”¯æŒé…ç½®åŒ–å’Œæ™ºèƒ½åŒ–æ‰©å±•

### å®æ–½ä¼˜å…ˆçº§

**ç«‹å³å®æ–½**ï¼ˆé˜¶æ®µ1ï¼‰:
- âœ… è·¯å¾„å…³é”®è¯è¿‡æ»¤
- âœ… æ–‡ä»¶ç±»å‹è¿‡æ»¤
- âœ… åŸŸåèŒƒå›´è¿‡æ»¤
- âœ… åŸºç¡€æ—¥å¿—ç»Ÿè®¡

**çŸ­æœŸä¼˜åŒ–**ï¼ˆé˜¶æ®µ2ï¼‰:
- âœ… URLè§„èŒƒåŒ–
- âœ… URLå»é‡ä¼˜åŒ–
- âœ… é»‘åå•åˆ†ç±»ç®¡ç†

**é•¿æœŸæ‰©å±•**ï¼ˆé˜¶æ®µ3+ï¼‰:
- æ™ºèƒ½é»‘åå•å­¦ä¹ 
- ç½‘ç«™ç±»å‹è¯†åˆ«
- é…ç½®åŒ–ç®¡ç†ç•Œé¢

### é¢„æœŸROI

**æŠ•å…¥**: 4-6å°æ—¶å¼€å‘æ—¶é—´

**äº§å‡º**:
- èŠ‚çœ40% Scrape APIæˆæœ¬
- æå‡40% çˆ¬å–æ•ˆç‡
- æ”¹å–„ç”¨æˆ·ä½“éªŒå’Œç»“æœè´¨é‡

**ROI**: æé«˜ â­â­â­â­â­

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-10
**çŠ¶æ€**: å¾…å®æ–½
