# è‡ªç„¶è¯­è¨€æœç´¢ - æ¨¡å—åŒ–å®ç°æŒ‡å—

**ç‰ˆæœ¬**: v1.0.0-beta
**çŠ¶æ€**: ğŸš§ å®æ–½æŒ‡å—
**ç›®æ ‡è¯»è€…**: åç«¯å¼€å‘å·¥ç¨‹å¸ˆ
**å‰ç½®æ–‡æ¡£**: [æ¨¡å—åŒ–è®¾è®¡æ–‡æ¡£](NL_SEARCH_MODULAR_DESIGN.md)

---

## ğŸ“‹ å®æ–½å‰å‡†å¤‡

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿ä»¥ä¸‹ç¯å¢ƒå·²å°±ç»ªï¼š

```bash
# Python ç‰ˆæœ¬
python --version  # â‰¥ 3.9

# ä¾èµ–åŒ…å®‰è£…
pip install openai  # LLM API å®¢æˆ·ç«¯
pip install pydantic  # é…ç½®ç®¡ç†
pip install pytest pytest-asyncio pytest-cov  # æµ‹è¯•æ¡†æ¶
```

### 2. API Key å‡†å¤‡

éœ€è¦å‡†å¤‡ä»¥ä¸‹ API Keysï¼š

- OpenAI API Key (ç”¨äº LLM å¤„ç†)
- GPT-5 Search API Key (ç”¨äºæœç´¢)
- Firecrawl API Key (å·²æœ‰)

### 3. åŠŸèƒ½å¼€å…³ç¡®è®¤

**é‡è¦**ï¼šé»˜è®¤å…³é—­åŠŸèƒ½å¼€å…³ï¼

```bash
# .env æ–‡ä»¶
NL_SEARCH_ENABLED=false  # ğŸš¨ é»˜è®¤å¿…é¡»ä¸º false
```

---

## Phase 1: åŸºç¡€æ¶æ„æ­å»º

### æ­¥éª¤ 1.1: åˆ›å»ºç›®å½•ç»“æ„

```bash
# æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºç›®å½•
mkdir -p src/services/nl_search
mkdir -p src/core/domain/entities/nl_search
mkdir -p tests/nl_search
```

**ç›®å½•ç»“æ„éªŒè¯**ï¼š

```
src/
â”œâ”€â”€ services/
â”‚   â””â”€â”€ nl_search/
â”‚       â”œâ”€â”€ __init__.py                    # ç©ºæ–‡ä»¶
â”‚       â”œâ”€â”€ config.py                      # å¾…åˆ›å»º
â”‚       â”œâ”€â”€ nl_search_service.py           # å¾…åˆ›å»º
â”‚       â”œâ”€â”€ llm_processor.py               # å¾…åˆ›å»º
â”‚       â”œâ”€â”€ gpt5_search_adapter.py         # å¾…åˆ›å»º
â”‚       â””â”€â”€ content_enricher.py            # å¾…åˆ›å»º
â”œâ”€â”€ core/domain/entities/
â”‚   â””â”€â”€ nl_search/
â”‚       â”œâ”€â”€ __init__.py                    # å¾…åˆ›å»º
â”‚       â”œâ”€â”€ nl_search_log.py               # å¾…åˆ›å»º
â”‚       â”œâ”€â”€ nl_user_selection.py           # å¾…åˆ›å»º
â”‚       â””â”€â”€ enums.py                       # å¾…åˆ›å»º
â””â”€â”€ tests/
    â””â”€â”€ nl_search/
        â”œâ”€â”€ __init__.py                    # ç©ºæ–‡ä»¶
        â”œâ”€â”€ test_llm_processor.py          # å¾…åˆ›å»º
        â”œâ”€â”€ test_gpt5_adapter.py           # å¾…åˆ›å»º
        â””â”€â”€ test_nl_search_service.py      # å¾…åˆ›å»º
```

### æ­¥éª¤ 1.2: å®šä¹‰å®ä½“æ¨¡å‹

**æ–‡ä»¶**: `src/core/domain/entities/nl_search/nl_search_log.py`

```python
"""
è‡ªç„¶è¯­è¨€æœç´¢è®°å½•å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""
from datetime import datetime
from typing import Optional, Dict, Any
from pydantic import BaseModel, Field


class NLSearchLog(BaseModel):
    """è‡ªç„¶è¯­è¨€æœç´¢è®°å½•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    id: Optional[int] = Field(None, description="ä¸»é”®ID")
    query_text: str = Field(..., description="åŸå§‹ç”¨æˆ·è¾“å…¥", max_length=1000)
    llm_analysis: Optional[Dict[str, Any]] = Field(
        None,
        description="å¤§æ¨¡å‹è§£æç»“æ„ï¼ˆå…³é”®è¯ã€å®ä½“ã€æ—¶é—´èŒƒå›´ç­‰ï¼‰"
    )
    created_at: Optional[datetime] = Field(None, description="åˆ›å»ºæ—¶é—´")

    class Config:
        from_attributes = True  # SQLAlchemy ORM æ”¯æŒ
        json_schema_extra = {
            "example": {
                "id": 123456,
                "query_text": "æœ€è¿‘æœ‰å“ªäº›AIæŠ€æœ¯çªç ´",
                "llm_analysis": {
                    "intent": "technology_news",
                    "keywords": ["AI", "æŠ€æœ¯çªç ´", "2024"],
                    "entities": [
                        {"type": "technology", "value": "AI"}
                    ],
                    "time_range": {
                        "type": "recent",
                        "from": "2024-01-01",
                        "to": "2024-12-31"
                    },
                    "confidence": 0.95
                },
                "created_at": "2024-11-10T10:00:00"
            }
        }
```

**æ–‡ä»¶**: `src/core/domain/entities/nl_search/__init__.py`

```python
"""
NL Search å®ä½“æ¨¡å—ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""
from .nl_search_log import NLSearchLog

__all__ = [
    "NLSearchLog",
]
```

### æ­¥éª¤ 1.3: å®ç°ä»“åº“å±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰

**æ–‡ä»¶**: `src/infrastructure/database/nl_search_repositories.py`

```python
"""
NL Search æ•°æ®ä»“åº“ï¼ˆç®€åŒ–ç‰ˆ - MySQLï¼‰
"""
from typing import List, Optional, Dict, Any
from datetime import datetime
import json

from src.infrastructure.database.connection import get_database
from src.core.domain.entities.nl_search import NLSearchLog


class NLSearchLogRepository:
    """è‡ªç„¶è¯­è¨€æœç´¢è®°å½•ä»“åº“ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.db = get_database()  # è·å–æ•°æ®åº“è¿æ¥

    async def create(
        self,
        query_text: str,
        llm_analysis: Optional[Dict[str, Any]] = None
    ) -> int:
        """åˆ›å»ºæœç´¢è®°å½•"""
        query = """
        INSERT INTO nl_search_logs (query_text, llm_analysis, created_at)
        VALUES (:query_text, :llm_analysis, NOW())
        """

        llm_analysis_json = json.dumps(llm_analysis) if llm_analysis else None

        result = await self.db.execute(
            query=query,
            values={
                "query_text": query_text,
                "llm_analysis": llm_analysis_json
            }
        )
        return result  # è¿”å›æ’å…¥çš„ ID

    async def get_by_id(self, log_id: int) -> Optional[NLSearchLog]:
        """æ ¹æ®IDè·å–æœç´¢è®°å½•"""
        query = """
        SELECT id, query_text, llm_analysis, created_at
        FROM nl_search_logs
        WHERE id = :log_id
        """

        row = await self.db.fetch_one(query=query, values={"log_id": log_id})

        if not row:
            return None

        # è§£æ JSON å­—æ®µ
        llm_analysis = json.loads(row["llm_analysis"]) if row["llm_analysis"] else None

        return NLSearchLog(
            id=row["id"],
            query_text=row["query_text"],
            llm_analysis=llm_analysis,
            created_at=row["created_at"]
        )

    async def update_llm_analysis(
        self,
        log_id: int,
        llm_analysis: Dict[str, Any]
    ) -> bool:
        """æ›´æ–° LLM è§£æç»“æœ"""
        query = """
        UPDATE nl_search_logs
        SET llm_analysis = :llm_analysis
        WHERE id = :log_id
        """

        result = await self.db.execute(
            query=query,
            values={
                "log_id": log_id,
                "llm_analysis": json.dumps(llm_analysis)
            }
        )
        return result > 0

    async def get_recent(
        self,
        limit: int = 10,
        offset: int = 0
    ) -> List[NLSearchLog]:
        """è·å–æœ€è¿‘çš„æœç´¢è®°å½•"""
        query = """
        SELECT id, query_text, llm_analysis, created_at
        FROM nl_search_logs
        ORDER BY created_at DESC
        LIMIT :limit OFFSET :offset
        """

        rows = await self.db.fetch_all(
            query=query,
            values={"limit": limit, "offset": offset}
        )

        logs = []
        for row in rows:
            llm_analysis = json.loads(row["llm_analysis"]) if row["llm_analysis"] else None
            logs.append(NLSearchLog(
                id=row["id"],
                query_text=row["query_text"],
                llm_analysis=llm_analysis,
                created_at=row["created_at"]
            ))

        return logs

    async def search_by_keyword(
        self,
        keyword: str,
        limit: int = 20
    ) -> List[NLSearchLog]:
        """æ ¹æ®å…³é”®è¯æœç´¢ï¼ˆMySQL JSON æŸ¥è¯¢ï¼‰"""
        query = """
        SELECT id, query_text, llm_analysis, created_at
        FROM nl_search_logs
        WHERE JSON_CONTAINS(
            llm_analysis->'$.keywords',
            JSON_QUOTE(:keyword)
        )
        OR query_text LIKE :query_pattern
        ORDER BY created_at DESC
        LIMIT :limit
        """

        rows = await self.db.fetch_all(
            query=query,
            values={
                "keyword": keyword,
                "query_pattern": f"%{keyword}%",
                "limit": limit
            }
        )

        logs = []
        for row in rows:
            llm_analysis = json.loads(row["llm_analysis"]) if row["llm_analysis"] else None
            logs.append(NLSearchLog(
                id=row["id"],
                query_text=row["query_text"],
                llm_analysis=llm_analysis,
                created_at=row["created_at"]
            ))

        return logs

    async def delete_old_records(self, days: int = 30) -> int:
        """åˆ é™¤æ—§è®°å½•"""
        query = """
        DELETE FROM nl_search_logs
        WHERE created_at < DATE_SUB(NOW(), INTERVAL :days DAY)
        """

        result = await self.db.execute(
            query=query,
            values={"days": days}
        )
        return result  # è¿”å›åˆ é™¤çš„è¡Œæ•°
```

### æ­¥éª¤ 1.5: é…ç½®åŠŸèƒ½å¼€å…³

**æ–‡ä»¶**: `src/services/nl_search/config.py`

```python
"""
NL Search åŠŸèƒ½é…ç½®
"""
from pydantic_settings import BaseSettings
from typing import Optional


class NLSearchConfig(BaseSettings):
    """NL Search åŠŸèƒ½é…ç½®"""

    # åŠŸèƒ½å¼€å…³ï¼ˆé»˜è®¤å…³é—­ï¼ï¼‰
    enabled: bool = False

    # LLM é…ç½®
    llm_api_key: Optional[str] = None
    llm_model: str = "gpt-4"
    llm_temperature: float = 0.7
    llm_max_tokens: int = 500

    # GPT-5 æœç´¢é…ç½®
    gpt5_search_api_key: Optional[str] = None
    gpt5_max_results: int = 10

    # Scrape é…ç½®
    scrape_timeout: int = 30
    scrape_max_concurrent: int = 3

    # ä¸šåŠ¡é…ç½®
    max_results_per_query: int = 20
    enable_auto_scrape: bool = True

    # æ€§èƒ½é…ç½®
    query_timeout: int = 30  # ç§’
    cache_ttl: int = 3600    # ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰

    class Config:
        env_prefix = "NL_SEARCH_"
        env_file = ".env"
        extra = "ignore"


# å…¨å±€é…ç½®å®ä¾‹
nl_search_config = NLSearchConfig()
```

### æ­¥éª¤ 1.4: åˆ›å»ºæ•°æ®åº“è¡¨å’Œç´¢å¼•è„šæœ¬

**æ–‡ä»¶**: `scripts/create_nl_search_tables.sql`

```sql
-- ===================================================================
-- NL Search æ•°æ®è¡¨åˆ›å»ºè„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
-- ===================================================================

-- 1. åˆ›å»º nl_search_logs è¡¨
CREATE TABLE IF NOT EXISTS nl_search_logs (
  id BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®',
  query_text TEXT NOT NULL COMMENT 'åŸå§‹ç”¨æˆ·è¾“å…¥',
  llm_analysis JSON NULL COMMENT 'å¤§æ¨¡å‹è§£æç»“æ„ï¼ˆå…³é”®è¯ã€å®ä½“ã€æ—¶é—´èŒƒå›´ç­‰ï¼‰',
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',

  -- ç´¢å¼•
  INDEX idx_created (created_at DESC)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='è‡ªç„¶è¯­è¨€æœç´¢è®°å½•è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰';

-- 2. å¯é€‰ï¼šæ‰©å±• search_results è¡¨ï¼ˆå¦‚éœ€å…³è”ï¼‰
-- ALTER TABLE search_results
-- ADD COLUMN nl_search_log_id BIGINT NULL COMMENT 'å…³è”çš„NLæœç´¢è®°å½•ID',
-- ADD INDEX idx_nl_search_log (nl_search_log_id);

-- 3. å¯é€‰ï¼šåˆ›å»ºå…³è”è¡¨
-- CREATE TABLE IF NOT EXISTS nl_search_result_relations (
--   id BIGINT AUTO_INCREMENT PRIMARY KEY,
--   nl_search_log_id BIGINT NOT NULL COMMENT 'NLæœç´¢è®°å½•ID',
--   result_id BIGINT NOT NULL COMMENT 'æœç´¢ç»“æœID',
--   result_type ENUM('search_result', 'news_result') DEFAULT 'search_result',
--   created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
--   INDEX idx_log (nl_search_log_id),
--   INDEX idx_result (result_id, result_type),
--   UNIQUE KEY uk_log_result (nl_search_log_id, result_id, result_type)
-- ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- ===================================================================
-- éªŒè¯è¡¨åˆ›å»º
-- ===================================================================
SELECT
  TABLE_NAME,
  TABLE_COMMENT,
  CREATE_TIME
FROM information_schema.TABLES
WHERE TABLE_SCHEMA = DATABASE()
  AND TABLE_NAME = 'nl_search_logs';

-- éªŒè¯ç´¢å¼•
SHOW INDEX FROM nl_search_logs;
```

**Python è„šæœ¬æ‰§è¡Œ SQL**ï¼š

**æ–‡ä»¶**: `scripts/create_nl_search_tables.py`

```python
"""
åˆ›å»º NL Search æ•°æ®è¡¨
"""
import asyncio
from pathlib import Path

from src.infrastructure.database.connection import get_database


async def create_nl_search_tables():
    """åˆ›å»º NL Search ç›¸å…³è¡¨"""
    db = get_database()

    # è¯»å– SQL è„šæœ¬
    sql_file = Path(__file__).parent / "create_nl_search_tables.sql"
    with open(sql_file, 'r', encoding='utf-8') as f:
        sql_content = f.read()

    # åˆ†å‰²å¹¶æ‰§è¡Œ SQL è¯­å¥
    statements = [s.strip() for s in sql_content.split(';') if s.strip()]

    for statement in statements:
        # è·³è¿‡æ³¨é‡Šè¡Œ
        if statement.startswith('--') or statement.startswith('SELECT') or statement.startswith('SHOW'):
            continue

        try:
            await db.execute(statement)
            print(f"âœ… æ‰§è¡ŒæˆåŠŸ: {statement[:50]}...")
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
            print(f"   SQL: {statement[:100]}...")

    print("\nğŸ‰ æ•°æ®è¡¨åˆ›å»ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(create_nl_search_tables())
```

**è¿è¡Œè¡¨åˆ›å»º**ï¼š

```bash
# æ–¹å¼1ï¼šç›´æ¥æ‰§è¡Œ SQL æ–‡ä»¶
mysql -u username -p database_name < scripts/create_nl_search_tables.sql

# æ–¹å¼2ï¼šé€šè¿‡ Python è„šæœ¬
python scripts/create_nl_search_tables.py
```

### æ­¥éª¤ 1.5: ç¼–å†™åŸºç¡€æµ‹è¯•

**æ–‡ä»¶**: `tests/nl_search/test_entities.py`

```python
"""
æµ‹è¯• NL Search å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""
import pytest
from datetime import datetime

from src.core.domain.entities.nl_search import NLSearchLog


class TestNLSearchLog:
    def test_create_log(self):
        """æµ‹è¯•åˆ›å»ºæœç´¢è®°å½•"""
        log = NLSearchLog(
            query_text="æœ€è¿‘æœ‰å“ªäº›AIæŠ€æœ¯çªç ´"
        )

        assert log.query_text == "æœ€è¿‘æœ‰å“ªäº›AIæŠ€æœ¯çªç ´"
        assert log.llm_analysis is None
        assert log.id is None

    def test_create_log_with_analysis(self):
        """æµ‹è¯•åˆ›å»ºåŒ…å«åˆ†æç»“æœçš„è®°å½•"""
        llm_analysis = {
            "intent": "technology_news",
            "keywords": ["AI", "æŠ€æœ¯çªç ´"],
            "confidence": 0.95
        }

        log = NLSearchLog(
            query_text="AIæŠ€æœ¯",
            llm_analysis=llm_analysis
        )

        assert log.query_text == "AIæŠ€æœ¯"
        assert log.llm_analysis["intent"] == "technology_news"
        assert len(log.llm_analysis["keywords"]) == 2

    def test_log_with_id(self):
        """æµ‹è¯•åŒ…å« ID çš„è®°å½•"""
        log = NLSearchLog(
            id=123456,
            query_text="æµ‹è¯•æŸ¥è¯¢",
            created_at=datetime.now()
        )

        assert log.id == 123456
        assert log.query_text == "æµ‹è¯•æŸ¥è¯¢"
        assert log.created_at is not None
```

**æ–‡ä»¶**: `tests/nl_search/test_repository.py`

```python
"""
æµ‹è¯• NL Search ä»“åº“å±‚
"""
import pytest
from datetime import datetime

from src.infrastructure.database.nl_search_repositories import NLSearchLogRepository


@pytest.fixture
async def repository():
    """åˆ›å»ºä»“åº“å®ä¾‹"""
    return NLSearchLogRepository()


@pytest.mark.asyncio
class TestNLSearchLogRepository:
    async def test_create_search_log(self, repository):
        """æµ‹è¯•åˆ›å»ºæœç´¢è®°å½•"""
        query_text = "æœ€è¿‘æœ‰å“ªäº›AIæŠ€æœ¯çªç ´"
        llm_analysis = {
            "intent": "technology_news",
            "keywords": ["AI", "æŠ€æœ¯çªç ´"]
        }

        log_id = await repository.create(
            query_text=query_text,
            llm_analysis=llm_analysis
        )

        assert log_id > 0

    async def test_get_by_id(self, repository):
        """æµ‹è¯•æ ¹æ®IDè·å–è®°å½•"""
        # å…ˆåˆ›å»ºè®°å½•
        log_id = await repository.create(
            query_text="æµ‹è¯•æŸ¥è¯¢"
        )

        # è·å–è®°å½•
        log = await repository.get_by_id(log_id)

        assert log is not None
        assert log.id == log_id
        assert log.query_text == "æµ‹è¯•æŸ¥è¯¢"

    async def test_update_llm_analysis(self, repository):
        """æµ‹è¯•æ›´æ–° LLM åˆ†æç»“æœ"""
        # åˆ›å»ºè®°å½•
        log_id = await repository.create(
            query_text="æµ‹è¯•æŸ¥è¯¢"
        )

        # æ›´æ–°åˆ†æç»“æœ
        llm_analysis = {
            "intent": "test",
            "keywords": ["æµ‹è¯•"]
        }
        success = await repository.update_llm_analysis(
            log_id=log_id,
            llm_analysis=llm_analysis
        )

        assert success is True

        # éªŒè¯æ›´æ–°
        log = await repository.get_by_id(log_id)
        assert log.llm_analysis["intent"] == "test"

    async def test_get_recent(self, repository):
        """æµ‹è¯•è·å–æœ€è¿‘è®°å½•"""
        # åˆ›å»ºå¤šæ¡è®°å½•
        for i in range(5):
            await repository.create(query_text=f"æŸ¥è¯¢ {i}")

        # è·å–æœ€è¿‘3æ¡
        logs = await repository.get_recent(limit=3)

        assert len(logs) <= 3
        # éªŒè¯æŒ‰æ—¶é—´å€’åº
        if len(logs) > 1:
            assert logs[0].created_at >= logs[1].created_at
```

**è¿è¡Œæµ‹è¯•**ï¼š

```bash
# è¿è¡Œæ‰€æœ‰ NL Search æµ‹è¯•
pytest tests/nl_search/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
pytest tests/nl_search/test_entities.py -v
pytest tests/nl_search/test_repository.py -v

# æŸ¥çœ‹æµ‹è¯•è¦†ç›–ç‡
pytest tests/nl_search/ --cov=src/core/domain/entities/nl_search --cov=src/infrastructure/database/nl_search_repositories --cov-report=html
```

### âœ… Phase 1 éªŒæ”¶æ ‡å‡†

å®Œæˆä»¥ä¸‹æ£€æŸ¥ï¼š

- [ ] ç›®å½•ç»“æ„å®Œæ•´åˆ›å»º
- [ ] æ‰€æœ‰å®ä½“æ¨¡å‹å®šä¹‰å®Œæˆ
- [ ] ä»“åº“å±‚å®ç°å®Œæˆ
- [ ] é…ç½®ç±»å®šä¹‰å®Œæˆ
- [ ] æ•°æ®åº“ç´¢å¼•åˆ›å»ºæˆåŠŸ
- [ ] åŸºç¡€æµ‹è¯•é€šè¿‡

---

## Phase 2-8: åç»­å®æ–½

**å®Œæ•´çš„å®æ–½æ­¥éª¤è¯·å‚è€ƒ**:
- [æ¨¡å—åŒ–è®¾è®¡æ–‡æ¡£ - å®ç°è®¡åˆ’ç« èŠ‚](NL_SEARCH_MODULAR_DESIGN.md#å®ç°è®¡åˆ’)

**å…³é”®æé†’**:

1. **åŠŸèƒ½å¼€å…³**: å§‹ç»ˆä¿æŒ `NL_SEARCH_ENABLED=false`ï¼Œç›´åˆ°æ‰€æœ‰æµ‹è¯•é€šè¿‡
2. **ä»£ç éš”ç¦»**: ä¸è¦åœ¨ç°æœ‰æ¨¡å—ä¸­å¯¼å…¥ NL Search ä»£ç 
3. **æµ‹è¯•å…ˆè¡Œ**: æ¯ä¸ªé˜¶æ®µå®Œæˆåç«‹å³ç¼–å†™æµ‹è¯•
4. **æ–‡æ¡£åŒæ­¥**: åŠæ—¶æ›´æ–° API æ–‡æ¡£å’ŒæŠ€æœ¯æ–‡æ¡£
5. **ä»£ç å®¡æŸ¥**: æ¯ä¸ª Phase å®Œæˆåè¿›è¡Œä»£ç å®¡æŸ¥

---

## å¼€å‘è§„èŒƒ

### å‘½åè§„èŒƒ

```python
# âœ… æ­£ç¡®
class NLSearchService:
    """NL Search æ ¸å¿ƒæœåŠ¡"""
    pass

async def create_nl_search_query(...):
    """åˆ›å»ºè‡ªç„¶è¯­è¨€æœç´¢"""
    pass

# âŒ é”™è¯¯
class NaturalLanguageSearchService:  # å¤ªé•¿
    pass

async def create_search(...):  # ä¸æ˜ç¡®
    pass
```

### å¯¼å…¥è§„èŒƒ

```python
# âœ… æ­£ç¡®ï¼šNL Search å¯ä»¥å¯¼å…¥ç°æœ‰æ¨¡å—
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter
from src.core.domain.entities.search_result import SearchResult

# âŒ é”™è¯¯ï¼šç°æœ‰æ¨¡å—ä¸åº”å¯¼å…¥ NL Search
# åœ¨ç°æœ‰æœåŠ¡ä¸­ï¼š
from src.services.nl_search.nl_search_service import NLSearchService  # ç¦æ­¢ï¼
```

### é”™è¯¯å¤„ç†è§„èŒƒ

```python
# âœ… æ­£ç¡®ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†
try:
    result = await llm_processor.parse_query(query_text)
except Exception as e:
    logger.error(f"LLM å¤„ç†å¤±è´¥: {e}", exc_info=True)
    # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
    await log_repo.update_status(
        log_id,
        SearchStatus.FAILED,
        error_message=str(e)
    )
    # è¿”å›å‹å¥½é”™è¯¯ä¿¡æ¯
    raise HTTPException(
        status_code=500,
        detail="æŸ¥è¯¢å¤„ç†å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
    )

# âŒ é”™è¯¯ï¼šä¸å¤„ç†é”™è¯¯
result = await llm_processor.parse_query(query_text)  # å¯èƒ½æŠ›å‡ºå¼‚å¸¸
```

---

## æµ‹è¯•è§„èŒƒ

### å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¦æ±‚

- å®ä½“ç±»: 100%
- ä»“åº“ç±»: >90%
- æœåŠ¡ç±»: >85%
- å·¥å…·ç±»: >90%

### æµ‹è¯•ç¤ºä¾‹

```python
import pytest
from unittest.mock import Mock, patch, AsyncMock


class TestNLSearchService:
    @pytest.fixture
    def service(self):
        """åˆ›å»ºæœåŠ¡å®ä¾‹"""
        return NLSearchService()

    @pytest.mark.asyncio
    async def test_create_search_success(self, service):
        """æµ‹è¯•æˆåŠŸåˆ›å»ºæœç´¢"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        user_id = "test_user"
        query_text = "AIæŠ€æœ¯"

        # Mock å¤–éƒ¨ä¾èµ–
        with patch.object(service.llm_processor, 'parse_query') as mock_parse:
            mock_parse.return_value = {"intent": "tech_news"}

            # æ‰§è¡Œæµ‹è¯•
            log_id = await service.create_search(user_id, query_text)

            # éªŒè¯ç»“æœ
            assert log_id is not None
            mock_parse.assert_called_once_with(query_text)

    @pytest.mark.asyncio
    async def test_create_search_with_llm_error(self, service):
        """æµ‹è¯• LLM é”™è¯¯å¤„ç†"""
        user_id = "test_user"
        query_text = "æµ‹è¯•æŸ¥è¯¢"

        # Mock LLM æŠ›å‡ºå¼‚å¸¸
        with patch.object(service.llm_processor, 'parse_query') as mock_parse:
            mock_parse.side_effect = Exception("API Error")

            # éªŒè¯å¼‚å¸¸è¢«æ­£ç¡®å¤„ç†
            with pytest.raises(Exception):
                await service.create_search(user_id, query_text)
```

---

## éƒ¨ç½²æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ç‡ >85%ï¼‰
- [ ] æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡
- [ ] åŠŸèƒ½å¼€å…³é»˜è®¤ä¸º `false`
- [ ] API æ–‡æ¡£å·²æ›´æ–°
- [ ] æ•°æ®åº“ç´¢å¼•å·²åˆ›å»º
- [ ] é…ç½®æ–‡ä»¶æ¨¡æ¿å·²å‡†å¤‡
- [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®
- [ ] å›æ»šæ–¹æ¡ˆå·²ç¡®è®¤

### éƒ¨ç½²æ­¥éª¤

1. **ä»£ç éƒ¨ç½²**
   ```bash
   git pull origin main
   pip install -r requirements.txt
   ```

2. **æ•°æ®åº“è¿ç§»**
   ```bash
   python scripts/create_nl_search_indexes.py
   ```

3. **é…ç½®æ£€æŸ¥**
   ```bash
   # ç¡®è®¤åŠŸèƒ½å¼€å…³ä¸ºå…³é—­
   grep "NL_SEARCH_ENABLED" .env
   # åº”è¯¥è¾“å‡º: NL_SEARCH_ENABLED=false
   ```

4. **é‡å¯æœåŠ¡**
   ```bash
   supervisorctl restart gunicorn
   ```

5. **å¥åº·æ£€æŸ¥**
   ```bash
   curl http://localhost:8000/api/v1/nl-search/status
   # åº”è¯¥è¿”å›: {"enabled": false, "version": "1.0.0-beta"}
   ```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¯ç”¨åŠŸèƒ½ï¼Ÿ

**A**: ä¿®æ”¹ `.env` æ–‡ä»¶ï¼Œè®¾ç½® `NL_SEARCH_ENABLED=true`ï¼Œç„¶åé‡å¯æœåŠ¡ã€‚

### Q2: å¦‚ä½•éªŒè¯åŠŸèƒ½éš”ç¦»ï¼Ÿ

**A**: è¿è¡Œéš”ç¦»æµ‹è¯•ï¼š
```bash
pytest tests/nl_search/test_isolation.py -v
```

### Q3: å¦‚ä½•å›æ»šï¼Ÿ

**A**: è®¾ç½® `NL_SEARCH_ENABLED=false`ï¼Œé‡å¯æœåŠ¡å³å¯ã€‚

### Q4: æ•°æ®å¦‚ä½•æ¸…ç†ï¼Ÿ

**A**: æ‰§è¡Œæ¸…ç†è„šæœ¬ï¼š
```bash
python scripts/cleanup_nl_search_data.py
```

---

## é™„å½•

### A. å®Œæ•´ä»£ç ç¤ºä¾‹

å®Œæ•´çš„ä»£ç ç¤ºä¾‹è¯·æŸ¥çœ‹ï¼š


- `src/services/nl_search/` - æœåŠ¡å±‚ä»£ç 
- `src/core/domain/entities/nl_search/` - å®ä½“å±‚ä»£ç 
- `tests/nl_search/` - æµ‹è¯•ä»£ç 

### B. å‚è€ƒæ–‡æ¡£

- [æ¨¡å—åŒ–è®¾è®¡æ–‡æ¡£](NL_SEARCH_MODULAR_DESIGN.md)
- [API æ–‡æ¡£](API_USAGE_GUIDE_V2.md)
- [Firecrawl é›†æˆæ–‡æ¡£](FIRECRAWL_MAP_API_GUIDE.md)

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å·²å®Œæˆ
**æœ€åæ›´æ–°**: 2025-11-10
**ç»´æŠ¤è€…**: Backend Team
