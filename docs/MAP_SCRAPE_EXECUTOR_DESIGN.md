# Map+Scrape æ‰§è¡Œå™¨è®¾è®¡æ–‡æ¡£

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-06
**çŠ¶æ€**: è®¾è®¡é˜¶æ®µ

---

## ğŸ“‹ ç›®å½•

1. [éœ€æ±‚èƒŒæ™¯](#éœ€æ±‚èƒŒæ™¯)
2. [æŠ€æœ¯æ–¹æ¡ˆ](#æŠ€æœ¯æ–¹æ¡ˆ)
3. [APIå¯¹æ¯”åˆ†æ](#apiå¯¹æ¯”åˆ†æ)
4. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
5. [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
6. [æ•°æ®åº“å…¼å®¹æ€§](#æ•°æ®åº“å…¼å®¹æ€§)
7. [ç§¯åˆ†æ¶ˆè€—è®¡ç®—](#ç§¯åˆ†æ¶ˆè€—è®¡ç®—)
8. [å®ç°è·¯çº¿å›¾](#å®ç°è·¯çº¿å›¾)
9. [æµ‹è¯•ç­–ç•¥](#æµ‹è¯•ç­–ç•¥)

---

## éœ€æ±‚èƒŒæ™¯

### ä¸šåŠ¡éœ€æ±‚

**æ ¸å¿ƒéœ€æ±‚**ï¼šå®ç°æŒ‡å®šURL + æ—¶é—´èŒƒå›´çš„ç²¾ç¡®ç½‘ç«™å†…å®¹çˆ¬å–

**å…·ä½“åœºæ™¯**ï¼š
- å®šæœŸç›‘æ§ç‰¹å®šç½‘ç«™çš„æœ€æ–°å†…å®¹
- åªçˆ¬å–ç‰¹å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ç« ï¼ˆå¦‚ï¼šæœ€è¿‘30å¤©ï¼‰
- é¿å…é‡å¤çˆ¬å–å†å²å†…å®¹ï¼ŒèŠ‚çœAPIç§¯åˆ†

**ç°æœ‰æ–¹æ¡ˆçš„å±€é™æ€§**ï¼š
- **Crawl API**ï¼šé€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼Œæ— æ³•ç²¾ç¡®æ§åˆ¶çˆ¬å–å“ªäº›é¡µé¢
- **æ—¶é—´è¿‡æ»¤æ»å**ï¼šéœ€è¦å…ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼Œå†æ ¹æ®å‘å¸ƒæ—¶é—´è¿‡æ»¤
- **ç§¯åˆ†æµªè´¹**ï¼šçˆ¬å–äº†å¤§é‡ä¸éœ€è¦çš„å†å²é¡µé¢

### æŠ€æœ¯ç›®æ ‡

1. âœ… **ç²¾ç¡®URLå‘ç°**ï¼šä½¿ç”¨Map APIå¿«é€Ÿè·å–ç½‘ç«™æ‰€æœ‰URL
2. âœ… **æŒ‰éœ€çˆ¬å–**ï¼šåªçˆ¬å–ç¬¦åˆæ—¶é—´èŒƒå›´çš„é¡µé¢
3. âœ… **èŠ‚çœç§¯åˆ†**ï¼šé¿å…ä¸å¿…è¦çš„é¡µé¢çˆ¬å–
4. âœ… **æ•°æ®å…¼å®¹**ï¼šä¿æŒæ•°æ®åº“å­—æ®µç»“æ„ä¸å˜
5. âœ… **å¤‡ç”¨æ–¹æ¡ˆ**ï¼šä¿ç•™Crawl APIä½œä¸ºfallback

---

## æŠ€æœ¯æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¦‚è¿°

**æ–°æ‰§è¡Œå™¨**ï¼š`MapScrapeExecutor`
**ä»»åŠ¡ç±»å‹**ï¼š`TaskType.MAP_SCRAPE_WEBSITE = "map_scrape_website"`
**æ ¸å¿ƒæµç¨‹**ï¼šMap API â†’ Batch Scrape â†’ æ—¶é—´è¿‡æ»¤ â†’ ä¿å­˜ç»“æœ

### æ‰§è¡Œæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Map API - å‘ç°æ‰€æœ‰URL                                 â”‚
â”‚  è¾“å…¥: èµ·å§‹URL, searchå‚æ•°ï¼ˆå¯é€‰ï¼‰                             â”‚
â”‚  è¾“å‡º: URLåˆ—è¡¨ + å…ƒæ•°æ®ï¼ˆtitle, descriptionï¼‰                 â”‚
â”‚  æ—¶é—´: ~5ç§’                                                   â”‚
â”‚  ç§¯åˆ†: 1 credit                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: Batch Scrape - æ‰¹é‡çˆ¬å–å†…å®¹                           â”‚
â”‚  è¾“å…¥: URLåˆ—è¡¨                                                 â”‚
â”‚  å¤„ç†: å¹¶å‘scrapeï¼ˆSemaphoreæ§åˆ¶å¹¶å‘æ•°ï¼‰                       â”‚
â”‚  è¾“å‡º: æ¯ä¸ªURLçš„å®Œæ•´å†…å®¹ + metadataï¼ˆå«publishedDateï¼‰        â”‚
â”‚  æ—¶é—´: ~N*2ç§’ï¼ˆN=URLæ•°é‡ï¼Œè€ƒè™‘å¹¶å‘ï¼‰                           â”‚
â”‚  ç§¯åˆ†: N credits                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: æ—¶é—´è¿‡æ»¤ - ç­›é€‰ç¬¦åˆæ¡ä»¶çš„å†…å®¹                          â”‚
â”‚  è¾“å…¥: æ‰€æœ‰scrapeç»“æœ                                          â”‚
â”‚  è¿‡æ»¤: metadata.publishedDateåœ¨[start_date, end_date]èŒƒå›´å†…   â”‚
â”‚  è¾“å‡º: ç¬¦åˆæ—¶é—´æ¡ä»¶çš„ç»“æœåˆ—è¡¨                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ4: æ•°æ®ä¿å­˜                                               â”‚
â”‚  - è½¬æ¢ä¸ºSearchResultå®ä½“                                     â”‚
â”‚  - ä¿å­˜åˆ°search_resultsé›†åˆ                                   â”‚
â”‚  - ä¿å­˜åŸå§‹å“åº”åˆ°firecrawl_raw_responses                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸Crawl APIçš„å¯¹æ¯”

| ç‰¹æ€§ | Crawl API | Map + Scrape API |
|------|-----------|------------------|
| **URLå‘ç°** | é€’å½’çˆ¬å– | Map APIä¸€æ¬¡æ€§è·å– |
| **é€Ÿåº¦** | è¾ƒæ…¢ï¼ˆéœ€è¦é€’å½’ï¼‰ | è¾ƒå¿«ï¼ˆå¹¶å‘scrapeï¼‰ |
| **æ—¶é—´è¿‡æ»¤** | çˆ¬å–åè¿‡æ»¤ | å¯æå‰è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰ |
| **ç§¯åˆ†æ¶ˆè€—** | æŒ‰çˆ¬å–é¡µé¢æ•° | Map(1) + Scrape(N) |
| **é€‚ç”¨åœºæ™¯** | å®Œæ•´å½’æ¡£ | ç²¾ç¡®ç›®æ ‡çˆ¬å– |
| **æ§åˆ¶ç²¾åº¦** | è·¯å¾„è¿‡æ»¤ | URLçº§åˆ«è¿‡æ»¤ |

---

## APIå¯¹æ¯”åˆ†æ

### Firecrawl Map API

**ç«¯ç‚¹**: `POST /v2/map`

**åŠŸèƒ½**ï¼š
- å‘ç°ç½‘ç«™çš„æ‰€æœ‰å¯è®¿é—®URL
- å¯é€‰searchå‚æ•°è¿›è¡Œå…³é”®è¯è¿‡æ»¤
- è¿”å›URLåˆ—è¡¨åŠåŸºæœ¬å…ƒæ•°æ®

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://example.com",
      "search": "blog",
      "limit": 5000
    }'
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
{
  "success": true,
  "links": [
    {
      "url": "https://example.com/blog/post-1",
      "title": "Post 1 Title",
      "description": "Post 1 description"
    },
    {
      "url": "https://example.com/blog/post-2",
      "title": "Post 2 Title",
      "description": "Post 2 description"
    }
  ]
}
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- âœ… å¿«é€Ÿï¼ˆé€šå¸¸<5ç§’ï¼‰
- âœ… å‡†ç¡®ï¼ˆä½¿ç”¨sitemapå’Œæ™ºèƒ½çˆ¬å–ï¼‰
- âœ… å›ºå®šæˆæœ¬ï¼ˆ1 creditï¼‰
- âŒ ä¸åŒ…å«é¡µé¢å†…å®¹
- âŒ ä¸åŒ…å«å‘å¸ƒæ—¶é—´

### Firecrawl Scrape API

**ç«¯ç‚¹**: `POST /v2/scrape`

**åŠŸèƒ½**ï¼š
- çˆ¬å–å•ä¸ªURLçš„å®Œæ•´å†…å®¹
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
- æå–metadataï¼ˆåŒ…å«publishedDateï¼‰

**æ‰¹é‡Scrapeç­–ç•¥**ï¼š
```python
async def batch_scrape(urls: List[str], max_concurrent: int = 5):
    semaphore = asyncio.Semaphore(max_concurrent)

    async def scrape_with_limit(url):
        async with semaphore:
            return await scrape_adapter.scrape(url)

    tasks = [scrape_with_limit(url) for url in urls]
    return await asyncio.gather(*tasks, return_exceptions=True)
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- âœ… å®Œæ•´å†…å®¹ï¼ˆmarkdown, htmlï¼‰
- âœ… å…ƒæ•°æ®å®Œæ•´ï¼ˆå«publishedDateï¼‰
- âœ… æ”¯æŒå¹¶å‘
- âš ï¸ æŒ‰URLè®¡è´¹ï¼ˆN creditsï¼‰

---

## æ¶æ„è®¾è®¡

### é…ç½®ç±»è®¾è®¡

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, List

@dataclass
class MapScrapeConfig:
    """Map + Scrape æ‰§è¡Œå™¨é…ç½®"""

    # Map API é…ç½®
    search: Optional[str] = None          # æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
    map_limit: int = 5000                 # Map APIè¿”å›URLæ•°é‡é™åˆ¶

    # æ—¶é—´è¿‡æ»¤é…ç½®
    start_date: Optional[datetime] = None # å¼€å§‹æ—¥æœŸï¼ˆåŒ…å«ï¼‰
    end_date: Optional[datetime] = None   # ç»“æŸæ—¥æœŸï¼ˆåŒ…å«ï¼‰

    # Scrape API é…ç½®
    max_concurrent_scrapes: int = 5       # æœ€å¤§å¹¶å‘scrapeæ•°
    scrape_delay: float = 0.5             # scrapeé—´éš”ï¼ˆç§’ï¼‰
    only_main_content: bool = True        # åªæå–ä¸»è¦å†…å®¹
    exclude_tags: List[str] = field(      # æ’é™¤çš„HTMLæ ‡ç­¾
        default_factory=lambda: ["nav", "footer", "header", "aside"]
    )
    timeout: int = 90                     # å•ä¸ªscrapeè¶…æ—¶ï¼ˆç§’ï¼‰

    # é”™è¯¯å¤„ç†
    allow_partial_failure: bool = True    # å…è®¸éƒ¨åˆ†scrapeå¤±è´¥
    min_success_rate: float = 0.8         # æœ€ä½æˆåŠŸç‡ï¼ˆ80%ï¼‰
```

### æ‰§è¡Œå™¨ç±»è®¾è®¡

```python
class MapScrapeExecutor(TaskExecutor):
    """Map + Scrape ä»»åŠ¡æ‰§è¡Œå™¨

    é€‚ç”¨äºéœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–URLå’Œæ—¶é—´èŒƒå›´çš„åœºæ™¯
    """

    def __init__(self):
        super().__init__()
        self.firecrawl_adapter = FirecrawlAdapter()

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡ŒMap + Scrapeä»»åŠ¡"""
        start_time = datetime.utcnow()

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_map_scrape_config(task.crawl_config)

        # 3. é˜¶æ®µ1: Map - å‘ç°URL
        urls = await self._execute_map(task.crawl_url, config)
        self.logger.info(f"ğŸ—ºï¸  Mapå‘ç° {len(urls)} ä¸ªURL")

        # 4. é˜¶æ®µ2: Batch Scrape - çˆ¬å–å†…å®¹
        scrape_results = await self._batch_scrape(urls, config)
        self.logger.info(f"âœ… Scrapeå®Œæˆ {len(scrape_results)} ä¸ªé¡µé¢")

        # 5. é˜¶æ®µ3: æ—¶é—´è¿‡æ»¤
        filtered_results = self._filter_by_date(scrape_results, config)
        self.logger.info(f"ğŸ” æ—¶é—´è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} ä¸ªç»“æœ")

        # 6. ä¿å­˜åŸå§‹å“åº”
        await self._save_raw_responses(scrape_results, task)

        # 7. è½¬æ¢ä¸ºSearchResult
        search_results = self._convert_to_search_results(
            filtered_results, task
        )

        # 8. åˆ›å»ºç»“æœæ‰¹æ¬¡
        batch = self._create_result_batch(task, query=f"Map+Scrape: {task.crawl_url}")
        for result in search_results:
            batch.add_result(result)

        # 9. è®¡ç®—ç§¯åˆ†æ¶ˆè€—
        batch.credits_used = FirecrawlCreditsCalculator.calculate_map_scrape_credits(
            map_calls=1,
            urls_scraped=len(scrape_results)
        )

        # 10. è®¡ç®—æ‰§è¡Œæ—¶é—´
        batch.execution_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

        self._log_execution_end(task, len(search_results), batch.execution_time_ms)
        return batch

    async def _execute_map(self, url: str, config: MapScrapeConfig) -> List[str]:
        """æ‰§è¡ŒMap APIè°ƒç”¨"""
        pass

    async def _batch_scrape(self, urls: List[str], config: MapScrapeConfig) -> List[CrawlResult]:
        """æ‰¹é‡scrape URL"""
        pass

    def _filter_by_date(self, results: List[CrawlResult], config: MapScrapeConfig) -> List[CrawlResult]:
        """æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤ç»“æœ"""
        pass
```

---

## å®ç°ç»†èŠ‚

### 1. FirecrawlAdapteræ‰©å±•

**æ–°å¢map()æ–¹æ³•**ï¼š

```python
async def map(
    self,
    url: str,
    search: Optional[str] = None,
    limit: int = 5000
) -> List[Dict[str, Any]]:
    """è°ƒç”¨Firecrawl Map API

    Args:
        url: èµ·å§‹URL
        search: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›URLæ•°é‡é™åˆ¶

    Returns:
        List[Dict]: [
            {"url": "...", "title": "...", "description": "..."},
            ...
        ]

    Raises:
        MapAPIError: Map APIè°ƒç”¨å¤±è´¥
    """
    payload = {
        "url": url,
        "limit": limit
    }

    if search:
        payload["search"] = search

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {self.api_key}"
    }

    try:
        response = await self.client.post(
            f"{self.base_url}/v2/map",
            json=payload,
            headers=headers,
            timeout=30
        )
        response.raise_for_status()

        data = response.json()
        if not data.get("success"):
            raise MapAPIError(f"Map APIè¿”å›å¤±è´¥: {data}")

        links = data.get("links", [])
        self.logger.info(f"âœ… Map APIè¿”å› {len(links)} ä¸ªURL")

        return links

    except Exception as e:
        self.logger.error(f"âŒ Map APIè°ƒç”¨å¤±è´¥: {e}")
        raise MapAPIError(f"Map APIè°ƒç”¨å¤±è´¥: {str(e)}")
```

### 2. æ‰¹é‡Scrapeå®ç°

**å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†**ï¼š

```python
async def _batch_scrape(
    self,
    urls: List[str],
    config: MapScrapeConfig
) -> List[CrawlResult]:
    """æ‰¹é‡scrape URLï¼ˆå¸¦å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†ï¼‰

    Args:
        urls: URLåˆ—è¡¨
        config: Scrapeé…ç½®

    Returns:
        List[CrawlResult]: æˆåŠŸscrapeçš„ç»“æœåˆ—è¡¨
    """
    semaphore = asyncio.Semaphore(config.max_concurrent_scrapes)
    results = []
    failed_urls = []

    async def scrape_with_limit(url: str, index: int) -> Optional[CrawlResult]:
        async with semaphore:
            try:
                self.logger.info(f"ğŸ” [{index+1}/{len(urls)}] Scraping: {url}")

                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                if index > 0:
                    await asyncio.sleep(config.scrape_delay)

                # æ‰§è¡Œscrape
                result = await self.firecrawl_adapter.scrape(
                    url,
                    only_main_content=config.only_main_content,
                    exclude_tags=config.exclude_tags,
                    timeout=config.timeout
                )

                return result

            except Exception as e:
                self.logger.warning(f"âš ï¸  Scrapeå¤±è´¥ {url}: {e}")
                failed_urls.append(url)
                return None

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰scrape
    tasks = [scrape_with_limit(url, i) for i, url in enumerate(urls)]
    scrape_results = await asyncio.gather(*tasks)

    # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
    results = [r for r in scrape_results if r is not None]

    # æ£€æŸ¥æˆåŠŸç‡
    success_rate = len(results) / len(urls) if urls else 0
    self.logger.info(
        f"ğŸ“Š Scrapeç»Ÿè®¡: æˆåŠŸ={len(results)}, å¤±è´¥={len(failed_urls)}, "
        f"æˆåŠŸç‡={success_rate*100:.1f}%"
    )

    # éªŒè¯æœ€ä½æˆåŠŸç‡
    if not config.allow_partial_failure and success_rate < config.min_success_rate:
        raise ExecutionError(
            f"ScrapeæˆåŠŸç‡è¿‡ä½: {success_rate*100:.1f}% < "
            f"{config.min_success_rate*100:.1f}%"
        )

    return results
```

### 3. æ—¶é—´è¿‡æ»¤å®ç°

```python
def _filter_by_date(
    self,
    results: List[CrawlResult],
    config: MapScrapeConfig
) -> List[CrawlResult]:
    """æ ¹æ®å‘å¸ƒæ—¶é—´è¿‡æ»¤ç»“æœ

    Args:
        results: Scrapeç»“æœåˆ—è¡¨
        config: é…ç½®ï¼ˆåŒ…å«start_dateå’Œend_dateï¼‰

    Returns:
        List[CrawlResult]: ç¬¦åˆæ—¶é—´èŒƒå›´çš„ç»“æœ
    """
    # å¦‚æœæ²¡æœ‰é…ç½®æ—¶é—´èŒƒå›´ï¼Œè¿”å›æ‰€æœ‰ç»“æœ
    if not config.start_date and not config.end_date:
        return results

    filtered = []

    for result in results:
        # æå–å‘å¸ƒæ—¶é—´
        metadata = result.metadata or {}
        published_date_str = (
            metadata.get('publishedDate') or
            metadata.get('published_date') or
            metadata.get('article:published_time')
        )

        if not published_date_str:
            # æ²¡æœ‰å‘å¸ƒæ—¶é—´çš„é¡µé¢ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿ç•™
            self.logger.debug(f"âš ï¸  {result.url} æ— å‘å¸ƒæ—¶é—´")
            continue

        try:
            # è§£æå‘å¸ƒæ—¶é—´
            published_date = datetime.fromisoformat(published_date_str)

            # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            if config.start_date and published_date < config.start_date:
                continue

            if config.end_date and published_date > config.end_date:
                continue

            # ç¬¦åˆæ¡ä»¶
            filtered.append(result)

        except Exception as e:
            self.logger.warning(f"âš ï¸  è§£æå‘å¸ƒæ—¶é—´å¤±è´¥ {result.url}: {e}")
            continue

    return filtered
```

---

## æ•°æ®åº“å…¼å®¹æ€§

### SearchResultå­—æ®µæ˜ å°„

**å®Œå…¨å…¼å®¹ç°æœ‰å­—æ®µç»“æ„**ï¼š

```python
def _convert_to_search_results(
    self,
    scrape_results: List[CrawlResult],
    task: SearchTask
) -> List[SearchResult]:
    """è½¬æ¢ä¸ºSearchResultï¼ˆä¸ç°æœ‰ç»“æ„å®Œå…¨å…¼å®¹ï¼‰"""

    search_results = []

    for idx, result in enumerate(scrape_results, start=1):
        # æå–å…ƒæ•°æ®
        metadata_dict = result.metadata if isinstance(result.metadata, dict) else {}
        metadata_fields = self._extract_metadata_fields(metadata_dict)

        # åˆ›å»ºSearchResultï¼ˆå­—æ®µå®Œå…¨ç›¸åŒï¼‰
        search_result = SearchResult(
            task_id=str(task.id),
            title=metadata_dict.get("title", result.url),
            url=result.url,
            snippet=result.content[:200] if result.content else "",
            source="map_scrape",  # æ–°çš„sourceæ ‡è¯†

            # å…ƒæ•°æ®å­—æ®µï¼ˆå®Œå…¨ç›¸åŒï¼‰
            published_date=metadata_fields.get('published_date'),
            author=metadata_fields.get('author'),
            language=metadata_fields.get('language'),
            article_tag=metadata_fields.get('article_tag'),
            article_published_time=metadata_fields.get('article_published_time'),
            source_url=metadata_fields.get('source_url'),
            http_status_code=metadata_fields.get('http_status_code'),

            search_position=idx,
            markdown_content=result.markdown if result.markdown else result.content,
            html_content=result.html,
            metadata={},  # ä¸å†ä¼ é€’metadataï¼Œå­—æ®µå·²ç‹¬ç«‹
            relevance_score=1.0,
            status=ResultStatus.PENDING
        )

        search_results.append(search_result)

    return search_results
```

**æ•°æ®åº“é›†åˆ**ï¼š
- âœ… `search_results`: ä¿å­˜SearchResultå®ä½“
- âœ… `firecrawl_raw_responses`: ä¿å­˜åŸå§‹APIå“åº”
- âœ… å­—æ®µç»“æ„æ— ä»»ä½•å˜åŒ–

---

## ç§¯åˆ†æ¶ˆè€—è®¡ç®—

### è®¡ç®—é€»è¾‘

```python
class FirecrawlCreditsCalculator:
    """Firecrawlç§¯åˆ†æ¶ˆè€—è®¡ç®—å™¨"""

    @staticmethod
    def calculate_map_scrape_credits(
        map_calls: int,
        urls_scraped: int
    ) -> int:
        """è®¡ç®—Map + Scrapeæ“ä½œçš„ç§¯åˆ†æ¶ˆè€—

        Args:
            map_calls: Map APIè°ƒç”¨æ¬¡æ•°ï¼ˆé€šå¸¸ä¸º1ï¼‰
            urls_scraped: Scrapeçš„URLæ•°é‡

        Returns:
            int: æ€»ç§¯åˆ†æ¶ˆè€—
        """
        map_cost = map_calls * 1  # Map API: 1 credit per call
        scrape_cost = urls_scraped * 1  # Scrape API: 1 credit per URL

        total = map_cost + scrape_cost

        return total
```

### æˆæœ¬å¯¹æ¯”ç¤ºä¾‹

**åœºæ™¯**: çˆ¬å–åšå®¢ç½‘ç«™ï¼Œåªéœ€è¦æœ€è¿‘30å¤©çš„100ç¯‡æ–‡ç« 

**Crawl APIæ–¹å¼**ï¼š
```
- ç½‘ç«™æ€»é¡µé¢: 1000é¡µ
- çˆ¬å–æ–¹å¼: é€’å½’çˆ¬å–æ‰€æœ‰é¡µé¢
- ç§¯åˆ†æ¶ˆè€—: 500-1000 creditsï¼ˆéœ€è¦çˆ¬å–å¾ˆå¤šå†å²é¡µé¢ï¼‰
- æ—¶é—´: ~20-30åˆ†é’Ÿ
```

**Map + Scrapeæ–¹å¼**ï¼š
```
- Map API: å‘ç°æ‰€æœ‰URL (1 credit)
- Scrape API: åªscrape 100ä¸ªç¬¦åˆæ—¶é—´çš„é¡µé¢ (100 credits)
- æ€»ç§¯åˆ†: 101 credits
- æ—¶é—´: ~5-10åˆ†é’Ÿ
- èŠ‚çœ: ~80-90%
```

---

## å®ç°è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŠŸèƒ½ï¼ˆ1-2å¤©ï¼‰

- [x] æ‰©å±•FirecrawlAdapterï¼šæ–°å¢map()æ–¹æ³•
- [x] åˆ›å»ºMapScrapeConfigé…ç½®ç±»
- [x] å®ç°MapScrapeExecutoråŸºç¡€æ¡†æ¶
- [x] å®ç°_execute_map()æ–¹æ³•
- [x] å®ç°_batch_scrape()æ–¹æ³•
- [ ] å•å…ƒæµ‹è¯•

### Phase 2: æ—¶é—´è¿‡æ»¤ï¼ˆ1å¤©ï¼‰

- [ ] å®ç°_filter_by_date()æ–¹æ³•
- [ ] æ”¯æŒstart_dateå’Œend_dateé…ç½®
- [ ] å¤„ç†æ— å‘å¸ƒæ—¶é—´çš„é¡µé¢
- [ ] æ—¥æœŸè§£æå®¹é”™
- [ ] é›†æˆæµ‹è¯•

### Phase 3: é”™è¯¯å¤„ç†å’Œä¼˜åŒ–ï¼ˆ1å¤©ï¼‰

- [ ] å¹¶å‘æ§åˆ¶ä¼˜åŒ–
- [ ] é‡è¯•æœºåˆ¶
- [ ] éƒ¨åˆ†å¤±è´¥å®¹å¿
- [ ] æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
- [ ] å‹åŠ›æµ‹è¯•

### Phase 4: é›†æˆå’Œæ–‡æ¡£ï¼ˆ1å¤©ï¼‰

- [ ] æ›´æ–°TaskTypeæšä¸¾
- [ ] åœ¨ExecutorFactoryæ³¨å†Œ
- [ ] æ›´æ–°ç§¯åˆ†è®¡ç®—å™¨
- [ ] å®Œå–„æŠ€æœ¯æ–‡æ¡£
- [ ] åˆ›å»ºä½¿ç”¨ç¤ºä¾‹

### Phase 5: æµ‹è¯•å’Œä¸Šçº¿ï¼ˆ1å¤©ï¼‰

- [ ] ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] çœŸå®åœºæ™¯éªŒè¯
- [ ] æ€§èƒ½å¯¹æ¯”åˆ†æ
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] ç›‘æ§é…ç½®

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_map_scrape_executor.py

class TestMapScrapeExecutor:

    async def test_execute_map(self):
        """æµ‹è¯•Map APIè°ƒç”¨"""
        executor = MapScrapeExecutor()
        config = MapScrapeConfig()

        urls = await executor._execute_map("https://example.com", config)

        assert len(urls) > 0
        assert all(url.startswith("http") for url in urls)

    async def test_batch_scrape(self):
        """æµ‹è¯•æ‰¹é‡Scrape"""
        executor = MapScrapeExecutor()
        config = MapScrapeConfig(max_concurrent_scrapes=3)

        urls = ["https://example.com/page1", "https://example.com/page2"]
        results = await executor._batch_scrape(urls, config)

        assert len(results) == 2
        assert all(r.markdown is not None for r in results)

    def test_filter_by_date(self):
        """æµ‹è¯•æ—¶é—´è¿‡æ»¤"""
        executor = MapScrapeExecutor()
        config = MapScrapeConfig(
            start_date=datetime(2025, 1, 1),
            end_date=datetime(2025, 12, 31)
        )

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        results = [...]  # åŒ…å«ä¸åŒå‘å¸ƒæ—¶é—´çš„ç»“æœ

        filtered = executor._filter_by_date(results, config)

        # éªŒè¯è¿‡æ»¤ç»“æœ
        assert all(
            config.start_date <= r.metadata.get('published_date') <= config.end_date
            for r in filtered
        )
```

### é›†æˆæµ‹è¯•

```python
# tests/integration/test_map_scrape_integration.py

async def test_full_map_scrape_workflow():
    """æµ‹è¯•å®Œæ•´çš„Map+Scrapeå·¥ä½œæµ"""

    # åˆ›å»ºä»»åŠ¡
    task = SearchTask(
        name="æµ‹è¯•åšå®¢çˆ¬å–",
        crawl_url="https://example.com/blog",
        task_type=TaskType.MAP_SCRAPE_WEBSITE,
        crawl_config={
            "search": "python",
            "start_date": "2025-01-01",
            "end_date": "2025-12-31",
            "max_concurrent_scrapes": 3
        }
    )

    # æ‰§è¡Œ
    executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
    batch = await executor.execute(task)

    # éªŒè¯
    assert batch.returned_count > 0
    assert batch.credits_used > 0
    assert all(r.source == "map_scrape" for r in batch.results)
```

---

## æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

1. **ç²¾ç¡®æ§åˆ¶**ï¼šMap APIæä¾›ç²¾ç¡®çš„URLå‘ç°èƒ½åŠ›
2. **æˆæœ¬ä¼˜åŒ–**ï¼šåªçˆ¬å–éœ€è¦çš„é¡µé¢ï¼ŒèŠ‚çœ80-90%ç§¯åˆ†
3. **æ—¶é—´è¿‡æ»¤**ï¼šæ”¯æŒå‘å¸ƒæ—¶é—´èŒƒå›´è¿‡æ»¤
4. **æ•°æ®å…¼å®¹**ï¼šå®Œå…¨å…¼å®¹ç°æœ‰æ•°æ®åº“ç»“æ„
5. **å¤‡ç”¨æ–¹æ¡ˆ**ï¼šä¿ç•™Crawl APIä½œä¸ºfallback

### é€‚ç”¨åœºæ™¯

- âœ… å®šæœŸç›‘æ§ç‰¹å®šç½‘ç«™æœ€æ–°å†…å®¹
- âœ… åªéœ€è¦ç‰¹å®šæ—¶é—´èŒƒå›´çš„æ–‡ç« 
- âœ… ç½‘ç«™æœ‰æ˜ç¡®çš„URLç»“æ„
- âœ… éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡

### ä¸é€‚ç”¨åœºæ™¯

- âŒ éœ€è¦å®Œæ•´å½’æ¡£æ•´ä¸ªç½‘ç«™
- âŒ ç½‘ç«™URLç»“æ„ä¸è§„åˆ™
- âŒ æ— æ³•é€šè¿‡Map APIå‘ç°æ‰€æœ‰é¡µé¢
- âŒ ä¸å…³å¿ƒç§¯åˆ†æˆæœ¬

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Development Team
**æœ€åæ›´æ–°**: 2025-11-06
