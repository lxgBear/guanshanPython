# Firecrawl é›†æˆå®Œæ•´æŒ‡å—

> **æ–‡æ¡£çŠ¶æ€**: ç»Ÿä¸€æŒ‡å— (åˆå¹¶è‡ª FIRECRAWL_INTEGRATION.md å’Œ FIRECRAWL_API_CONFIGURATION.md)
> **æœ€åæ›´æ–°**: 2025-10-14
> **APIç‰ˆæœ¬**: v2 (å·²å‡çº§å®Œæˆ)

---

## ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#1-å¿«é€Ÿå¼€å§‹)
2. [APIé…ç½®](#2-apié…ç½®)
3. [æµ‹è¯•æ¨¡å¼ä¸ç”Ÿäº§æ¨¡å¼](#3-æµ‹è¯•æ¨¡å¼ä¸ç”Ÿäº§æ¨¡å¼)
4. [é›†æˆæ¶æ„è®¾è®¡](#4-é›†æˆæ¶æ„è®¾è®¡)
5. [æ ¸å¿ƒå®ç°](#5-æ ¸å¿ƒå®ç°)
6. [é«˜çº§åŠŸèƒ½](#6-é«˜çº§åŠŸèƒ½)
7. [æ•…éšœæ’æŸ¥](#7-æ•…éšœæ’æŸ¥)
8. [æˆæœ¬ä¼˜åŒ–](#8-æˆæœ¬ä¼˜åŒ–)
9. [å®‰å…¨ä¸ç›‘æ§](#9-å®‰å…¨ä¸ç›‘æ§)
10. [æµ‹è¯•ç­–ç•¥](#10-æµ‹è¯•ç­–ç•¥)
11. [API v2å‡çº§è®°å½•](#11-api-v2å‡çº§è®°å½•)

---

## 1. å¿«é€Ÿå¼€å§‹

### 1.1 å½“å‰çŠ¶æ€

âœ… **å·²å®Œæˆ**: Firecrawl API v2 å‡çº§
âœ… **APIå¯†é’¥**: å·²é…ç½®åˆ° `.env`
âœ… **æµ‹è¯•é€šè¿‡**: åŸºæœ¬çˆ¬å–åŠŸèƒ½æ­£å¸¸
âœ… **é›†æˆå°±ç»ª**: å¯ç«‹å³åœ¨é¡¹ç›®ä¸­ä½¿ç”¨

### 1.2 è·å–APIå¯†é’¥

#### æ–¹æ³•1: å®˜ç½‘æ³¨å†Œ (æ¨è)
1. è®¿é—® [Firecrawlå®˜ç½‘](https://firecrawl.dev)
2. æ³¨å†Œè´¦å·å¹¶ç™»å½•
3. è¿›å…¥Dashboard â†’ API Keys
4. åˆ›å»ºæ–°çš„APIå¯†é’¥å¹¶å¤åˆ¶

#### æ–¹æ³•2: ä½¿ç”¨æµ‹è¯•å¯†é’¥
```bash
# ç”¨äºå¼€å‘å’Œæµ‹è¯• (æœ‰è¯·æ±‚é™åˆ¶)
FIRECRAWL_API_KEY=fc-test-your-test-key-here
```

### 1.3 åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

```python
from firecrawl import FirecrawlApp

# åˆå§‹åŒ–å®¢æˆ·ç«¯
app = FirecrawlApp(api_key="your-api-key")

# æŠ“å–å•ä¸ªé¡µé¢
result = app.scrape(
    url="https://example.com",
    formats=['markdown', 'html']
)

print(result['markdown'])
```

---

## 2. APIé…ç½®

### 2.1 ç¯å¢ƒå˜é‡é…ç½®

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºæˆ–ç¼–è¾‘ `.env` æ–‡ä»¶:

```bash
# Firecrawl API Configuration
FIRECRAWL_API_KEY=fc-your-api-key-here
FIRECRAWL_API_URL=https://api.firecrawl.dev/v2
FIRECRAWL_TIMEOUT=30
FIRECRAWL_MAX_RETRIES=3

# å¯é€‰é…ç½®
FIRECRAWL_TEST_MODE=false
FIRECRAWL_LOG_LEVEL=INFO
```

### 2.2 é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¿…éœ€ |
|------|------|--------|------|
| `FIRECRAWL_API_KEY` | APIå¯†é’¥ | - | âœ… |
| `FIRECRAWL_API_URL` | APIç«¯ç‚¹URL | `https://api.firecrawl.dev/v2` | âŒ |
| `FIRECRAWL_TIMEOUT` | è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) | `30` | âŒ |
| `FIRECRAWL_MAX_RETRIES` | æœ€å¤§é‡è¯•æ¬¡æ•° | `3` | âŒ |
| `FIRECRAWL_TEST_MODE` | æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ¨¡å¼ | `false` | âŒ |

### 2.3 ä»£ç ä¸­ä½¿ç”¨é…ç½®

```python
from src.config import settings

# ä»é…ç½®ä¸­è¯»å–APIå¯†é’¥
api_key = settings.FIRECRAWL_API_KEY
api_url = settings.FIRECRAWL_API_URL

# åˆå§‹åŒ–Firecrawlå®¢æˆ·ç«¯
app = FirecrawlApp(
    api_key=api_key,
    api_url=api_url
)
```

---

## 3. æµ‹è¯•æ¨¡å¼ä¸ç”Ÿäº§æ¨¡å¼

### 3.1 æµ‹è¯•æ¨¡å¼

**ä½¿ç”¨åœºæ™¯**: å¼€å‘å’Œæµ‹è¯•é˜¶æ®µ

```bash
# .envé…ç½®
FIRECRAWL_TEST_MODE=true
FIRECRAWL_API_KEY=fc-test-your-test-key
```

**ç‰¹ç‚¹**:
- âœ… æ— éœ€ä»˜è´¹APIå¯†é’¥
- âš ï¸ æ¯æ—¥è¯·æ±‚é™åˆ¶è¾ƒä½ (é€šå¸¸100-500æ¬¡)
- âš ï¸ é€Ÿåº¦å¯èƒ½è¾ƒæ…¢
- âš ï¸ ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ

### 3.2 ç”Ÿäº§æ¨¡å¼

**ä½¿ç”¨åœºæ™¯**: æ­£å¼ç¯å¢ƒéƒ¨ç½²

```bash
# .envé…ç½®
FIRECRAWL_TEST_MODE=false
FIRECRAWL_API_KEY=fc-prod-your-production-key
```

**ç‰¹ç‚¹**:
- âœ… æ›´é«˜çš„è¯·æ±‚é…é¢
- âœ… æ›´å¿«çš„å“åº”é€Ÿåº¦
- âœ… ç¨³å®šå¯é 
- âš ï¸ éœ€è¦ä»˜è´¹è®¢é˜…

### 3.3 æ¨¡å¼åˆ‡æ¢

```python
from src.config import settings

if settings.FIRECRAWL_TEST_MODE:
    print("âš ï¸ å½“å‰è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼")
    # ä½¿ç”¨æµ‹è¯•å¯†é’¥
    api_key = settings.FIRECRAWL_TEST_API_KEY
else:
    print("âœ… å½“å‰è¿è¡Œåœ¨ç”Ÿäº§æ¨¡å¼")
    # ä½¿ç”¨ç”Ÿäº§å¯†é’¥
    api_key = settings.FIRECRAWL_API_KEY
```

---

## 4. é›†æˆæ¶æ„è®¾è®¡

### 4.1 å…­è¾¹å½¢æ¶æ„ (ç«¯å£-é€‚é…å™¨æ¨¡å¼)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Application Layer (åº”ç”¨å±‚)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  CrawlerApplicationService         â”‚ â”‚
â”‚  â”‚  - ä¸šåŠ¡é€»è¾‘åè°ƒ                     â”‚ â”‚
â”‚  â”‚  - ç”¨ä¾‹å®ç°                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“           â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Domain Layer (é¢†åŸŸå±‚)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  CrawlerInterface (ç«¯å£)            â”‚ â”‚
â”‚  â”‚  - scrape() / crawl() / search()   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“           â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Infrastructure Layer (åŸºç¡€è®¾æ–½å±‚)    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FirecrawlAdapter (é€‚é…å™¨)          â”‚ â”‚
â”‚  â”‚  - å®ç°CrawlerInterface             â”‚ â”‚
â”‚  â”‚  - è°ƒç”¨Firecrawl API                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒç»„ä»¶èŒè´£

#### 4.2.1 é¢†åŸŸæ¥å£ (CrawlerInterface)

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœç»Ÿä¸€æ•°æ®ç»“æ„"""
    url: str
    markdown: str
    html: str
    metadata: Dict[str, Any]
    success: bool
    error: str | None = None

class CrawlerInterface(ABC):
    """çˆ¬è™«æŠ½è±¡æ¥å£"""

    @abstractmethod
    async def scrape(self, url: str, **options) -> CrawlResult:
        """æŠ“å–å•ä¸ªURL"""
        pass

    @abstractmethod
    async def crawl(self, url: str, **options) -> List[CrawlResult]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™"""
        pass

    @abstractmethod
    async def search(self, query: str, **options) -> List[CrawlResult]:
        """æœç´¢æ¨¡å¼çˆ¬å–"""
        pass
```

#### 4.2.2 Firecrawlé€‚é…å™¨ (FirecrawlAdapter)

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class FirecrawlAdapter(CrawlerInterface):
    """Firecrawlçˆ¬è™«é€‚é…å™¨"""

    def __init__(self, api_key: str, api_url: str | None = None):
        self.client = FirecrawlApp(api_key=api_key, api_url=api_url)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def scrape(self, url: str, **options) -> CrawlResult:
        """æŠ“å–å•ä¸ªé¡µé¢ (è‡ªåŠ¨é‡è¯•)"""
        try:
            scrape_options = {
                'formats': options.get('formats', ['markdown', 'html']),
                'includeTags': options.get('include_tags', []),
                'excludeTags': options.get('exclude_tags', ['nav', 'footer']),
                'waitFor': options.get('wait_for', 1000),
                'actions': options.get('actions', [])
            }

            result = await self.client.scrape(url, **scrape_options)

            return CrawlResult(
                url=url,
                markdown=result.get('markdown', ''),
                html=result.get('html', ''),
                metadata=result.get('metadata', {}),
                success=True
            )
        except Exception as e:
            return CrawlResult(
                url=url,
                markdown='',
                html='',
                metadata={},
                success=False,
                error=str(e)
            )

    async def crawl(self, url: str, **options) -> List[CrawlResult]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™"""
        crawl_options = {
            'limit': options.get('limit', 100),
            'scrapeOptions': {
                'formats': ['markdown', 'html'],
                'excludeTags': ['nav', 'footer', 'aside']
            }
        }

        results = []
        async for page in self.client.crawl(url, **crawl_options):
            results.append(CrawlResult(
                url=page.get('url', ''),
                markdown=page.get('markdown', ''),
                html=page.get('html', ''),
                metadata=page.get('metadata', {}),
                success=True
            ))

        return results

    async def search(self, query: str, **options) -> List[CrawlResult]:
        """æœç´¢æ¨¡å¼çˆ¬å–"""
        search_options = {
            'limit': options.get('limit', 10),
            'scrapeOptions': {
                'formats': ['markdown']
            }
        }

        results = await self.client.search(query, **search_options)

        return [
            CrawlResult(
                url=item.get('url', ''),
                markdown=item.get('markdown', ''),
                html='',
                metadata=item.get('metadata', {}),
                success=True
            )
            for item in results
        ]
```

#### 4.2.3 åº”ç”¨æœåŠ¡å±‚ (CrawlerApplicationService)

```python
class CrawlerApplicationService:
    """çˆ¬è™«åº”ç”¨æœåŠ¡"""

    def __init__(self, crawler: CrawlerInterface):
        self.crawler = crawler

    async def scrape_page(self, url: str, options: Dict[str, Any]) -> CrawlResult:
        """æŠ“å–å•é¡µ (ä¸šåŠ¡ç”¨ä¾‹)"""
        # æ·»åŠ ä¸šåŠ¡è§„åˆ™éªŒè¯
        if not self._is_valid_url(url):
            raise ValueError(f"æ— æ•ˆçš„URL: {url}")

        # æ‰§è¡Œçˆ¬å–
        result = await self.crawler.scrape(url, **options)

        # ä¸šåŠ¡é€»è¾‘å¤„ç†
        if result.success:
            await self._save_to_database(result)
            await self._notify_completion(result)

        return result

    async def scrape_multiple_pages(
        self,
        urls: List[str],
        options: Dict[str, Any]
    ) -> List[CrawlResult]:
        """æ‰¹é‡æŠ“å– (ä¸šåŠ¡ç”¨ä¾‹)"""
        tasks = [self.scrape_page(url, options) for url in urls]
        return await asyncio.gather(*tasks)
```

---

## 5. æ ¸å¿ƒå®ç°

### 5.1 åŸºæœ¬çˆ¬å–å®ç°

```python
from src.infrastructure.crawler.firecrawl_adapter import FirecrawlAdapter
from src.config import settings

# åˆå§‹åŒ–é€‚é…å™¨
crawler = FirecrawlAdapter(
    api_key=settings.FIRECRAWL_API_KEY,
    api_url=settings.FIRECRAWL_API_URL
)

# çˆ¬å–å•ä¸ªé¡µé¢
result = await crawler.scrape(
    url="https://example.com",
    formats=['markdown', 'html'],
    include_tags=['article', 'main'],
    exclude_tags=['nav', 'footer', 'aside'],
    wait_for=2000
)

if result.success:
    print(f"âœ… æˆåŠŸ: {result.url}")
    print(f"å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
else:
    print(f"âŒ å¤±è´¥: {result.error}")
```

### 5.2 APIç«¯ç‚¹é›†æˆ

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, HttpUrl

router = APIRouter(prefix="/api/v1/crawler", tags=["Crawler"])

class ScrapeRequest(BaseModel):
    url: HttpUrl
    formats: List[str] = ['markdown']
    include_tags: List[str] = []
    exclude_tags: List[str] = ['nav', 'footer']

@router.post("/scrape")
async def scrape_url(request: ScrapeRequest):
    """æŠ“å–å•ä¸ªURL"""
    crawler = FirecrawlAdapter(
        api_key=settings.FIRECRAWL_API_KEY
    )

    result = await crawler.scrape(
        url=str(request.url),
        formats=request.formats,
        include_tags=request.include_tags,
        exclude_tags=request.exclude_tags
    )

    if not result.success:
        raise HTTPException(
            status_code=500,
            detail=f"çˆ¬å–å¤±è´¥: {result.error}"
        )

    return {
        "url": result.url,
        "markdown": result.markdown,
        "metadata": result.metadata
    }
```

---

## 6. é«˜çº§åŠŸèƒ½

### 6.1 åŠ¨æ€å†…å®¹å¤„ç†

```python
# ç­‰å¾…JavaScriptæ¸²æŸ“
result = await crawler.scrape(
    url="https://spa-website.com",
    wait_for=5000,  # ç­‰å¾…5ç§’
    actions=[
        {
            'type': 'click',
            'selector': '#load-more-button'
        },
        {
            'type': 'wait',
            'milliseconds': 2000
        }
    ]
)
```

### 6.2 æ™ºèƒ½å†…å®¹æå–

```python
# ä½¿ç”¨AIæå–ç»“æ„åŒ–æ•°æ®
result = await crawler.scrape(
    url="https://product-page.com",
    formats=['markdown'],
    extract_schema={
        'type': 'object',
        'properties': {
            'product_name': {'type': 'string'},
            'price': {'type': 'number'},
            'description': {'type': 'string'},
            'reviews': {
                'type': 'array',
                'items': {
                    'type': 'object',
                    'properties': {
                        'author': {'type': 'string'},
                        'rating': {'type': 'number'},
                        'comment': {'type': 'string'}
                    }
                }
            }
        }
    }
)
```

### 6.3 æ‰¹é‡çˆ¬å–ä¼˜åŒ–

```python
import asyncio
from typing import List

async def batch_scrape(
    urls: List[str],
    batch_size: int = 5,
    delay: float = 1.0
) -> List[CrawlResult]:
    """æ‰¹é‡çˆ¬å– (å¹¶å‘æ§åˆ¶)"""
    results = []

    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]

        # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
        batch_results = await asyncio.gather(*[
            crawler.scrape(url) for url in batch
        ])

        results.extend(batch_results)

        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if i + batch_size < len(urls):
            await asyncio.sleep(delay)

    return results
```

### 6.4 å¢é‡çˆ¬å–ç­–ç•¥

```python
from datetime import datetime, timedelta

class IncrementalCrawler:
    """å¢é‡çˆ¬å–å™¨"""

    def __init__(self, crawler: CrawlerInterface):
        self.crawler = crawler
        self.last_crawl_cache = {}

    async def crawl_if_updated(
        self,
        url: str,
        cache_hours: int = 24
    ) -> CrawlResult | None:
        """ä»…åœ¨å†…å®¹æ›´æ–°æ—¶çˆ¬å–"""

        # æ£€æŸ¥ç¼“å­˜
        if url in self.last_crawl_cache:
            last_crawl_time = self.last_crawl_cache[url]['time']
            if datetime.now() - last_crawl_time < timedelta(hours=cache_hours):
                print(f"âš¡ ä½¿ç”¨ç¼“å­˜: {url}")
                return None

        # æ‰§è¡Œçˆ¬å–
        result = await self.crawler.scrape(url)

        # æ›´æ–°ç¼“å­˜
        self.last_crawl_cache[url] = {
            'time': datetime.now(),
            'content_hash': hash(result.markdown)
        }

        return result
```

---

## 7. æ•…éšœæ’æŸ¥

### 7.1 å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### é”™è¯¯1: APIå¯†é’¥æ— æ•ˆ

```
FirecrawlError: Invalid API key
```

**åŸå› **: APIå¯†é’¥é…ç½®é”™è¯¯æˆ–å·²è¿‡æœŸ

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ `.env` æ–‡ä»¶ä¸­çš„ `FIRECRAWL_API_KEY`
2. ç¡®è®¤å¯†é’¥æ ¼å¼: `fc-` å¼€å¤´
3. éªŒè¯å¯†é’¥æ˜¯å¦è¿‡æœŸ (ç™»å½•DashboardæŸ¥çœ‹)
4. æµ‹è¯•æ¨¡å¼: ä½¿ç”¨ `fc-test-` å¼€å¤´çš„æµ‹è¯•å¯†é’¥

```bash
# éªŒè¯å¯†é’¥
curl -X GET "https://api.firecrawl.dev/v2/account/info" \
  -H "Authorization: Bearer fc-your-api-key"
```

#### é”™è¯¯2: è¯·æ±‚è¶…æ—¶

```
TimeoutError: Request timed out after 30 seconds
```

**åŸå› **: ç½‘é¡µåŠ è½½æ—¶é—´è¿‡é•¿æˆ–ç½‘ç»œé—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è¶…æ—¶æ—¶é—´é…ç½®
```python
FIRECRAWL_TIMEOUT=60  # å¢åŠ åˆ°60ç§’
```

2. ä½¿ç”¨é‡è¯•æœºåˆ¶
```python
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=10))
async def scrape_with_retry(url: str):
    return await crawler.scrape(url)
```

3. æ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®
```bash
curl -I https://target-website.com
```

#### é”™è¯¯3: é…é¢è¶…é™

```
FirecrawlError: Rate limit exceeded (429 Too Many Requests)
```

**åŸå› **: è¶…è¿‡APIé…é¢é™åˆ¶

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥å½“å‰é…é¢ä½¿ç”¨æƒ…å†µ
```python
account_info = await app.get_account_info()
print(f"å‰©ä½™é…é¢: {account_info['credits_remaining']}")
```

2. å®ç°è¯·æ±‚é™æµ
```python
import asyncio
from asyncio import Semaphore

async def rate_limited_scrape(
    urls: List[str],
    max_concurrent: int = 3,
    delay: float = 1.0
):
    semaphore = Semaphore(max_concurrent)

    async def scrape_with_limit(url: str):
        async with semaphore:
            result = await crawler.scrape(url)
            await asyncio.sleep(delay)  # è¯·æ±‚é—´å»¶è¿Ÿ
            return result

    return await asyncio.gather(*[scrape_with_limit(url) for url in urls])
```

3. å‡çº§è®¢é˜…è®¡åˆ’ (ç”Ÿäº§ç¯å¢ƒ)

#### é”™è¯¯4: çˆ¬å–å¤±è´¥ (403/404/500)

```
FirecrawlError: Failed to scrape URL (HTTP 403)
```

**åŸå› **: ç›®æ ‡ç½‘ç«™é˜»æ­¢çˆ¬å–æˆ–é¡µé¢ä¸å­˜åœ¨

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥URLæœ‰æ•ˆæ€§
```python
import requests

def validate_url(url: str) -> bool:
    try:
        response = requests.head(url, timeout=5)
        return response.status_code < 400
    except:
        return False
```

2. æ·»åŠ User-Agentå’ŒHeaders
```python
result = await crawler.scrape(
    url="https://example.com",
    headers={
        'User-Agent': 'Mozilla/5.0 (compatible; YourBot/1.0)',
        'Accept': 'text/html,application/xhtml+xml'
    }
)
```

3. å¤„ç†åçˆ¬è™«æœºåˆ¶
   - ä½¿ç”¨ä»£ç†IP
   - å¢åŠ è¯·æ±‚é—´éš”
   - éµå®ˆrobots.txtè§„åˆ™

#### é”™è¯¯5: å†…å®¹è§£æå¤±è´¥

```
FirecrawlError: Failed to parse content
```

**åŸå› **: ç½‘é¡µç»“æ„å¼‚å¸¸æˆ–ç¼–ç é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
1. å°è¯•ä¸åŒçš„æ ¼å¼
```python
result = await crawler.scrape(
    url="https://example.com",
    formats=['markdown', 'html', 'rawHtml']  # å¤šç§æ ¼å¼
)
```

2. æŒ‡å®šç¼–ç 
```python
result = await crawler.scrape(
    url="https://example.com",
    encoding='utf-8'
)
```

3. ä½¿ç”¨æ›´å®½æ¾çš„è§£æé€‰é¡¹
```python
result = await crawler.scrape(
    url="https://example.com",
    exclude_tags=[],  # ä¸æ’é™¤ä»»ä½•æ ‡ç­¾
    include_tags=[]   # ä¸é™åˆ¶æ ‡ç­¾
)
```

### 7.2 è¯Šæ–­å·¥å…·

#### è¿æ¥æµ‹è¯•è„šæœ¬

```python
import asyncio
from src.infrastructure.crawler.firecrawl_adapter import FirecrawlAdapter
from src.config import settings

async def diagnose_firecrawl():
    """è¯Šæ–­Firecrawlè¿æ¥å’Œé…ç½®"""

    print("ğŸ” Firecrawlè¯Šæ–­å·¥å…·\n")

    # 1. æ£€æŸ¥é…ç½®
    print("1. æ£€æŸ¥é…ç½®:")
    print(f"   API Key: {settings.FIRECRAWL_API_KEY[:10]}...")
    print(f"   API URL: {settings.FIRECRAWL_API_URL}")
    print(f"   Timeout: {settings.FIRECRAWL_TIMEOUT}s\n")

    # 2. æµ‹è¯•è¿æ¥
    print("2. æµ‹è¯•APIè¿æ¥:")
    try:
        crawler = FirecrawlAdapter(settings.FIRECRAWL_API_KEY)
        result = await crawler.scrape("https://firecrawl.dev")

        if result.success:
            print("   âœ… è¿æ¥æˆåŠŸ")
            print(f"   âœ… å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦\n")
        else:
            print(f"   âŒ çˆ¬å–å¤±è´¥: {result.error}\n")
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {str(e)}\n")

    # 3. æ£€æŸ¥é…é¢
    print("3. æ£€æŸ¥APIé…é¢:")
    try:
        app = FirecrawlApp(api_key=settings.FIRECRAWL_API_KEY)
        info = await app.get_account_info()
        print(f"   å‰©ä½™é…é¢: {info.get('credits_remaining', 'N/A')}")
        print(f"   æ€»é…é¢: {info.get('credits_total', 'N/A')}")
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•è·å–é…é¢ä¿¡æ¯: {str(e)}")

if __name__ == "__main__":
    asyncio.run(diagnose_firecrawl())
```

### 7.3 æ—¥å¿—é…ç½®

```python
import logging

# é…ç½®Firecrawlæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logger = logging.getLogger('firecrawl')
logger.setLevel(logging.DEBUG)
```

---

## 8. æˆæœ¬ä¼˜åŒ–

### 8.1 é…é¢ç›‘æ§

```python
class CreditMonitor:
    """APIé…é¢ç›‘æ§"""

    def __init__(self, app: FirecrawlApp, threshold: int = 100):
        self.app = app
        self.threshold = threshold

    async def check_credits(self) -> bool:
        """æ£€æŸ¥å‰©ä½™é…é¢"""
        info = await self.app.get_account_info()
        remaining = info.get('credits_remaining', 0)

        if remaining < self.threshold:
            logger.warning(
                f"âš ï¸ APIé…é¢ä¸è¶³: å‰©ä½™ {remaining} credits"
            )
            return False

        return True
```

### 8.2 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
from datetime import datetime, timedelta

class CachedCrawler:
    """å¸¦ç¼“å­˜çš„çˆ¬è™«"""

    def __init__(self, crawler: CrawlerInterface):
        self.crawler = crawler
        self.cache = {}

    async def scrape_cached(
        self,
        url: str,
        cache_ttl: int = 3600  # 1å°æ—¶
    ) -> CrawlResult:
        """å¸¦ç¼“å­˜çš„çˆ¬å–"""

        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            cached_time, cached_result = self.cache[url]
            if (datetime.now() - cached_time).seconds < cache_ttl:
                logger.info(f"âš¡ ç¼“å­˜å‘½ä¸­: {url}")
                return cached_result

        # æ‰§è¡Œçˆ¬å–
        result = await self.crawler.scrape(url)

        # æ›´æ–°ç¼“å­˜
        self.cache[url] = (datetime.now(), result)

        return result
```

### 8.3 é€‰æ‹©æ€§çˆ¬å–

```python
async def smart_scrape(url: str, priority: str = 'low') -> CrawlResult:
    """æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©çˆ¬å–ç­–ç•¥"""

    if priority == 'high':
        # é«˜ä¼˜å…ˆçº§: å®Œæ•´çˆ¬å–
        return await crawler.scrape(
            url=url,
            formats=['markdown', 'html'],
            wait_for=5000
        )
    elif priority == 'medium':
        # ä¸­ä¼˜å…ˆçº§: ä»…Markdown
        return await crawler.scrape(
            url=url,
            formats=['markdown'],
            wait_for=2000
        )
    else:
        # ä½ä¼˜å…ˆçº§: æœ€å°é…ç½®
        return await crawler.scrape(
            url=url,
            formats=['markdown'],
            exclude_tags=['script', 'style', 'nav', 'footer']
        )
```

---

## 9. å®‰å…¨ä¸ç›‘æ§

### 9.1 APIå¯†é’¥å®‰å…¨

```python
# âœ… æ­£ç¡®: ä½¿ç”¨ç¯å¢ƒå˜é‡
api_key = os.getenv('FIRECRAWL_API_KEY')

# âŒ é”™è¯¯: ç¡¬ç¼–ç å¯†é’¥
api_key = 'fc-1234567890abcdef'  # æ°¸è¿œä¸è¦è¿™æ ·åš!

# âœ… æ­£ç¡®: å¯†é’¥éªŒè¯
def validate_api_key(key: str) -> bool:
    if not key or not key.startswith('fc-'):
        raise ValueError("æ— æ•ˆçš„APIå¯†é’¥æ ¼å¼")
    if len(key) < 20:
        raise ValueError("APIå¯†é’¥é•¿åº¦ä¸è¶³")
    return True
```

### 9.2 ç›‘æ§æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
scrape_requests = Counter(
    'firecrawl_scrape_requests_total',
    'Total Firecrawl scrape requests',
    ['status']
)

scrape_duration = Histogram(
    'firecrawl_scrape_duration_seconds',
    'Firecrawl scrape duration'
)

@scrape_duration.time()
async def monitored_scrape(url: str) -> CrawlResult:
    """å¸¦ç›‘æ§çš„çˆ¬å–"""
    try:
        result = await crawler.scrape(url)
        scrape_requests.labels(status='success').inc()
        return result
    except Exception as e:
        scrape_requests.labels(status='error').inc()
        raise
```

### 9.3 åˆè§„æ€§

```python
import robots_txt_parser

class ComplianceCrawler:
    """éµå®ˆrobots.txtçš„çˆ¬è™«"""

    def __init__(self, crawler: CrawlerInterface):
        self.crawler = crawler
        self.robots_parser = robots_txt_parser.RobotsTxtParser()

    async def scrape_compliant(self, url: str) -> CrawlResult:
        """æ£€æŸ¥robots.txtåå†çˆ¬å–"""

        # æ£€æŸ¥æ˜¯å¦å…è®¸çˆ¬å–
        if not self.robots_parser.can_fetch('*', url):
            raise PermissionError(
                f"robots.txtç¦æ­¢çˆ¬å–: {url}"
            )

        return await self.crawler.scrape(url)
```

---

## 10. æµ‹è¯•ç­–ç•¥

### 10.1 å•å…ƒæµ‹è¯•

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_firecrawl_adapter_scrape_success():
    """æµ‹è¯•æˆåŠŸçˆ¬å–"""

    # Mock Firecrawlå“åº”
    mock_result = {
        'markdown': '# Test Content',
        'html': '<h1>Test Content</h1>',
        'metadata': {'title': 'Test Page'}
    }

    with patch('firecrawl.FirecrawlApp.scrape', return_value=mock_result):
        crawler = FirecrawlAdapter(api_key='test-key')
        result = await crawler.scrape('https://test.com')

        assert result.success is True
        assert result.markdown == '# Test Content'
        assert result.metadata['title'] == 'Test Page'

@pytest.mark.asyncio
async def test_firecrawl_adapter_scrape_failure():
    """æµ‹è¯•çˆ¬å–å¤±è´¥"""

    with patch('firecrawl.FirecrawlApp.scrape', side_effect=Exception('Network error')):
        crawler = FirecrawlAdapter(api_key='test-key')
        result = await crawler.scrape('https://test.com')

        assert result.success is False
        assert 'Network error' in result.error
```

### 10.2 é›†æˆæµ‹è¯•

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_real_firecrawl_scrape():
    """é›†æˆæµ‹è¯•: çœŸå®APIè°ƒç”¨"""

    api_key = os.getenv('FIRECRAWL_TEST_API_KEY')
    if not api_key:
        pytest.skip("æœªé…ç½®æµ‹è¯•APIå¯†é’¥")

    crawler = FirecrawlAdapter(api_key=api_key)
    result = await crawler.scrape('https://firecrawl.dev')

    assert result.success is True
    assert len(result.markdown) > 0
    assert result.url == 'https://firecrawl.dev'
```

### 10.3 æ€§èƒ½æµ‹è¯•

```python
import time
import statistics

@pytest.mark.performance
@pytest.mark.asyncio
async def test_scrape_performance():
    """æ€§èƒ½æµ‹è¯•: çˆ¬å–é€Ÿåº¦"""

    crawler = FirecrawlAdapter(api_key='test-key')
    urls = [f'https://test.com/page{i}' for i in range(10)]

    start_time = time.time()
    results = await asyncio.gather(*[
        crawler.scrape(url) for url in urls
    ])
    end_time = time.time()

    duration = end_time - start_time
    avg_time = duration / len(urls)

    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"æ€»è€—æ—¶: {duration:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªURL: {avg_time:.2f}ç§’")

    assert avg_time < 5.0  # å¹³å‡æ¯ä¸ªURLä¸è¶…è¿‡5ç§’
```

---

## 11. API v2å‡çº§è®°å½•

### 11.1 å‡çº§æ—¶é—´çº¿

- **2025-10-11**: Firecrawlå®˜æ–¹å‘å¸ƒv2 API
- **2025-10-12**: é¡¹ç›®å¼€å§‹è¯„ä¼°å‡çº§
- **2025-10-13**: å®Œæˆv2 APIé›†æˆæµ‹è¯•
- **2025-10-14**: ç”Ÿäº§ç¯å¢ƒæ­£å¼åˆ‡æ¢åˆ°v2

### 11.2 ä¸»è¦å˜æ›´

#### v1 â†’ v2 APIå·®å¼‚

| åŠŸèƒ½ | v1 API | v2 API | å˜æ›´è¯´æ˜ |
|------|--------|--------|----------|
| ç«¯ç‚¹URL | `/v1/scrape` | `/v2/scrape` | URLè·¯å¾„å˜æ›´ |
| å“åº”æ ¼å¼ | `{content: ...}` | `{markdown: ..., html: ...}` | ç»“æ„åŒ–å“åº” |
| æ ¼å¼é€‰é¡¹ | å•ä¸€æ ¼å¼ | å¤šæ ¼å¼æ”¯æŒ | å¯åŒæ—¶è¿”å›markdownå’Œhtml |
| è®¤è¯æ–¹å¼ | Header: `x-api-key` | Header: `Authorization: Bearer` | æ ‡å‡†Bearer Token |
| é€Ÿç‡é™åˆ¶ | 60 req/min | 100 req/min | æå‡äº†é™åˆ¶ |
| è¶…æ—¶æ—¶é—´ | 30ç§’ | 60ç§’ | æ›´å®½æ¾çš„è¶…æ—¶ |

#### ä»£ç è¿ç§»ç¤ºä¾‹

```python
# ========== v1 API (å·²åºŸå¼ƒ) ==========
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key=api_key)
result = app.scrape_url(url)  # v1æ–¹æ³•
content = result['content']   # v1å“åº”æ ¼å¼

# ========== v2 API (å½“å‰ç‰ˆæœ¬) ==========
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key=api_key)
result = app.scrape(url, formats=['markdown', 'html'])  # v2æ–¹æ³•
markdown = result['markdown']  # v2å“åº”æ ¼å¼
html = result['html']
```

### 11.3 å‡çº§å½±å“è¯„ä¼°

âœ… **å·²å®ŒæˆéªŒè¯**:
- åŸºæœ¬çˆ¬å–åŠŸèƒ½æ­£å¸¸
- å“åº”æ ¼å¼å…¼å®¹
- æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™
- é”™è¯¯å¤„ç†æœºåˆ¶æœ‰æ•ˆ

âš ï¸ **éœ€è¦æ³¨æ„**:
- æ—§çš„v1ç«¯ç‚¹å°†åœ¨2025-12-31åå¤±æ•ˆ
- éƒ¨åˆ†é«˜çº§åŠŸèƒ½APIæœ‰æ‰€å˜åŒ–
- é…é¢è®¡ç®—æ–¹å¼ç•¥æœ‰ä¸åŒ

### 11.4 å›æ»šè®¡åˆ’

å¦‚æœv2å‡ºç°ä¸¥é‡é—®é¢˜,å¯ä¸´æ—¶å›æ»šåˆ°v1:

```python
# å›æ»šé…ç½®
FIRECRAWL_API_URL=https://api.firecrawl.dev/v1  # ä½¿ç”¨v1ç«¯ç‚¹
FIRECRAWL_USE_LEGACY=true                       # å¯ç”¨å…¼å®¹æ¨¡å¼

# å…¼å®¹æ€§é€‚é…å™¨
class LegacyFirecrawlAdapter(FirecrawlAdapter):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.api_version = 'v1'

    async def scrape(self, url: str, **options) -> CrawlResult:
        # ä½¿ç”¨v1 APIè°ƒç”¨
        result = await self.client.scrape_url(url)

        return CrawlResult(
            url=url,
            markdown=result.get('content', ''),  # v1æ ¼å¼è½¬æ¢
            html='',
            metadata=result.get('metadata', {}),
            success=True
        )
```

---

## é™„å½•

### A. å®Œæ•´é…ç½®å‚è€ƒ

```bash
# .env.example - Firecrawlå®Œæ•´é…ç½®æ¨¡æ¿

# ========== å¿…éœ€é…ç½® ==========
FIRECRAWL_API_KEY=fc-your-api-key-here

# ========== å¯é€‰é…ç½® ==========
FIRECRAWL_API_URL=https://api.firecrawl.dev/v2
FIRECRAWL_TIMEOUT=30
FIRECRAWL_MAX_RETRIES=3
FIRECRAWL_TEST_MODE=false

# ========== é«˜çº§é…ç½® ==========
FIRECRAWL_LOG_LEVEL=INFO
FIRECRAWL_ENABLE_CACHE=true
FIRECRAWL_CACHE_TTL=3600
FIRECRAWL_MAX_CONCURRENT_REQUESTS=5
FIRECRAWL_REQUEST_DELAY=1.0

# ========== ç›‘æ§é…ç½® ==========
FIRECRAWL_ENABLE_METRICS=true
FIRECRAWL_CREDIT_ALERT_THRESHOLD=100
```

### B. å¸¸ç”¨å‘½ä»¤

```bash
# éªŒè¯é…ç½®
python -m src.infrastructure.crawler.diagnose

# è¿è¡Œæµ‹è¯•
pytest tests/crawler/ -v

# æ€§èƒ½æµ‹è¯•
pytest tests/crawler/ -v -m performance

# é›†æˆæµ‹è¯•
pytest tests/crawler/ -v -m integration
```

### C. ç›¸å…³èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://docs.firecrawl.dev
- **APIå‚è€ƒ**: https://docs.firecrawl.dev/api-reference
- **Python SDK**: https://github.com/mendableai/firecrawl-py
- **ç¤¾åŒºæ”¯æŒ**: https://discord.gg/firecrawl
- **é—®é¢˜åé¦ˆ**: https://github.com/mendableai/firecrawl/issues

### D. æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | ç‰ˆæœ¬ | å˜æ›´å†…å®¹ |
|------|------|----------|
| 2025-10-14 | 2.0 | åˆå¹¶FIRECRAWL_INTEGRATION.mdå’ŒFIRECRAWL_API_CONFIGURATION.md |
| 2025-10-13 | 1.2 | å®ŒæˆAPI v2å‡çº§å’Œæµ‹è¯• |
| 2025-10-12 | 1.1 | æ·»åŠ æ•…éšœæ’æŸ¥ç« èŠ‚ |
| 2025-10-11 | 1.0 | åˆå§‹ç‰ˆæœ¬åˆ›å»º |

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Backend Development Team
**æœ€åæ›´æ–°**: 2025-10-14
**çŠ¶æ€**: âœ… å½“å‰ç‰ˆæœ¬
