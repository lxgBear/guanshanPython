# Firecrawl API é›†æˆæŒ‡å—

**APIç‰ˆæœ¬**: v2 | **å®˜ç½‘**: https://firecrawl.dev

---

## å¿«é€Ÿå¼€å§‹

### è·å–APIå¯†é’¥

1. æ³¨å†Œè´¦å·: https://firecrawl.dev/app/sign-up
2. è·å–API Key: Dashboard â†’ API Keys
3. é…ç½®ç¯å¢ƒå˜é‡:

```bash
# .env
FIRECRAWL_API_KEY=your_api_key_here
FIRECRAWL_BASE_URL=https://api.firecrawl.dev
```

---

## APIåŠŸèƒ½

### 1. Search API (å…³é”®è¯æœç´¢)

**ç”¨é€”**: åŸºäºå…³é”®è¯æœç´¢ç½‘é¡µå†…å®¹

**è¯·æ±‚æ ¼å¼**:
```python
POST https://api.firecrawl.dev/v2/search
{
  "query": "ç‰¹æœ—æ™® è´¸æ˜“æˆ˜",
  "limit": 10,
  "lang": "zh",
  "tbs": "qdr:m",  # æ—¶é—´èŒƒå›´: æœ€è¿‘ä¸€æœˆ
  "scrapeOptions": {
    "formats": ["markdown", "html", "links"],
    "onlyMainContent": true
  }
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "data": {
    "web": [
      {
        "title": "æ ‡é¢˜",
        "url": "https://...",
        "markdown": "å†…å®¹...",
        "html": "<html>...",
        "description": "æ‘˜è¦",
        "metadata": {...}
      }
    ]
  },
  "creditsUsed": 1
}
```

### 2. Scrape API (URLçˆ¬å–)

**ç”¨é€”**: çˆ¬å–æŒ‡å®šURLçš„å®Œæ•´å†…å®¹

**è¯·æ±‚æ ¼å¼**:
```python
POST https://api.firecrawl.dev/v2/scrape
{
  "url": "https://example.com/article",
  "formats": ["markdown", "html"],
  "onlyMainContent": true,
  "waitFor": 1000  # ç­‰å¾…æ—¶é—´(ms)
}
```

---

## ç³»ç»Ÿé›†æˆ

### é€‚é…å™¨æ¶æ„

```python
# src/infrastructure/search/firecrawl_search_adapter.py

class FirecrawlSearchAdapter:
    def __init__(self):
        self.api_key = settings.FIRECRAWL_API_KEY
        self.base_url = settings.FIRECRAWL_BASE_URL

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(480))
    async def search(self, query: str, user_config: UserSearchConfig):
        # æ„å»ºè¯·æ±‚
        request_body = self._build_request_body(query, config)

        # è°ƒç”¨API
        async with httpx.AsyncClient(proxies={}) as client:
            response = await client.post(
                f"{self.base_url}/v2/search",
                headers=self.headers,
                json=request_body
            )

        # è§£æç»“æœ
        results = self._parse_search_results(response.json())
        return results
```

### é…ç½®å‚æ•°

```python
# æœç´¢é…ç½®
search_config = {
    "limit": 10,              # ç»“æœæ•°é‡(1-100)
    "time_range": "month",    # day/week/month/year
    "language": "zh",         # zh/en/auto
    "include_domains": [],    # é™å®šåŸŸå
    "scrape_formats": ["markdown", "html", "links"],
    "only_main_content": True
}
```

### æ—¶é—´èŒƒå›´æ˜ å°„

| é…ç½®å€¼ | Firecrawlå‚æ•° | è¯´æ˜ |
|--------|--------------|------|
| `day` | `qdr:d` | æœ€è¿‘24å°æ—¶ |
| `week` | `qdr:w` | æœ€è¿‘ä¸€å‘¨ |
| `month` | `qdr:m` | æœ€è¿‘ä¸€æœˆ |
| `year` | `qdr:y` | æœ€è¿‘ä¸€å¹´ |

---

## æ•°æ®å¤„ç†

### å†…å®¹ä¼˜åŒ–

**Markdownæˆªæ–­**:
```python
# é™åˆ¶æœ€å¤§5000å­—ç¬¦
markdown_full = item.get('markdown', '')
if len(markdown_full) > 5000:
    markdown_content = markdown_full[:5000]
else:
    markdown_content = markdown_full
```

**å…ƒæ•°æ®ç²¾ç®€**:
```python
# ä»…ä¿ç•™æœ‰ç”¨å­—æ®µ
filtered_metadata = {
    'language': item_metadata.get('language'),
    'og_type': item_metadata.get('og:type'),
}
# ç§»é™¤Noneå€¼
filtered_metadata = {k: v for k, v in filtered_metadata.items() if v is not None}
```

### æœç´¢ç»“æœå®ä½“

```python
SearchResult(
    task_id=task_id,
    title=title,
    url=url,
    content=content,                      # ä¸»å†…å®¹
    snippet=description,                  # æ‘˜è¦
    markdown_content=markdown_content,    # Markdownæ ¼å¼
    html_content=html_content,            # HTMLæ ¼å¼
    published_date=published_date,
    language=language,
    relevance_score=score,
    metadata=filtered_metadata
)
```

---

## æ•…éšœå¤„ç†

### é‡è¯•æœºåˆ¶

```python
@retry(
    stop=stop_after_attempt(3),      # æœ€å¤š3æ¬¡
    wait=wait_fixed(480),             # é—´éš”8åˆ†é’Ÿ
    retry=retry_if_exception_type((
        httpx.ConnectError,           # DNS/ç½‘ç»œé”™è¯¯
        httpx.TimeoutException,       # è¶…æ—¶
        httpx.HTTPStatusError         # HTTPé”™è¯¯
    ))
)
```

### å¸¸è§é”™è¯¯

**1. è®¤è¯å¤±è´¥ (401)**
```json
{
  "error": "Invalid API key"
}
```
**è§£å†³**: æ£€æŸ¥ `FIRECRAWL_API_KEY` é…ç½®

**2. é…é¢è¶…é™ (429)**
```json
{
  "error": "Rate limit exceeded"
}
```
**è§£å†³**: ç­‰å¾…æˆ–å‡çº§å¥—é¤

**3. è¯·æ±‚è¶…æ—¶**
```
httpx.TimeoutException
```
**è§£å†³**: å¢åŠ  `timeout` é…ç½®æˆ–å¯ç”¨é‡è¯•

---

## æœ€ä½³å®è·µ

### 1. ä»£ç†é…ç½®

```python
# ç¦ç”¨ä»£ç†(ç›´è¿Firecrawl)
client_config = {
    "proxies": {},  # ç©ºå­—å…¸ç¦ç”¨ä»£ç†
    "timeout": 30
}
```

### 2. åŸŸåé™å®š

```python
# ä½¿ç”¨site:æ“ä½œç¬¦é™å®šåŸŸå
if config.get('include_domains'):
    domains = config['include_domains']
    site_operators = ' OR '.join([f'site:{domain}' for domain in domains])
    final_query = f"({site_operators}) {query}"
```

**ç¤ºä¾‹**:
```python
query = "äººå·¥æ™ºèƒ½"
domains = ["nytimes.com", "reuters.com"]
# ç»“æœ: "(site:nytimes.com OR site:reuters.com) äººå·¥æ™ºèƒ½"
```

### 3. ç»“æœæ•°é‡æ§åˆ¶

| åœºæ™¯ | å»ºè®®å€¼ |
|------|--------|
| å¿«é€Ÿæ‰«æ | 10 |
| å¸¸è§„ç›‘æ§ | 20-30 |
| æ·±åº¦é‡‡é›† | 50-100 |

### 4. æˆæœ¬ä¼˜åŒ–

- æ¯æ¬¡Searchè¯·æ±‚æ¶ˆè€—1ä¸ªcredit
- æ‰¹é‡æœç´¢ä½¿ç”¨å¹¶å‘æ§åˆ¶
- åˆç†è®¾ç½®ç¼“å­˜é¿å…é‡å¤è¯·æ±‚

---

## ç›‘æ§æŒ‡æ ‡

### APIè°ƒç”¨æ—¥å¿—

```bash
# æˆåŠŸè°ƒç”¨
âœ… è§£æå¾—åˆ° 10 æ¡æœç´¢ç»“æœ

# å¤±è´¥é‡è¯•
ğŸ”„ æœç´¢è¯·æ±‚å¤±è´¥ï¼Œç¬¬ 1 æ¬¡é‡è¯• (å…±3æ¬¡)ï¼Œå°†åœ¨ 8 åˆ†é’Ÿåé‡è¯•...

# APIå“åº”
ğŸ“¡ API å“åº”çŠ¶æ€ç : 200
ğŸ“¦ å“åº”æ•°æ®ç»“æ„: ['success', 'data', 'creditsUsed']
```

### æ€§èƒ½æŒ‡æ ‡

- **å“åº”æ—¶é—´**: é€šå¸¸3-7ç§’
- **æˆåŠŸç‡**: >99% (å«é‡è¯•)
- **Creditsæ¶ˆè€—**: æ¯æ¬¡æœç´¢1 credit

---

## æµ‹è¯•æ¨¡å¼

### å¯ç”¨æµ‹è¯•æ¨¡å¼

```bash
# .env
TEST_MODE=true
```

**è¡Œä¸º**:
- ç”Ÿæˆ10æ¡æ¨¡æ‹Ÿæœç´¢ç»“æœ
- ä¸æ¶ˆè€—API credits
- å“åº”æ—¶é—´<100ms
- é€‚ç”¨äºå¼€å‘å’Œæµ‹è¯•

```python
result = SearchResult(
    title=f"æµ‹è¯•ç»“æœ {i+1}: {query}",
    url=f"https://example.com/test/{i+1}",
    content=f"æµ‹è¯•å†…å®¹...",
    is_test_data=True
)
```

---

## ç›¸å…³æ–‡æ¡£

- [APIä½¿ç”¨æŒ‡å—](API_GUIDE.md)
- [é‡è¯•æœºåˆ¶](RETRY_MECHANISM.md)
- [ç³»ç»Ÿæ¶æ„](SYSTEM_ARCHITECTURE.md)

**å®˜æ–¹æ–‡æ¡£**: https://docs.firecrawl.dev
**ç»´æŠ¤è€…**: Backend Team
