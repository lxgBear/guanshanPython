# Map+Scrape æŠ€æœ¯è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v2.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-14
**çŠ¶æ€**: å·²å®æ–½

---

## ğŸ“‹ ç›®å½•

1. [éœ€æ±‚èƒŒæ™¯](#éœ€æ±‚èƒŒæ™¯)
2. [æŠ€æœ¯æ–¹æ¡ˆ](#æŠ€æœ¯æ–¹æ¡ˆ)
3. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
4. [æ‰§è¡Œå™¨è®¾è®¡](#æ‰§è¡Œå™¨è®¾è®¡)
5. [URLè¿‡æ»¤ç³»ç»Ÿ](#urlè¿‡æ»¤ç³»ç»Ÿ)
6. [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
7. [æ•°æ®åº“å…¼å®¹æ€§](#æ•°æ®åº“å…¼å®¹æ€§)
8. [ç§¯åˆ†æ¶ˆè€—è®¡ç®—](#ç§¯åˆ†æ¶ˆè€—è®¡ç®—)
9. [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)

---

## éœ€æ±‚èƒŒæ™¯

### ä¸šåŠ¡éœ€æ±‚

**æ ¸å¿ƒéœ€æ±‚**ï¼šå®ç°æŒ‡å®šURL + æ—¶é—´èŒƒå›´çš„ç²¾ç¡®ç½‘ç«™å†…å®¹çˆ¬å–

**å…·ä½“åœºæ™¯**ï¼š
- å®šæœŸç›‘æ§ç‰¹å®šç½‘ç«™çš„æœ€æ–°å†…å®¹
- åªçˆ¬å–ç‰¹å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ç« ï¼ˆå¦‚ï¼šæœ€è¿‘30å¤©ï¼‰
- é¿å…é‡å¤çˆ¬å–å†å²å†…å®¹ï¼ŒèŠ‚çœAPIç§¯åˆ†

**ç°æœ‰æ–¹æ¡ˆçš„å±€é™æ€§**ï¼š
- **Crawl API**ï¼šé€’å½’çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼Œæ— æ³•ç²¾ç¡®æ§åˆ¶çˆ¬å–å“ªäº›é¡µé¢
- **æ—¶é—´è¿‡æ»¤æ»å**ï¼šéœ€è¦å…ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼Œå†æ ¹æ®å‘å¸ƒæ—¶é—´è¿‡æ»¤
- **ç§¯åˆ†æµªè´¹**ï¼šçˆ¬å–äº†å¤§é‡ä¸éœ€è¦çš„å†å²é¡µé¢

### æŠ€æœ¯ç›®æ ‡

1. âœ… **ç²¾ç¡®URLå‘ç°**ï¼šä½¿ç”¨Map APIå¿«é€Ÿè·å–ç½‘ç«™æ‰€æœ‰URL
2. âœ… **æŒ‰éœ€çˆ¬å–**ï¼šåªçˆ¬å–ç¬¦åˆæ—¶é—´èŒƒå›´çš„é¡µé¢
3. âœ… **èŠ‚çœç§¯åˆ†**ï¼šé¿å…ä¸å¿…è¦çš„é¡µé¢çˆ¬å–
4. âœ… **æ•°æ®å…¼å®¹**ï¼šä¿æŒæ•°æ®åº“å­—æ®µç»“æ„ä¸å˜
5. âœ… **å¤‡ç”¨æ–¹æ¡ˆ**ï¼šä¿ç•™Crawl APIä½œä¸ºfallback

---

## æŠ€æœ¯æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¦‚è¿°

**æ–°æ‰§è¡Œå™¨**ï¼š`MapScrapeExecutor`
**ä»»åŠ¡ç±»å‹**ï¼š`TaskType.MAP_SCRAPE_WEBSITE = "map_scrape_website"`
**æ ¸å¿ƒæµç¨‹**ï¼šMap API â†’ URLè¿‡æ»¤ â†’ Batch Scrape â†’ æ—¶é—´è¿‡æ»¤ â†’ ä¿å­˜ç»“æœ

### æ‰§è¡Œæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Map API - å‘ç°æ‰€æœ‰URL                                 â”‚
â”‚  è¾“å…¥: èµ·å§‹URL, searchå‚æ•°ï¼ˆå¯é€‰ï¼‰                             â”‚
â”‚  è¾“å‡º: URLåˆ—è¡¨ + å…ƒæ•°æ®ï¼ˆtitle, descriptionï¼‰                 â”‚
â”‚  æ—¶é—´: ~5ç§’                                                   â”‚
â”‚  ç§¯åˆ†: 1 credit                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: URLè¿‡æ»¤ - ç§»é™¤æ— ç”¨é“¾æ¥ï¼ˆv2.1.2æ–°å¢ï¼‰                  â”‚
â”‚  è¿‡æ»¤: è·¯å¾„å…³é”®è¯ã€æ–‡ä»¶ç±»å‹ã€åŸŸåã€å»é‡                       â”‚
â”‚  è¾“å‡º: è¿‡æ»¤åçš„æœ‰æ•ˆURLåˆ—è¡¨                                     â”‚
â”‚  æ—¶é—´: ~25-40ms                                               â”‚
â”‚  è¿‡æ»¤ç‡: 35-45%                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: Batch Scrape - æ‰¹é‡çˆ¬å–å†…å®¹                           â”‚
â”‚  è¾“å…¥: è¿‡æ»¤åçš„URLåˆ—è¡¨                                         â”‚
â”‚  å¤„ç†: å¹¶å‘scrapeï¼ˆSemaphoreæ§åˆ¶å¹¶å‘æ•°ï¼‰                       â”‚
â”‚  è¾“å‡º: æ¯ä¸ªURLçš„å®Œæ•´å†…å®¹ + metadataï¼ˆå«publishedDateï¼‰        â”‚
â”‚  æ—¶é—´: ~N*2ç§’ï¼ˆN=URLæ•°é‡ï¼Œè€ƒè™‘å¹¶å‘ï¼‰                           â”‚
â”‚  ç§¯åˆ†: N credits                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ4: æ—¶é—´è¿‡æ»¤ - ç­›é€‰ç¬¦åˆæ¡ä»¶çš„å†…å®¹                          â”‚
â”‚  è¾“å…¥: æ‰€æœ‰scrapeç»“æœ                                          â”‚
â”‚  è¿‡æ»¤: metadata.publishedDateåœ¨[start_date, end_date]èŒƒå›´å†…   â”‚
â”‚  è¾“å‡º: ç¬¦åˆæ—¶é—´æ¡ä»¶çš„ç»“æœåˆ—è¡¨                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ5: æ•°æ®ä¿å­˜                                               â”‚
â”‚  - è½¬æ¢ä¸ºSearchResultå®ä½“                                     â”‚
â”‚  - ä¿å­˜åˆ°search_resultsé›†åˆ                                   â”‚
â”‚  - ä¿å­˜åŸå§‹å“åº”åˆ°firecrawl_raw_responses                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸Crawl APIçš„å¯¹æ¯”

| ç‰¹æ€§ | Crawl API | Map + Scrape API |
|------|-----------|------------------|
| **URLå‘ç°** | é€’å½’çˆ¬å– | Map APIä¸€æ¬¡æ€§è·å– |
| **é€Ÿåº¦** | è¾ƒæ…¢ï¼ˆéœ€è¦é€’å½’ï¼‰ | è¾ƒå¿«ï¼ˆå¹¶å‘scrapeï¼‰ |
| **URLè¿‡æ»¤** | è·¯å¾„è¿‡æ»¤ï¼ˆç®€å•ï¼‰ | å¤šå±‚æ¬¡è¿‡æ»¤ï¼ˆæ™ºèƒ½ï¼‰ |
| **æ—¶é—´è¿‡æ»¤** | çˆ¬å–åè¿‡æ»¤ | å¯æå‰è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰ |
| **ç§¯åˆ†æ¶ˆè€—** | æŒ‰çˆ¬å–é¡µé¢æ•° | Map(1) + Scrape(N) |
| **é€‚ç”¨åœºæ™¯** | å®Œæ•´å½’æ¡£ | ç²¾ç¡®ç›®æ ‡çˆ¬å– |
| **æ§åˆ¶ç²¾åº¦** | è·¯å¾„è¿‡æ»¤ | URLçº§åˆ«è¿‡æ»¤ |

---

## æ¶æ„è®¾è®¡

### é…ç½®ç±»è®¾è®¡

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, List

@dataclass
class MapScrapeConfig:
    """Map + Scrape æ‰§è¡Œå™¨é…ç½®"""

    # Map API é…ç½®
    search: Optional[str] = None          # æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
    map_limit: int = 5000                 # Map APIè¿”å›URLæ•°é‡é™åˆ¶

    # æ—¶é—´è¿‡æ»¤é…ç½®
    start_date: Optional[datetime] = None # å¼€å§‹æ—¥æœŸï¼ˆåŒ…å«ï¼‰
    end_date: Optional[datetime] = None   # ç»“æŸæ—¥æœŸï¼ˆåŒ…å«ï¼‰

    # Scrape API é…ç½®
    max_concurrent_scrapes: int = 5       # æœ€å¤§å¹¶å‘scrapeæ•°
    scrape_delay: float = 0.5             # scrapeé—´éš”ï¼ˆç§’ï¼‰
    only_main_content: bool = True        # åªæå–ä¸»è¦å†…å®¹
    exclude_tags: List[str] = field(      # æ’é™¤çš„HTMLæ ‡ç­¾
        default_factory=lambda: ["nav", "footer", "header", "aside"]
    )
    timeout: int = 90                     # å•ä¸ªscrapeè¶…æ—¶ï¼ˆç§’ï¼‰

    # é”™è¯¯å¤„ç†
    allow_partial_failure: bool = True    # å…è®¸éƒ¨åˆ†scrapeå¤±è´¥
    min_success_rate: float = 0.8         # æœ€ä½æˆåŠŸç‡ï¼ˆ80%ï¼‰
```

---

## æ‰§è¡Œå™¨è®¾è®¡

### æ‰§è¡Œå™¨ç±»æ¡†æ¶

```python
class MapScrapeExecutor(TaskExecutor):
    """Map + Scrape ä»»åŠ¡æ‰§è¡Œå™¨

    é€‚ç”¨äºéœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–URLå’Œæ—¶é—´èŒƒå›´çš„åœºæ™¯
    """

    def __init__(self):
        super().__init__()
        self.firecrawl_adapter = FirecrawlAdapter()

    async def execute(self, task: SearchTask) -> SearchResultBatch:
        """æ‰§è¡ŒMap + Scrapeä»»åŠ¡"""
        start_time = datetime.utcnow()

        # 1. éªŒè¯é…ç½®
        if not self.validate_config(task):
            raise ConfigValidationError(f"ä»»åŠ¡é…ç½®æ— æ•ˆ: {task.id}")

        # 2. è§£æé…ç½®
        config = ConfigFactory.create_map_scrape_config(task.crawl_config)

        # 3. é˜¶æ®µ1: Map - å‘ç°URL
        urls = await self._execute_map(task.crawl_url, config)
        self.logger.info(f"ğŸ—ºï¸  Mapå‘ç° {len(urls)} ä¸ªURL")

        # 4. é˜¶æ®µ2: URLè¿‡æ»¤ï¼ˆv2.1.2ï¼‰
        urls = await self._filter_urls(urls, task, config)
        self.logger.info(f"âœ… è¿‡æ»¤åä¿ç•™ {len(urls)} ä¸ªæœ‰æ•ˆé“¾æ¥")

        # 5. é˜¶æ®µ3: Batch Scrape - çˆ¬å–å†…å®¹
        scrape_results = await self._batch_scrape(urls, config)
        self.logger.info(f"âœ… Scrapeå®Œæˆ {len(scrape_results)} ä¸ªé¡µé¢")

        # 6. é˜¶æ®µ4: æ—¶é—´è¿‡æ»¤
        filtered_results = self._filter_by_date(scrape_results, config)
        self.logger.info(f"ğŸ” æ—¶é—´è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} ä¸ªç»“æœ")

        # 7. ä¿å­˜åŸå§‹å“åº”
        await self._save_raw_responses(scrape_results, task)

        # 8. è½¬æ¢ä¸ºSearchResult
        search_results = self._convert_to_search_results(filtered_results, task)

        # 9. åˆ›å»ºç»“æœæ‰¹æ¬¡
        batch = self._create_result_batch(task, query=f"Map+Scrape: {task.crawl_url}")
        for result in search_results:
            batch.add_result(result)

        # 10. è®¡ç®—ç§¯åˆ†æ¶ˆè€—
        batch.credits_used = FirecrawlCreditsCalculator.calculate_map_scrape_credits(
            map_calls=1,
            urls_scraped=len(scrape_results)
        )

        # 11. è®¡ç®—æ‰§è¡Œæ—¶é—´
        batch.execution_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

        return batch
```

---

## URLè¿‡æ»¤ç³»ç»Ÿ

### è®¾è®¡åŸåˆ™

åŸºäº**SOLIDåŸåˆ™**çš„æ¨¡å—åŒ–æ¶æ„ï¼š

1. **å•ä¸€èŒè´£åŸåˆ™ (SRP)**ï¼šæ¯ä¸ªè¿‡æ»¤å™¨åªè´Ÿè´£ä¸€ç§è¿‡æ»¤é€»è¾‘
2. **å¼€æ”¾å°é—­åŸåˆ™ (OCP)**ï¼šå¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å°é—­
3. **æ¥å£éš”ç¦»åŸåˆ™ (ISP)**ï¼šå®šä¹‰æ¸…æ™°çš„è¿‡æ»¤å™¨æ¥å£
4. **ä¾èµ–å€’ç½®åŸåˆ™ (DIP)**ï¼šä¾èµ–æŠ½è±¡è€Œéå…·ä½“å®ç°

### è¿‡æ»¤æ¶æ„

```
URLè¿‡æ»¤ç³»ç»Ÿ
â”œâ”€â”€ 1. è¿‡æ»¤å™¨æ¥å£å±‚ (URLFilter Interface)
â”‚   â””â”€â”€ å®šä¹‰ç»Ÿä¸€çš„è¿‡æ»¤å™¨æ¥å£
â”‚
â”œâ”€â”€ 2. è¿‡æ»¤å™¨å®ç°å±‚ (Filter Implementations)
â”‚   â”œâ”€â”€ URLNormalizer - URLè§„èŒƒåŒ–
â”‚   â”œâ”€â”€ PathKeywordFilter - è·¯å¾„å…³é”®è¯è¿‡æ»¤
â”‚   â”œâ”€â”€ FileTypeFilter - æ–‡ä»¶ç±»å‹è¿‡æ»¤
â”‚   â”œâ”€â”€ DomainFilter - åŸŸåèŒƒå›´è¿‡æ»¤
â”‚   â””â”€â”€ URLDeduplicator - URLå»é‡
â”‚
â”œâ”€â”€ 3. è¿‡æ»¤å™¨ç®¡é“å±‚ (Filter Pipeline)
â”‚   â”œâ”€â”€ FilterChain - è¿‡æ»¤å™¨é“¾ï¼ˆè´£ä»»é“¾æ¨¡å¼ï¼‰
â”‚   â”œâ”€â”€ FilterRegistry - è¿‡æ»¤å™¨æ³¨å†Œè¡¨ï¼ˆå•ä¾‹+å·¥å‚æ¨¡å¼ï¼‰
â”‚   â””â”€â”€ PipelineBuilder - ç®¡é“æ„å»ºå™¨ï¼ˆå»ºé€ è€…æ¨¡å¼ï¼‰
â”‚
â””â”€â”€ 4. é›†æˆé€‚é…å±‚ (Integration Adapter)
    â””â”€â”€ MapScrapeExecutoré›†æˆç‚¹
```

### è¿‡æ»¤æµç¨‹

```
Map API è¿”å›URLs (1000ä¸ª)
    â†“
[æ­¥éª¤1: URLè§„èŒƒåŒ–]
  - ç§»é™¤fragment (#section)
  - ç»Ÿä¸€å°¾éƒ¨æ–œæ 
  - URL decode
    â†“ (995ä¸ª)
[æ­¥éª¤2: è·¯å¾„å…³é”®è¯è¿‡æ»¤]
  - é»‘åå•åŒ¹é…: login, about, contactç­‰
    â†“ (850ä¸ª, -145)
[æ­¥éª¤3: æ–‡ä»¶ç±»å‹è¿‡æ»¤]
  - æ‰©å±•åæ£€æŸ¥: .pdf, .jpg, .zipç­‰
    â†“ (780ä¸ª, -70)
[æ­¥éª¤4: åŸŸåèŒƒå›´è¿‡æ»¤]
  - æ’é™¤å¤–éƒ¨åŸŸå
    â†“ (720ä¸ª, -60)
[æ­¥éª¤5: URLå»é‡ä¼˜åŒ–]
  - å‚æ•°ç®€åŒ–ã€è·Ÿè¸ªå‚æ•°ç§»é™¤
    â†“ (650ä¸ª, -70)
è¿‡æ»¤åçš„URLs â†’ Scrape API
```

**æ€»è¿‡æ»¤ç‡**: 35-45% (å…¸å‹åœºæ™¯)

### è¿‡æ»¤å™¨æ¥å£

```python
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

@dataclass
class FilterContext:
    """è¿‡æ»¤ä¸Šä¸‹æ–‡ - ä¼ é€’è¿‡æ»¤æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    base_url: str  # åŸºç¡€URLï¼ˆç”¨äºåŸŸåè¿‡æ»¤ï¼‰
    task_id: str   # ä»»åŠ¡ID
    config: Dict[str, Any]  # é…ç½®ä¿¡æ¯

class URLFilter(ABC):
    """URLè¿‡æ»¤å™¨æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        """æ‰§è¡Œè¿‡æ»¤"""
        pass

    @abstractmethod
    def get_filter_name(self) -> str:
        """è·å–è¿‡æ»¤å™¨åç§°"""
        pass

    @property
    def enabled(self) -> bool:
        """è¿‡æ»¤å™¨æ˜¯å¦å¯ç”¨"""
        return True
```

### é»‘åå•é…ç½®

#### è·¯å¾„å…³é”®è¯é»‘åå•

**A. ç”¨æˆ·åŠŸèƒ½ç±»**:
```
login, signin, register, signup, logout
account, profile, dashboard, settings
forgot-password, reset-password
```

**B. ç½‘ç«™ä¿¡æ¯ç±»**:
```
about, about-us, contact, contact-us
privacy, privacy-policy, terms, terms-of-service
disclaimer, legal, cookies
```

**C. å¯¼èˆªåŠŸèƒ½ç±»**:
```
search, sitemap, category, categories
tag, tags, archive, archives
```

**D. æŠ€æœ¯é¡µé¢ç±»**:
```
rss, feed, atom, api, admin
wp-admin, wp-content (WordPress)
static, assets, resources
```

#### æ–‡ä»¶ç±»å‹é»‘åå•

**æ–‡æ¡£ç±»**: `.pdf, .doc, .docx, .xls, .xlsx, .ppt, .pptx`
**å›¾ç‰‡ç±»**: `.jpg, .jpeg, .png, .gif, .svg, .webp`
**å‹ç¼©åŒ…ç±»**: `.zip, .rar, .7z, .tar, .gz`
**å¤šåª’ä½“ç±»**: `.mp3, .mp4, .avi, .mov`
**æŠ€æœ¯æ–‡ä»¶ç±»**: `.xml, .json, .css, .js, .rss`

---

## å®ç°ç»†èŠ‚

### 1. FirecrawlAdapteræ‰©å±•

**æ–°å¢map()æ–¹æ³•**ï¼š

```python
async def map(
    self,
    url: str,
    search: Optional[str] = None,
    limit: int = 5000
) -> List[Dict[str, Any]]:
    """è°ƒç”¨Firecrawl Map API

    Args:
        url: èµ·å§‹URL
        search: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›URLæ•°é‡é™åˆ¶

    Returns:
        List[Dict]: [
            {"url": "...", "title": "...", "description": "..."},
            ...
        ]

    Raises:
        MapAPIError: Map APIè°ƒç”¨å¤±è´¥
    """
    payload = {"url": url, "limit": limit}
    if search:
        payload["search"] = search

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {self.api_key}"
    }

    try:
        response = await self.client.post(
            f"{self.base_url}/v2/map",
            json=payload,
            headers=headers,
            timeout=30
        )
        response.raise_for_status()

        data = response.json()
        if not data.get("success"):
            raise MapAPIError(f"Map APIè¿”å›å¤±è´¥: {data}")

        links = data.get("links", [])
        self.logger.info(f"âœ… Map APIè¿”å› {len(links)} ä¸ªURL")

        return links

    except Exception as e:
        self.logger.error(f"âŒ Map APIè°ƒç”¨å¤±è´¥: {e}")
        raise MapAPIError(f"Map APIè°ƒç”¨å¤±è´¥: {str(e)}")
```

### 2. URLè¿‡æ»¤å®ç°

**è·¯å¾„å…³é”®è¯è¿‡æ»¤**ï¼š

```python
class PathKeywordFilter(URLFilter):
    """è·¯å¾„å…³é”®è¯è¿‡æ»¤å™¨"""

    def __init__(self, blacklist: Optional[List[str]] = None):
        self._blacklist = set(blacklist or self._get_default_blacklist())

    def filter(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        filtered = []
        for url in urls:
            path = urlparse(url).path.lower()
            if not any(keyword in path for keyword in self._blacklist):
                filtered.append(url)
        return filtered

    @staticmethod
    def _get_default_blacklist() -> List[str]:
        return [
            'login', 'register', 'about', 'contact',
            'privacy', 'terms', 'search', 'category', 'tag'
        ]
```

**è¿‡æ»¤å™¨ç®¡é“**ï¼š

```python
class FilterChain:
    """è¿‡æ»¤å™¨é“¾ - è´£ä»»é“¾æ¨¡å¼"""

    def __init__(self):
        self._filters: List[URLFilter] = []
        self._statistics: Dict[str, Dict[str, int]] = {}

    def add_filter(self, filter: URLFilter) -> 'FilterChain':
        """æ·»åŠ è¿‡æ»¤å™¨ï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰"""
        self._filters.append(filter)
        return self

    def execute(self, urls: List[str], context: Optional[FilterContext] = None) -> List[str]:
        """æ‰§è¡Œè¿‡æ»¤å™¨é“¾"""
        current_urls = urls
        self._statistics = {}

        for filter in self._filters:
            if not filter.enabled:
                continue

            before_count = len(current_urls)
            current_urls = filter.filter(current_urls, context)
            after_count = len(current_urls)

            # è®°å½•ç»Ÿè®¡
            self._statistics[filter.get_filter_name()] = {
                "before": before_count,
                "after": after_count,
                "filtered": before_count - after_count
            }

        return current_urls
```

### 3. æ‰¹é‡Scrapeå®ç°

```python
async def _batch_scrape(
    self,
    urls: List[str],
    config: MapScrapeConfig
) -> List[CrawlResult]:
    """æ‰¹é‡scrape URLï¼ˆå¸¦å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†ï¼‰"""
    semaphore = asyncio.Semaphore(config.max_concurrent_scrapes)
    results = []
    failed_urls = []

    async def scrape_with_limit(url: str, index: int) -> Optional[CrawlResult]:
        async with semaphore:
            try:
                if index > 0:
                    await asyncio.sleep(config.scrape_delay)

                result = await self.firecrawl_adapter.scrape(
                    url,
                    only_main_content=config.only_main_content,
                    exclude_tags=config.exclude_tags,
                    timeout=config.timeout
                )
                return result

            except Exception as e:
                self.logger.warning(f"âš ï¸  Scrapeå¤±è´¥ {url}: {e}")
                failed_urls.append(url)
                return None

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰scrape
    tasks = [scrape_with_limit(url, i) for i, url in enumerate(urls)]
    scrape_results = await asyncio.gather(*tasks)

    # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
    results = [r for r in scrape_results if r is not None]

    # æ£€æŸ¥æˆåŠŸç‡
    success_rate = len(results) / len(urls) if urls else 0

    # éªŒè¯æœ€ä½æˆåŠŸç‡
    if not config.allow_partial_failure and success_rate < config.min_success_rate:
        raise ExecutionError(f"ScrapeæˆåŠŸç‡è¿‡ä½: {success_rate*100:.1f}%")

    return results
```

### 4. æ—¶é—´è¿‡æ»¤å®ç°

```python
def _filter_by_date(
    self,
    results: List[CrawlResult],
    config: MapScrapeConfig
) -> List[CrawlResult]:
    """æ ¹æ®å‘å¸ƒæ—¶é—´è¿‡æ»¤ç»“æœ"""
    if not config.start_date and not config.end_date:
        return results

    filtered = []

    for result in results:
        metadata = result.metadata or {}
        published_date_str = (
            metadata.get('publishedDate') or
            metadata.get('published_date') or
            metadata.get('article:published_time')
        )

        if not published_date_str:
            continue

        try:
            published_date = datetime.fromisoformat(published_date_str)

            if config.start_date and published_date < config.start_date:
                continue

            if config.end_date and published_date > config.end_date:
                continue

            filtered.append(result)

        except Exception as e:
            self.logger.warning(f"âš ï¸  è§£æå‘å¸ƒæ—¶é—´å¤±è´¥ {result.url}: {e}")
            continue

    return filtered
```

---

## æ•°æ®åº“å…¼å®¹æ€§

### SearchResultå­—æ®µæ˜ å°„

**å®Œå…¨å…¼å®¹ç°æœ‰å­—æ®µç»“æ„**ï¼š

```python
def _convert_to_search_results(
    self,
    scrape_results: List[CrawlResult],
    task: SearchTask
) -> List[SearchResult]:
    """è½¬æ¢ä¸ºSearchResultï¼ˆä¸ç°æœ‰ç»“æ„å®Œå…¨å…¼å®¹ï¼‰"""

    search_results = []

    for idx, result in enumerate(scrape_results, start=1):
        metadata_dict = result.metadata if isinstance(result.metadata, dict) else {}
        metadata_fields = self._extract_metadata_fields(metadata_dict)

        search_result = SearchResult(
            task_id=str(task.id),
            title=metadata_dict.get("title", result.url),
            url=result.url,
            snippet=result.content[:200] if result.content else "",
            source="map_scrape",  # æ–°çš„sourceæ ‡è¯†

            # å…ƒæ•°æ®å­—æ®µï¼ˆå®Œå…¨ç›¸åŒï¼‰
            published_date=metadata_fields.get('published_date'),
            author=metadata_fields.get('author'),
            language=metadata_fields.get('language'),
            article_tag=metadata_fields.get('article_tag'),
            article_published_time=metadata_fields.get('article_published_time'),
            source_url=metadata_fields.get('source_url'),
            http_status_code=metadata_fields.get('http_status_code'),

            search_position=idx,
            markdown_content=result.markdown if result.markdown else result.content,
            html_content=result.html,
            metadata={},
            relevance_score=1.0,
            status=ResultStatus.PENDING
        )

        search_results.append(search_result)

    return search_results
```

---

## ç§¯åˆ†æ¶ˆè€—è®¡ç®—

### è®¡ç®—é€»è¾‘

```python
class FirecrawlCreditsCalculator:
    """Firecrawlç§¯åˆ†æ¶ˆè€—è®¡ç®—å™¨"""

    @staticmethod
    def calculate_map_scrape_credits(
        map_calls: int,
        urls_scraped: int
    ) -> int:
        """è®¡ç®—Map + Scrapeæ“ä½œçš„ç§¯åˆ†æ¶ˆè€—

        Args:
            map_calls: Map APIè°ƒç”¨æ¬¡æ•°ï¼ˆé€šå¸¸ä¸º1ï¼‰
            urls_scraped: Scrapeçš„URLæ•°é‡

        Returns:
            int: æ€»ç§¯åˆ†æ¶ˆè€—
        """
        map_cost = map_calls * 1  # Map API: 1 credit per call
        scrape_cost = urls_scraped * 1  # Scrape API: 1 credit per URL

        return map_cost + scrape_cost
```

### æˆæœ¬å¯¹æ¯”ç¤ºä¾‹

**åœºæ™¯**: çˆ¬å–åšå®¢ç½‘ç«™ï¼Œåªéœ€è¦æœ€è¿‘30å¤©çš„100ç¯‡æ–‡ç« 

**Crawl APIæ–¹å¼**ï¼š
```
- ç½‘ç«™æ€»é¡µé¢: 1000é¡µ
- çˆ¬å–æ–¹å¼: é€’å½’çˆ¬å–æ‰€æœ‰é¡µé¢
- ç§¯åˆ†æ¶ˆè€—: 500-1000 credits
- æ—¶é—´: ~20-30åˆ†é’Ÿ
```

**Map + Scrapeæ–¹å¼**ï¼š
```
- Map API: å‘ç°æ‰€æœ‰URL (1 credit)
- URLè¿‡æ»¤: 1000 â†’ 600 (è¿‡æ»¤40%)
- Scrape API: åªscrape 100ä¸ªç¬¦åˆæ—¶é—´çš„é¡µé¢ (100 credits)
- æ€»ç§¯åˆ†: 101 credits
- æ—¶é—´: ~5-10åˆ†é’Ÿ
- èŠ‚çœ: ~80-90%
```

---

## å®æ–½è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŠŸèƒ½ï¼ˆv2.1.0ï¼‰

**å®Œæˆæ—¶é—´**: 2025-11-06

- âœ… æ‰©å±•FirecrawlAdapterï¼šæ–°å¢map()æ–¹æ³•
- âœ… åˆ›å»ºMapScrapeConfigé…ç½®ç±»
- âœ… å®ç°MapScrapeExecutoråŸºç¡€æ¡†æ¶
- âœ… å®ç°_execute_map()æ–¹æ³•
- âœ… å®ç°_batch_scrape()æ–¹æ³•
- âœ… å®ç°_filter_by_date()æ–¹æ³•
- âœ… æ›´æ–°TaskTypeæšä¸¾
- âœ… åœ¨ExecutorFactoryæ³¨å†Œ
- âœ… æ›´æ–°ç§¯åˆ†è®¡ç®—å™¨

### Phase 2: URLè¿‡æ»¤ç³»ç»Ÿï¼ˆv2.1.2ï¼‰

**å®Œæˆæ—¶é—´**: 2025-11-10

- âœ… è®¾è®¡æ¨¡å—åŒ–è¿‡æ»¤æ¶æ„ï¼ˆSOLIDåŸåˆ™ï¼‰
- âœ… å®ç°URLFilteræ¥å£
- âœ… å®ç°PathKeywordFilter
- âœ… å®ç°FileTypeFilter
- âœ… å®ç°DomainFilter
- âœ… å®ç°URLDeduplicator
- âœ… å®ç°FilterChainï¼ˆè´£ä»»é“¾æ¨¡å¼ï¼‰
- âœ… å®ç°FilterRegistryï¼ˆå•ä¾‹+å·¥å‚æ¨¡å¼ï¼‰
- âœ… å®ç°PipelineBuilderï¼ˆå»ºé€ è€…æ¨¡å¼ï¼‰
- âœ… é›†æˆåˆ°MapScrapeExecutor

### Phase 3: æµ‹è¯•å’Œä¼˜åŒ–

**å®Œæˆæ ‡å‡†**:
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ >80%
- âœ… çœŸå®æµ‹è¯•è¿‡æ»¤ç‡è¾¾åˆ°35-45%
- âœ… è¯¯æ€ç‡ <5%
- âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼ˆè¿‡æ»¤è€—æ—¶ <50msï¼‰

---

## æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

1. **ç²¾ç¡®æ§åˆ¶**ï¼šMap APIæä¾›ç²¾ç¡®çš„URLå‘ç°èƒ½åŠ›
2. **æ™ºèƒ½è¿‡æ»¤**ï¼šæ¨¡å—åŒ–è¿‡æ»¤ç³»ç»Ÿï¼Œå¯æ‰©å±•æ€§å¼º
3. **æˆæœ¬ä¼˜åŒ–**ï¼šèŠ‚çœ80-90%ç§¯åˆ†ï¼ˆç›¸æ¯”Crawl APIï¼‰
4. **æ—¶é—´è¿‡æ»¤**ï¼šæ”¯æŒå‘å¸ƒæ—¶é—´èŒƒå›´è¿‡æ»¤
5. **æ•°æ®å…¼å®¹**ï¼šå®Œå…¨å…¼å®¹ç°æœ‰æ•°æ®åº“ç»“æ„

### é€‚ç”¨åœºæ™¯

- âœ… å®šæœŸç›‘æ§ç‰¹å®šç½‘ç«™æœ€æ–°å†…å®¹
- âœ… åªéœ€è¦ç‰¹å®šæ—¶é—´èŒƒå›´çš„æ–‡ç« 
- âœ… éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡
- âœ… å…³æ³¨APIç§¯åˆ†æˆæœ¬

### ä¸é€‚ç”¨åœºæ™¯

- âŒ éœ€è¦å®Œæ•´å½’æ¡£æ•´ä¸ªç½‘ç«™
- âŒ ç½‘ç«™URLç»“æ„ä¸è§„åˆ™
- âŒ æ— æ³•é€šè¿‡Map APIå‘ç°æ‰€æœ‰é¡µé¢

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Development Team
**æœ€åæ›´æ–°**: 2025-11-14
**çŠ¶æ€**: å·²å®æ–½ï¼ˆv2.1.2ï¼‰
