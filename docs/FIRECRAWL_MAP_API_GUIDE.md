# Firecrawl Map API ä½¿ç”¨æŒ‡å—

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-06

---

## ğŸ“‹ ç›®å½•

1. [Map API æ¦‚è¿°](#map-api-æ¦‚è¿°)
2. [ä¸Crawl APIå¯¹æ¯”](#ä¸crawl-apiå¯¹æ¯”)
3. [APIè¯¦ç»†è¯´æ˜](#apiè¯¦ç»†è¯´æ˜)
4. [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## Map API æ¦‚è¿°

### ä»€ä¹ˆæ˜¯Map API

Firecrawl **Map API** æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘ç°ç½‘ç«™æ‰€æœ‰å¯è®¿é—®URLçš„å·¥å…·ï¼Œå®ƒå¯ä»¥ï¼š

- ğŸ—ºï¸ **å¿«é€Ÿæ‰«æ**ï¼šå‡ ç§’å†…è·å–ç½‘ç«™çš„å®Œæ•´URLåˆ—è¡¨
- ğŸ¯ **æ™ºèƒ½å‘ç°**ï¼šä½¿ç”¨sitemap + æ™ºèƒ½çˆ¬å–ç®—æ³•
- ğŸ” **å…³é”®è¯è¿‡æ»¤**ï¼šå¯é€‰çš„searchå‚æ•°è¿›è¡ŒURLè¿‡æ»¤
- ğŸ’° **å›ºå®šæˆæœ¬**ï¼šæ¯æ¬¡è°ƒç”¨åªæ¶ˆè€—1ä¸ªç§¯åˆ†

### æ ¸å¿ƒç‰¹ç‚¹

| ç‰¹ç‚¹ | è¯´æ˜ |
|------|------|
| **é€Ÿåº¦** | é€šå¸¸<5ç§’å®Œæˆæ•´ä¸ªç½‘ç«™çš„URLå‘ç° |
| **å‡†ç¡®æ€§** | ç»“åˆsitemapå’Œæ™ºèƒ½çˆ¬å–ï¼Œå‘ç°ç‡>95% |
| **æˆæœ¬** | å›ºå®š1 creditï¼Œæ— è®ºç½‘ç«™å¤§å° |
| **è¾“å‡º** | URLåˆ—è¡¨ + åŸºæœ¬å…ƒæ•°æ®ï¼ˆtitle, descriptionï¼‰ |
| **é™åˆ¶** | é»˜è®¤è¿”å›5000ä¸ªURL |

### å·¥ä½œåŸç†

```
è¾“å…¥: ç½‘ç«™èµ·å§‹URL
   â†“
[ä¼˜å…ˆä½¿ç”¨Sitemap]
   â†“
[æ™ºèƒ½çˆ¬å–è¡¥å……]
   â†“
[å»é‡å’Œæ’åº]
   â†“
è¾“å‡º: URLåˆ—è¡¨ + å…ƒæ•°æ®
```

---

## ä¸Crawl APIå¯¹æ¯”

### åŠŸèƒ½å¯¹æ¯”è¡¨

| ç»´åº¦ | Map API | Crawl API |
|------|---------|-----------|
| **ç›®çš„** | å‘ç°URL | çˆ¬å–å†…å®¹ |
| **é€Ÿåº¦** | æå¿«ï¼ˆ<5ç§’ï¼‰ | è¾ƒæ…¢ï¼ˆåˆ†é’Ÿçº§ï¼‰ |
| **è¾“å‡º** | URLåˆ—è¡¨ + å…ƒæ•°æ® | å®Œæ•´é¡µé¢å†…å®¹ |
| **å†…å®¹** | âŒ æ— é¡µé¢å†…å®¹ | âœ… Markdown + HTML |
| **æ—¶é—´ä¿¡æ¯** | âŒ æ— å‘å¸ƒæ—¶é—´ | âœ… å®Œæ•´metadata |
| **ç§¯åˆ†** | 1 credit | N creditsï¼ˆN=é¡µé¢æ•°ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | URLå‘ç° | å†…å®¹è·å– |

### ä½¿ç”¨å†³ç­–æ ‘

```
éœ€è¦è·å–é¡µé¢å†…å®¹ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨Crawl APIæˆ–Map+Scrape
â””â”€ å¦ â†’ ä½¿ç”¨Map API

éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–å“ªäº›é¡µé¢ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨Map API + Scrape API
â””â”€ å¦ â†’ ä½¿ç”¨Crawl API

éœ€è¦èŠ‚çœç§¯åˆ†ï¼Ÿ
â”œâ”€ æ˜¯ï¼Œåªéœ€è¦éƒ¨åˆ†é¡µé¢ â†’ Map API + Scrape
â””â”€ å¦ï¼Œéœ€è¦å…¨éƒ¨å†…å®¹ â†’ Crawl API

ç½‘ç«™æœ‰æ˜ç¡®çš„URLç»“æ„ï¼Ÿ
â”œâ”€ æ˜¯ â†’ Map APIæ•ˆæœå¥½
â””â”€ å¦ â†’ Crawl APIæ›´å…¨é¢
```

### ç»„åˆä½¿ç”¨ç­–ç•¥

**Map + Scrapeï¼ˆæ¨èï¼‰**ï¼š
```
åœºæ™¯: åªéœ€è¦æœ€è¿‘30å¤©çš„æ–‡ç« 
æˆæœ¬: 1 (map) + 100 (scrape) = 101 credits
ä¼˜åŠ¿: ç²¾ç¡®æ§åˆ¶ + èŠ‚çœç§¯åˆ†
```

**Crawl APIï¼ˆä¼ ç»Ÿï¼‰**ï¼š
```
åœºæ™¯: éœ€è¦å®Œæ•´å½’æ¡£
æˆæœ¬: 500-1000 credits
ä¼˜åŠ¿: ç®€å•ç›´æ¥
```

---

## APIè¯¦ç»†è¯´æ˜

### ç«¯ç‚¹ä¿¡æ¯

```
POST https://api.firecrawl.dev/v2/map
```

### è¯·æ±‚æ ¼å¼

#### åŸºæœ¬è¯·æ±‚

```bash
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://example.com"
    }'
```

#### å¸¦æœç´¢è¿‡æ»¤

```bash
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://example.com",
      "search": "blog",
      "limit": 1000
    }'
```

### è¯·æ±‚å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `url` | string | âœ… | - | ç½‘ç«™èµ·å§‹URL |
| `search` | string | âŒ | null | æœç´¢å…³é”®è¯ï¼ˆURLè¿‡æ»¤ï¼‰ |
| `limit` | integer | âŒ | 5000 | è¿”å›URLæ•°é‡é™åˆ¶ |

### å“åº”æ ¼å¼

#### æˆåŠŸå“åº”

```json
{
  "success": true,
  "links": [
    {
      "url": "https://example.com/blog/post-1",
      "title": "First Blog Post",
      "description": "This is my first blog post about..."
    },
    {
      "url": "https://example.com/blog/post-2",
      "title": "Second Blog Post",
      "description": "In this post, I discuss..."
    },
    {
      "url": "https://example.com/about",
      "title": "About Us",
      "description": "Learn more about our company..."
    }
  ]
}
```

#### å“åº”å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `success` | boolean | è¯·æ±‚æ˜¯å¦æˆåŠŸ |
| `links` | array | URLåˆ—è¡¨ |
| `links[].url` | string | å®Œæ•´URL |
| `links[].title` | string | é¡µé¢æ ‡é¢˜ |
| `links[].description` | string | é¡µé¢æè¿° |

### Python SDK ç¤ºä¾‹

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# åŸºæœ¬ä½¿ç”¨
result = app.map_url("https://example.com")
print(f"å‘ç° {len(result['links'])} ä¸ªURL")

# å¸¦æœç´¢è¿‡æ»¤
result = app.map_url(
    "https://example.com",
    params={"search": "blog", "limit": 1000}
)

# å¤„ç†ç»“æœ
for link in result['links']:
    print(f"{link['title']}: {link['url']}")
```

### JavaScript SDK ç¤ºä¾‹

```javascript
import Firecrawl from '@mendable/firecrawl-js';

const app = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

// åŸºæœ¬ä½¿ç”¨
const result = await app.map('https://example.com');
console.log(`å‘ç° ${result.links.length} ä¸ªURL`);

// å¸¦æœç´¢è¿‡æ»¤
const blogResult = await app.map('https://example.com', {
  search: 'blog',
  limit: 1000
});

// å¤„ç†ç»“æœ
blogResult.links.forEach(link => {
  console.log(`${link.title}: ${link.url}`);
});
```

---

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: ç²¾ç¡®çˆ¬å–ç‰¹å®šå†…å®¹

**éœ€æ±‚**ï¼šåªçˆ¬å–åšå®¢æ–‡ç« ï¼Œä¸éœ€è¦å…¶ä»–é¡µé¢

**æ–¹æ¡ˆ**ï¼š
```python
# 1. ä½¿ç”¨Map APIå‘ç°æ‰€æœ‰URLï¼Œæœç´¢"blog"
map_result = app.map_url("https://example.com", params={"search": "blog"})

# 2. æ‰¹é‡scrapeè¿™äº›URL
for link in map_result['links']:
    content = app.scrape_url(link['url'])
    # ä¿å­˜å†…å®¹
```

**ä¼˜åŠ¿**ï¼š
- åªçˆ¬å–åšå®¢é¡µé¢ï¼ŒèŠ‚çœç§¯åˆ†
- å¿«é€Ÿå‘ç°æ‰€æœ‰åšå®¢URL
- é¿å…çˆ¬å–ä¸ç›¸å…³é¡µé¢

### åœºæ™¯2: æ—¶é—´èŒƒå›´çˆ¬å–

**éœ€æ±‚**ï¼šåªè·å–æœ€è¿‘30å¤©çš„æ–‡ç« 

**æ–¹æ¡ˆ**ï¼š
```python
from datetime import datetime, timedelta

# 1. Mapå‘ç°æ‰€æœ‰URL
map_result = app.map_url("https://example.com/blog")

# 2. Scrapeå¹¶è¿‡æ»¤
cutoff_date = datetime.now() - timedelta(days=30)
recent_articles = []

for link in map_result['links']:
    content = app.scrape_url(link['url'])

    # æ£€æŸ¥å‘å¸ƒæ—¶é—´
    pub_date_str = content['metadata'].get('publishedDate')
    if pub_date_str:
        pub_date = datetime.fromisoformat(pub_date_str)
        if pub_date >= cutoff_date:
            recent_articles.append(content)

print(f"å‘ç° {len(recent_articles)} ç¯‡æœ€è¿‘30å¤©çš„æ–‡ç« ")
```

**ç§¯åˆ†å¯¹æ¯”**ï¼š
- Crawlå…¨ç«™ï¼š1000 credits
- Map+Scrapeï¼š1 + 50 = 51 creditsï¼ˆå‡è®¾50ç¯‡ç¬¦åˆæ¡ä»¶ï¼‰
- èŠ‚çœï¼š95%

### åœºæ™¯3: ç½‘ç«™ç»“æ„åˆ†æ

**éœ€æ±‚**ï¼šåˆ†æç½‘ç«™çš„URLç»“æ„

**æ–¹æ¡ˆ**ï¼š
```python
from urllib.parse import urlparse
from collections import Counter

# è·å–æ‰€æœ‰URL
map_result = app.map_url("https://example.com")

# åˆ†æURLè·¯å¾„
paths = [urlparse(link['url']).path for link in map_result['links']]
path_segments = [p.split('/')[1] for p in paths if len(p.split('/')) > 1]

# ç»Ÿè®¡
counter = Counter(path_segments)
print("URLç»“æ„åˆ†æ:")
for segment, count in counter.most_common(10):
    print(f"  /{segment}/: {count} ä¸ªé¡µé¢")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
URLç»“æ„åˆ†æ:
  /blog/: 150 ä¸ªé¡µé¢
  /docs/: 80 ä¸ªé¡µé¢
  /products/: 30 ä¸ªé¡µé¢
  /about/: 5 ä¸ªé¡µé¢
```

### åœºæ™¯4: å¢é‡çˆ¬å–

**éœ€æ±‚**ï¼šå®šæœŸçˆ¬å–ï¼Œåªè·å–æ–°å¢é¡µé¢

**æ–¹æ¡ˆ**ï¼š
```python
# é¦–æ¬¡çˆ¬å–
initial_urls = set(link['url'] for link in app.map_url("https://example.com")['links'])
save_to_db(initial_urls)

# åç»­çˆ¬å–ï¼ˆ7å¤©åï¼‰
current_urls = set(link['url'] for link in app.map_url("https://example.com")['links'])
new_urls = current_urls - initial_urls

print(f"å‘ç° {len(new_urls)} ä¸ªæ–°é¡µé¢")

# åªscrapeæ–°é¡µé¢
for url in new_urls:
    content = app.scrape_url(url)
    save_to_db(content)
```

---

## æœ€ä½³å®è·µ

### 1. åˆç†ä½¿ç”¨searchå‚æ•°

**æ¨è**ï¼š
```python
# æ˜ç¡®çš„è¿‡æ»¤æ¡ä»¶
map_result = app.map_url("https://example.com", params={"search": "blog"})
```

**ä¸æ¨è**ï¼š
```python
# è¿‡äºå®½æ³›çš„æœç´¢
map_result = app.map_url("https://example.com", params={"search": "a"})
```

### 2. è®¾ç½®åˆç†çš„limit

**åœºæ™¯åˆ¤æ–­**ï¼š
```python
# å°å‹ç½‘ç«™ï¼ˆ<1000é¡µï¼‰
params = {"limit": 1000}

# ä¸­å‹ç½‘ç«™ï¼ˆ<5000é¡µï¼‰
params = {"limit": 5000}  # é»˜è®¤å€¼

# å¤§å‹ç½‘ç«™ï¼ˆ>5000é¡µï¼‰
# åˆ†æ‰¹mapä¸åŒsection
params = {"limit": 5000, "search": "blog"}
params = {"limit": 5000, "search": "docs"}
```

### 3. ç¼“å­˜Mapç»“æœ

```python
import json
from pathlib import Path

def get_urls_with_cache(url: str, cache_file: str = "map_cache.json"):
    """ä½¿ç”¨ç¼“å­˜çš„Mapç»“æœ"""
    cache_path = Path(cache_file)

    # æ£€æŸ¥ç¼“å­˜
    if cache_path.exists():
        with open(cache_path) as f:
            cached = json.load(f)
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆä¾‹å¦‚ï¼š24å°æ—¶ï¼‰
            if time.time() - cached['timestamp'] < 86400:
                return cached['links']

    # é‡æ–°Map
    result = app.map_url(url)

    # ä¿å­˜ç¼“å­˜
    with open(cache_path, 'w') as f:
        json.dump({
            'timestamp': time.time(),
            'links': result['links']
        }, f)

    return result['links']
```

### 4. é”™è¯¯å¤„ç†

```python
from firecrawl import MapAPIError

try:
    result = app.map_url("https://example.com")
except MapAPIError as e:
    logger.error(f"Map APIå¤±è´¥: {e}")
    # Fallback: ä½¿ç”¨Crawl API
    result = app.crawl_url("https://example.com")
except Exception as e:
    logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
    raise
```

### 5. ç»“åˆå…¶ä»–API

**Map + Scrapeï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰**ï¼š
```python
# 1. Mapå‘ç°URL
urls = [link['url'] for link in app.map_url("https://example.com")['links']]

# 2. æ‰¹é‡Scrape
results = []
for url in urls[:100]:  # é™åˆ¶æ•°é‡
    try:
        content = app.scrape_url(url)
        results.append(content)
    except Exception as e:
        logger.warning(f"Scrapeå¤±è´¥ {url}: {e}")

print(f"æˆåŠŸçˆ¬å– {len(results)} ä¸ªé¡µé¢")
```

**Map + Crawlï¼ˆæ··åˆæ–¹æ¡ˆï¼‰**ï¼š
```python
# 1. Mapåˆ†æç»“æ„
map_result = app.map_url("https://example.com")
blog_urls = [l['url'] for l in map_result['links'] if '/blog/' in l['url']]

# 2. Crawlç‰¹å®šsection
for url in blog_urls[:10]:  # å‰10ä¸ªåšå®¢åˆ†ç±»
    crawl_result = app.crawl_url(url, limit=50)
    # å¤„ç†ç»“æœ
```

---

## å¸¸è§é—®é¢˜

### Q1: Map APIä¸ºä»€ä¹ˆä¸è¿”å›é¡µé¢å†…å®¹ï¼Ÿ

**A**: Map APIçš„è®¾è®¡ç›®æ ‡æ˜¯**å¿«é€Ÿå‘ç°URL**ï¼Œè€Œä¸æ˜¯è·å–å†…å®¹ã€‚è¿™æ ·å¯ä»¥ï¼š
- æå¿«çš„å“åº”é€Ÿåº¦ï¼ˆ<5ç§’ï¼‰
- å›ºå®šçš„ä½æˆæœ¬ï¼ˆ1 creditï¼‰
- è®©ç”¨æˆ·ç²¾ç¡®æ§åˆ¶åç»­çˆ¬å–å“ªäº›é¡µé¢

å¦‚æœéœ€è¦å†…å®¹ï¼Œä½¿ç”¨**Map + Scrape**æˆ–**Crawl API**ã€‚

### Q2: Map APIèƒ½å‘ç°æ‰€æœ‰é¡µé¢å—ï¼Ÿ

**A**: Map APIç»“åˆäº†sitemapå’Œæ™ºèƒ½çˆ¬å–ï¼Œå‘ç°ç‡é€šå¸¸>95%ï¼Œä½†ä»¥ä¸‹æƒ…å†µå¯èƒ½é—æ¼ï¼š
- JavaScriptåŠ¨æ€ç”Ÿæˆçš„é“¾æ¥
- éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®çš„é¡µé¢
- éšè—åœ¨å¤æ‚äº¤äº’åçš„é“¾æ¥

å¯¹äºå®Œæ•´æ€§è¦æ±‚æé«˜çš„åœºæ™¯ï¼Œå»ºè®®ä½¿ç”¨**Crawl API**ã€‚

### Q3: searchå‚æ•°å¦‚ä½•å·¥ä½œï¼Ÿ

**A**: `search`å‚æ•°ä¼šè¿‡æ»¤URLå’Œæ ‡é¢˜ä¸­åŒ…å«å…³é”®è¯çš„é¡µé¢ï¼š

```python
# åªè¿”å›URLæˆ–æ ‡é¢˜åŒ…å«"blog"çš„é¡µé¢
result = app.map_url("https://example.com", params={"search": "blog"})

# ç¤ºä¾‹ç»“æœ:
# âœ… https://example.com/blog/post-1
# âœ… https://example.com/about (æ ‡é¢˜åŒ…å«"blog")
# âŒ https://example.com/products
```

### Q4: limitå‚æ•°å¦‚ä½•è®¾ç½®ï¼Ÿ

**A**: æ ¹æ®ç½‘ç«™è§„æ¨¡è®¾ç½®ï¼š

| ç½‘ç«™è§„æ¨¡ | æ¨èlimit | è¯´æ˜ |
|----------|-----------|------|
| å°å‹ | 1000 | ä¸ªäººåšå®¢ã€å°ç½‘ç«™ |
| ä¸­å‹ | 5000 | ä¼ä¸šç½‘ç«™ã€ä¸­å‹åª’ä½“ |
| å¤§å‹ | åˆ†æ‰¹map | åˆ†sectionå¤šæ¬¡è°ƒç”¨ |

### Q5: Map APIçš„ç§¯åˆ†æˆæœ¬å¦‚ä½•è®¡ç®—ï¼Ÿ

**A**: éå¸¸ç®€å•ï¼š
```
æ¯æ¬¡Mapè°ƒç”¨ = 1 credit
```

æ— è®ºç½‘ç«™å¤§å°ï¼Œæ— è®ºè¿”å›å¤šå°‘URLï¼Œéƒ½æ˜¯å›ºå®š1 creditã€‚

### Q6: ä»€ä¹ˆæ—¶å€™ç”¨Map+Scrapeï¼Œä»€ä¹ˆæ—¶å€™ç”¨Crawlï¼Ÿ

**å†³ç­–è¡¨**ï¼š

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | åŸå›  |
|------|---------|------|
| åªéœ€è¦éƒ¨åˆ†é¡µé¢ | Map+Scrape | èŠ‚çœç§¯åˆ† |
| éœ€è¦æ—¶é—´è¿‡æ»¤ | Map+Scrape | ç²¾ç¡®æ§åˆ¶ |
| å®Œæ•´ç½‘ç«™å½’æ¡£ | Crawl | ç®€å•ç›´æ¥ |
| ä¸ç¡®å®šéœ€è¦å“ªäº›é¡µé¢ | Crawl | å…¨é¢è¦†ç›– |
| ç½‘ç«™ç»“æ„è§„åˆ™ | Map+Scrape | é«˜æ•ˆå‡†ç¡® |
| ç½‘ç«™ç»“æ„å¤æ‚ | Crawl | æ›´å…¨é¢ |

---

## æ€»ç»“

### Map API çš„æ ¸å¿ƒä»·å€¼

1. **å¿«é€Ÿå‘ç°**ï¼šå‡ ç§’å†…è·å–æ‰€æœ‰URL
2. **æˆæœ¬å›ºå®š**ï¼š1 creditæ— è®ºç½‘ç«™å¤§å°
3. **ç²¾ç¡®æ§åˆ¶**ï¼šä¸Scrapeç»„åˆå®ç°ç²¾ç¡®çˆ¬å–
4. **èŠ‚çœç§¯åˆ†**ï¼šé¿å…ä¸å¿…è¦çš„é¡µé¢çˆ¬å–

### æœ€ä½³ä½¿ç”¨æ¨¡å¼

```
Map API (å‘ç°) â†’ æ—¶é—´/å†…å®¹è¿‡æ»¤ â†’ Scrape API (è·å–)
```

è¿™ç§æ¨¡å¼åœ¨ä»¥ä¸‹åœºæ™¯æœ€æœ‰ä»·å€¼ï¼š
- å®šæœŸç›‘æ§ç½‘ç«™æ›´æ–°
- åªéœ€è¦ç‰¹å®šæ—¶é—´èŒƒå›´çš„å†…å®¹
- éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡
- å…³æ³¨APIç§¯åˆ†æˆæœ¬

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Development Team
**æœ€åæ›´æ–°**: 2025-11-06
