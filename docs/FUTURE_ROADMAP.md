# å…³å±±æ™ºèƒ½æƒ…æŠ¥å¤„ç†å¹³å° - æœªæ¥è§„åˆ’ä¸æ¶æ„æ„¿æ™¯

> **æ–‡æ¡£æ€§è´¨**: æœªæ¥è§„åˆ’æ–‡æ¡£ (Future Roadmap)
> **çŠ¶æ€**: è®¾è®¡é˜¶æ®µï¼Œå°šæœªå®æ–½
> **ç‰ˆæœ¬**: 1.0.0
> **æ›´æ–°æ—¥æœŸ**: 2025-01-10
> **ä»£å·**: Guanshan Intelligence System (GIS)

---

## âš ï¸ é‡è¦è¯´æ˜

**æœ¬æ–‡æ¡£æè¿°çš„æ˜¯å…³å±±æ™ºèƒ½ç³»ç»Ÿçš„æœªæ¥æ„¿æ™¯å’Œé•¿æœŸè§„åˆ’**ï¼ŒåŒ…å«å¤§é‡å°šæœªå®ç°çš„åŠŸèƒ½å’Œæ¶æ„è®¾è®¡ã€‚

### å½“å‰å®ç°çŠ¶æ€
âœ… **å·²å®ç°**: å®šæ—¶æœç´¢ä»»åŠ¡ç³»ç»Ÿ (å‚è§ `SYSTEM_ARCHITECTURE.md`)
- MongoDBæŒä¹…åŒ–å­˜å‚¨
- APSchedulerä»»åŠ¡è°ƒåº¦
- Firecrawl APIé›†æˆ
- RESTful APIæ¥å£

âŒ **æœªå®ç°** (æœ¬æ–‡æ¡£æè¿°çš„åŠŸèƒ½):
- RAG Pipeline + Reranking
- æ™ºèƒ½ç¿»è¯‘æœåŠ¡
- æŠ¥å‘Šç”Ÿæˆæ¨¡å—
- å‘é‡æ•°æ®åº“é›†æˆ
- Celeryåˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
- LLMæ·±åº¦é›†æˆ

### æ–‡æ¡£ç”¨é€”
- ğŸ“‹ æŠ€æœ¯è§„åˆ’å‚è€ƒ
- ğŸ¯ é•¿æœŸå‘å±•è·¯çº¿å›¾
- ğŸ’¡ æ¶æ„è®¾è®¡æ€è·¯
- ğŸ”® åŠŸèƒ½æ¼”è¿›æ–¹å‘

**å¦‚éœ€äº†è§£å½“å‰ç³»ç»Ÿå®é™…æ¶æ„ï¼Œè¯·å‚é˜…**: [`SYSTEM_ARCHITECTURE.md`](./SYSTEM_ARCHITECTURE.md)

---

## ğŸ“‹ äº§å“å®šä½

å…³å±±å¼€æºä¿¡æ¯é‡‡é›†æ•´ç¼–ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäº **Firecrawl + LLM + RAG Pipeline + Reranking** çš„æ™ºèƒ½æƒ…æŠ¥å¤„ç†å¹³å°ï¼Œå®ç°ä»ä¿¡æ¯é‡‡é›†ã€æ–‡æœ¬æ¸…æ´—ã€æ™ºèƒ½ç¿»è¯‘åˆ°ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚

### æ ¸å¿ƒä»·å€¼
- ğŸŒ **å…¨ç½‘ä¿¡æ¯é‡‡é›†**: æ”¯æŒç½‘é¡µã€PDFã€APIç­‰å¤šæºæ•°æ®é‡‡é›†
- ğŸ§¹ **æ™ºèƒ½æ–‡æœ¬å¤„ç†**: è‡ªåŠ¨æ¸…æ´—ã€æ ¼å¼åŒ–ã€å»å™ª
- ğŸŒ **å¤šè¯­è¨€ç¿»è¯‘**: é›†æˆå¤šå®¶ç¿»è¯‘APIï¼Œæ”¯æŒ60+è¯­è¨€
- ğŸ¤– **RAGæ™ºèƒ½åˆ†æ**: åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ·±åº¦åˆ†æ
- ğŸ“Š **ç»“æ„åŒ–æŠ¥å‘Š**: è‡ªåŠ¨ç”Ÿæˆä¸“ä¸šæƒ…æŠ¥æŠ¥å‘Š

---

## 1. ç³»ç»Ÿæ¶æ„æ€»è§ˆ

### 1.1 æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å‰ç«¯å±•ç¤ºå±‚                            â”‚
â”‚           Web UI / API Gateway / WebSocket               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åº”ç”¨æœåŠ¡å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   FastAPI Application Server (å¼‚æ­¥å¤„ç†)           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  â€¢ é‡‡é›†ç®¡ç†  â€¢ ä»»åŠ¡è°ƒåº¦  â€¢ æŠ¥å‘Šç”Ÿæˆ              â”‚   â”‚
â”‚  â”‚  â€¢ ç”¨æˆ·è®¤è¯  â€¢ APIæœåŠ¡   â€¢ WebSocketæ¨é€         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ™ºèƒ½å¤„ç†å±‚ (AI Services)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Firecrawl  â”‚  â”‚     LLM     â”‚  â”‚     RAG      â”‚    â”‚
â”‚  â”‚   çˆ¬è™«å¼•æ“  â”‚  â”‚   GPT/Claude â”‚  â”‚   Pipeline   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ç¿»è¯‘æœåŠ¡   â”‚  â”‚  æ–‡æœ¬æ¸…æ´—   â”‚  â”‚   Reranking  â”‚    â”‚
â”‚  â”‚ DeepL/Googleâ”‚  â”‚   NLPå¤„ç†   â”‚  â”‚    é‡æ’åº    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¶ˆæ¯é˜Ÿåˆ—å±‚                             â”‚
â”‚         RabbitMQ / Kafka (å¼‚æ­¥ä»»åŠ¡å¤„ç†)                   â”‚
â”‚                  Celery (ä»»åŠ¡è°ƒåº¦)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ•°æ®å­˜å‚¨å±‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ MariaDB  â”‚  â”‚ MongoDB  â”‚  â”‚  Redis   â”‚  â”‚ MinIO  â”‚  â”‚
â”‚  â”‚ å…ƒæ•°æ®   â”‚  â”‚ æ–‡æ¡£å­˜å‚¨ â”‚  â”‚   ç¼“å­˜   â”‚  â”‚ æ–‡ä»¶   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          å‘é‡æ•°æ®åº“ (Qdrant/Milvus)              â”‚   â”‚
â”‚  â”‚              åµŒå…¥å‘é‡å­˜å‚¨ä¸æ£€ç´¢                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| ç»„ä»¶ç±»å‹ | æŠ€æœ¯é€‰å‹ | ç”¨é€” | é€‰å‹ç†ç”± |
|---------|---------|------|---------|
| **Webæ¡†æ¶** | FastAPI | APIæœåŠ¡ | å¼‚æ­¥æ”¯æŒã€é«˜æ€§èƒ½ã€è‡ªåŠ¨æ–‡æ¡£ |
| **çˆ¬è™«å¼•æ“** | Firecrawl | æ•°æ®é‡‡é›† | å¼ºå¤§çš„ç½‘é¡µè§£æã€JSæ¸²æŸ“æ”¯æŒ |
| **LLMé›†æˆ** | LangChain | AIç¼–æ’ | ç»Ÿä¸€çš„LLMæ¥å£ã€RAGæ”¯æŒ |
| **å‘é‡æ•°æ®åº“** | Qdrant | å‘é‡æ£€ç´¢ | é«˜æ€§èƒ½ã€æ”¯æŒè¿‡æ»¤ã€æ˜“æ‰©å±• |
| **æ–‡æ¡£æ•°æ®åº“** | MongoDB | éç»“æ„åŒ–å­˜å‚¨ | çµæ´»Schemaã€æ¨ªå‘æ‰©å±• |
| **å…³ç³»æ•°æ®åº“** | MariaDB | ç»“æ„åŒ–æ•°æ® | ACIDäº‹åŠ¡ã€æˆç†Ÿç¨³å®š |
| **æ¶ˆæ¯é˜Ÿåˆ—** | RabbitMQ | å¼‚æ­¥å¤„ç† | å¯é æ€§é«˜ã€æ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ— |
| **ä»»åŠ¡è°ƒåº¦** | Celery | å®šæ—¶ä»»åŠ¡ | åˆ†å¸ƒå¼ã€æ”¯æŒä»»åŠ¡é“¾ |
| **ç¼“å­˜** | Redis | é«˜é€Ÿç¼“å­˜ | å†…å­˜å­˜å‚¨ã€æ”¯æŒå‘å¸ƒè®¢é˜… |
| **æ–‡ä»¶å­˜å‚¨** | MinIO | å¯¹è±¡å­˜å‚¨ | S3å…¼å®¹ã€ç§æœ‰éƒ¨ç½² |

---

## 2. åŠŸèƒ½æ¨¡å—è®¾è®¡

### 2.1 ä¿¡æ¯é‡‡é›†æ¨¡å—

```python
# æ¨¡å—ç»“æ„
crawler/
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ firecrawl_engine.py    # Firecrawlé›†æˆ
â”‚   â”œâ”€â”€ scrapy_engine.py       # Scrapyå¤‡ç”¨
â”‚   â””â”€â”€ api_crawler.py         # APIæ•°æ®æº
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ html_parser.py         # HTMLè§£æ
â”‚   â”œâ”€â”€ pdf_parser.py          # PDFæå–
â”‚   â””â”€â”€ json_parser.py         # JSONå¤„ç†
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ task_queue.py          # ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
â”‚   â”œâ”€â”€ rate_limiter.py        # è®¿é—®é¢‘ç‡æ§åˆ¶
â”‚   â””â”€â”€ proxy_pool.py          # ä»£ç†æ± ç®¡ç†
â””â”€â”€ storage/
    â””â”€â”€ raw_data_store.py      # åŸå§‹æ•°æ®å­˜å‚¨
```

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- å¤šæºæ•°æ®é‡‡é›†ï¼ˆç½‘é¡µã€RSSã€APIã€æ–‡ä»¶ï¼‰
- JavaScriptæ¸²æŸ“æ”¯æŒ
- åçˆ¬è™«ç­–ç•¥ï¼ˆä»£ç†æ± ã€User-Agentè½®æ¢ï¼‰
- å¢é‡é‡‡é›†ä¸å»é‡
- é‡‡é›†ä»»åŠ¡è°ƒåº¦ä¸ç›‘æ§

### 2.2 æ–‡æœ¬å¤„ç†æ¨¡å—

```python
# æ•°æ®æ¸…æ´—æµæ°´çº¿
class TextProcessor:
    async def process_pipeline(self, raw_text: str) -> ProcessedDocument:
        # 1. åŸºç¡€æ¸…æ´—
        text = await self.clean_html(raw_text)
        text = await self.remove_noise(text)
        
        # 2. æ ¼å¼æ ‡å‡†åŒ–
        text = await self.normalize_format(text)
        
        # 3. è¯­è¨€æ£€æµ‹
        language = await self.detect_language(text)
        
        # 4. åˆ†å¥åˆ†æ®µ
        segments = await self.segment_text(text)
        
        # 5. å®ä½“è¯†åˆ«
        entities = await self.extract_entities(segments)
        
        # 6. å…³é”®è¯æå–
        keywords = await self.extract_keywords(segments)
        
        return ProcessedDocument(
            text=text,
            language=language,
            segments=segments,
            entities=entities,
            keywords=keywords
        )
```

### 2.3 æ™ºèƒ½ç¿»è¯‘æ¨¡å—

```python
# ç¿»è¯‘æœåŠ¡ç®¡ç†
class TranslationService:
    def __init__(self):
        self.engines = {
            'deepl': DeepLTranslator(),
            'google': GoogleTranslator(),
            'baidu': BaiduTranslator(),
            'openai': OpenAITranslator()
        }
    
    async def translate(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        engine: str = 'auto'
    ) -> TranslationResult:
        # æ™ºèƒ½é€‰æ‹©ç¿»è¯‘å¼•æ“
        if engine == 'auto':
            engine = self.select_best_engine(source_lang, target_lang)
        
        # æ‰¹é‡å¤„ç†é•¿æ–‡æœ¬
        if len(text) > 5000:
            return await self.batch_translate(text, source_lang, target_lang)
        
        # æ‰§è¡Œç¿»è¯‘
        result = await self.engines[engine].translate(
            text, source_lang, target_lang
        )
        
        # è´¨é‡è¯„ä¼°
        quality_score = await self.evaluate_quality(result)
        
        return TranslationResult(
            text=result.text,
            engine=engine,
            quality_score=quality_score
        )
```

### 2.4 RAGæ™ºèƒ½åˆ†ææ¨¡å—

```python
# RAG Pipelineå®ç°
class RAGPipeline:
    def __init__(self):
        self.embedder = SentenceTransformer('multilingual-e5-large')
        self.vector_db = QdrantClient(url='localhost:6333')
        self.llm = ChatOpenAI(model='gpt-4')
        self.reranker = CrossEncoder('ms-marco-MiniLM-L-6-v2')
    
    async def process(self, query: str, context_docs: List[Document]) -> str:
        # 1. æ–‡æ¡£åˆ‡ç‰‡
        chunks = await self.chunk_documents(context_docs)
        
        # 2. å‘é‡åŒ–
        embeddings = await self.embed_chunks(chunks)
        
        # 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        await self.store_vectors(chunks, embeddings)
        
        # 4. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = await self.retrieve(query, top_k=20)
        
        # 5. é‡æ’åº
        reranked_docs = await self.rerank(query, relevant_docs, top_k=5)
        
        # 6. ç”Ÿæˆç­”æ¡ˆ
        answer = await self.generate_answer(query, reranked_docs)
        
        return answer
    
    async def retrieve(self, query: str, top_k: int = 20) -> List[Document]:
        # å‘é‡æ£€ç´¢
        query_embedding = self.embedder.encode(query)
        
        search_result = self.vector_db.search(
            collection_name="intelligence",
            query_vector=query_embedding,
            limit=top_k,
            query_filter=Filter(...)  # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        )
        
        return [hit.payload for hit in search_result]
    
    async def rerank(
        self,
        query: str,
        docs: List[Document],
        top_k: int = 5
    ) -> List[Document]:
        # äº¤å‰ç¼–ç å™¨é‡æ’åº
        pairs = [[query, doc.content] for doc in docs]
        scores = self.reranker.predict(pairs)
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_docs = sorted(
            zip(docs, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [doc for doc, _ in sorted_docs[:top_k]]
```

### 2.5 æŠ¥å‘Šç”Ÿæˆæ¨¡å—

```python
# æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ
class ReportGenerator:
    def __init__(self):
        self.llm = ChatOpenAI(model='gpt-4')
        self.templates = TemplateManager()
        self.formatter = DocumentFormatter()
    
    async def generate_report(
        self,
        analysis_results: Dict,
        report_type: str = 'intelligence'
    ) -> Report:
        # 1. é€‰æ‹©æŠ¥å‘Šæ¨¡æ¿
        template = self.templates.get_template(report_type)
        
        # 2. ç»“æ„åŒ–å†…å®¹ç”Ÿæˆ
        sections = await self.generate_sections(analysis_results, template)
        
        # 3. æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ
        executive_summary = await self.generate_summary(sections)
        
        # 4. å›¾è¡¨ç”Ÿæˆ
        charts = await self.generate_charts(analysis_results)
        
        # 5. æ ¼å¼åŒ–è¾“å‡º
        formatted_report = await self.formatter.format(
            title=analysis_results['title'],
            summary=executive_summary,
            sections=sections,
            charts=charts,
            metadata=analysis_results['metadata']
        )
        
        # 6. å¯¼å‡ºå¤šç§æ ¼å¼
        outputs = {
            'html': await self.export_html(formatted_report),
            'pdf': await self.export_pdf(formatted_report),
            'docx': await self.export_docx(formatted_report),
            'json': await self.export_json(formatted_report)
        }
        
        return Report(
            id=str(uuid4()),
            content=formatted_report,
            outputs=outputs,
            created_at=datetime.utcnow()
        )
```

---

## 3. æ•°æ®æµè®¾è®¡

### 3.1 ä¸»æ•°æ®æµç¨‹

```mermaid
graph LR
    A[æ•°æ®æº] --> B[Firecrawlé‡‡é›†]
    B --> C[åŸå§‹æ•°æ®å­˜å‚¨]
    C --> D[æ–‡æœ¬æ¸…æ´—]
    D --> E[è¯­è¨€æ£€æµ‹]
    E --> F{éœ€è¦ç¿»è¯‘?}
    F -->|æ˜¯| G[æ™ºèƒ½ç¿»è¯‘]
    F -->|å¦| H[æ–‡æ¡£åˆ‡ç‰‡]
    G --> H
    H --> I[å‘é‡åŒ–]
    I --> J[å‘é‡æ•°æ®åº“]
    J --> K[RAGæ£€ç´¢]
    K --> L[é‡æ’åº]
    L --> M[LLMåˆ†æ]
    M --> N[æŠ¥å‘Šç”Ÿæˆ]
    N --> O[å¤šæ ¼å¼è¾“å‡º]
```

### 3.2 å¼‚æ­¥ä»»åŠ¡æµ

```python
# Celeryä»»åŠ¡é“¾å®šä¹‰
from celery import chain, group, chord

# å®šä¹‰ä»»åŠ¡é“¾
def create_intelligence_pipeline(source_url: str, config: Dict):
    return chain(
        # é‡‡é›†ä»»åŠ¡
        crawl_task.s(source_url, config),
        
        # å¹¶è¡Œå¤„ç†
        group(
            clean_text_task.s(),
            extract_metadata_task.s(),
            detect_language_task.s()
        ),
        
        # æ¡ä»¶ä»»åŠ¡
        translate_if_needed_task.s(),
        
        # RAGå¤„ç†é“¾
        chain(
            chunk_document_task.s(),
            generate_embeddings_task.s(),
            store_vectors_task.s()
        ),
        
        # åˆ†æå’ŒæŠ¥å‘Š
        chord(
            group(
                rag_analysis_task.s(),
                entity_extraction_task.s(),
                sentiment_analysis_task.s()
            )
        )(generate_report_task.s())
    )
```

---

## 4. APIæ¥å£è®¾è®¡

### 4.1 RESTful API

```python
# APIè·¯ç”±å®šä¹‰
from fastapi import APIRouter, Depends, BackgroundTasks
from typing import List, Optional

router = APIRouter(prefix="/api/v1")

# é‡‡é›†ä»»åŠ¡API
@router.post("/crawl/tasks", response_model=TaskResponse)
async def create_crawl_task(
    request: CrawlRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user)
):
    """
    åˆ›å»ºé‡‡é›†ä»»åŠ¡
    - æ”¯æŒå•ä¸ªURLæˆ–æ‰¹é‡URL
    - å¯é…ç½®é‡‡é›†æ·±åº¦ã€é¢‘ç‡ã€ä»£ç†ç­‰
    """
    task_id = str(uuid4())
    
    # å¼‚æ­¥æ‰§è¡Œé‡‡é›†
    background_tasks.add_task(
        execute_crawl_pipeline,
        task_id=task_id,
        urls=request.urls,
        config=request.config,
        user_id=current_user.id
    )
    
    return TaskResponse(
        task_id=task_id,
        status="pending",
        created_at=datetime.utcnow()
    )

# RAGæŸ¥è¯¢API
@router.post("/intelligence/query", response_model=QueryResponse)
async def query_intelligence(
    request: QueryRequest,
    current_user: User = Depends(get_current_user)
):
    """
    æ™ºèƒ½æŸ¥è¯¢æ¥å£
    - æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢
    - è¿”å›ç›¸å…³æ–‡æ¡£å’Œåˆ†æç»“æœ
    """
    # æ‰§è¡ŒRAGæ£€ç´¢
    results = await rag_pipeline.process(
        query=request.query,
        filters=request.filters,
        top_k=request.top_k
    )
    
    return QueryResponse(
        query=request.query,
        results=results,
        sources=results.sources,
        confidence=results.confidence_score
    )

# æŠ¥å‘Šç”ŸæˆAPI
@router.post("/reports/generate", response_model=ReportResponse)
async def generate_report(
    request: ReportRequest,
    current_user: User = Depends(get_current_user)
):
    """
    ç”Ÿæˆæƒ…æŠ¥æŠ¥å‘Š
    - æ”¯æŒå¤šç§æŠ¥å‘Šæ¨¡æ¿
    - å¯å¯¼å‡ºPDF/Word/HTML
    """
    report = await report_generator.generate(
        data_sources=request.source_ids,
        template=request.template,
        language=request.language,
        format=request.output_format
    )
    
    return ReportResponse(
        report_id=report.id,
        title=report.title,
        download_url=report.download_url,
        preview_url=report.preview_url
    )
```

### 4.2 WebSocketå®æ—¶æ¨é€

```python
# WebSocketè¿æ¥ç®¡ç†
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        await self.send_personal_message(
            {"type": "connection", "message": "Connected successfully"},
            client_id
        )
    
    async def broadcast_task_update(self, task_id: str, update: Dict):
        """å¹¿æ’­ä»»åŠ¡çŠ¶æ€æ›´æ–°"""
        message = {
            "type": "task_update",
            "task_id": task_id,
            "status": update["status"],
            "progress": update["progress"],
            "message": update.get("message"),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        for connection in self.active_connections.values():
            await connection.send_json(message)

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(
    websocket: WebSocket,
    client_id: str,
    token: str = Query(...)
):
    # éªŒè¯token
    user = await verify_ws_token(token)
    if not user:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return
    
    await manager.connect(websocket, client_id)
    
    try:
        while True:
            # æ¥æ”¶å®¢æˆ·ç«¯æ¶ˆæ¯
            data = await websocket.receive_json()
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if data["type"] == "subscribe_task":
                await subscribe_to_task(client_id, data["task_id"])
            elif data["type"] == "query":
                result = await process_realtime_query(data["query"])
                await manager.send_personal_message(result, client_id)
                
    except WebSocketDisconnect:
        manager.disconnect(client_id)
```

---

## 5. å®‰å…¨ä¸åˆè§„

### 5.1 æ•°æ®å®‰å…¨

```python
# æ•°æ®åŠ å¯†å’Œè„±æ•
class DataSecurity:
    def __init__(self):
        self.encryptor = Fernet(settings.ENCRYPTION_KEY)
        self.pii_detector = PIIDetector()
    
    async def process_sensitive_data(self, data: str) -> str:
        # 1. PIIæ£€æµ‹
        pii_entities = await self.pii_detector.detect(data)
        
        # 2. æ•°æ®è„±æ•
        masked_data = await self.mask_pii(data, pii_entities)
        
        # 3. åŠ å¯†å­˜å‚¨
        if self.requires_encryption(data):
            encrypted_data = self.encrypt(masked_data)
            return encrypted_data
        
        return masked_data
    
    def mask_pii(self, text: str, entities: List[PIIEntity]) -> str:
        """PIIæ•°æ®è„±æ•"""
        for entity in entities:
            if entity.type == 'email':
                text = text.replace(entity.value, self.mask_email(entity.value))
            elif entity.type == 'phone':
                text = text.replace(entity.value, self.mask_phone(entity.value))
            elif entity.type == 'id_card':
                text = text.replace(entity.value, '***')
        return text
```

### 5.2 è®¿é—®æ§åˆ¶

```python
# RBACæƒé™æ¨¡å‹
class RBACPermission:
    """åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶"""
    
    ROLES = {
        'admin': ['*'],  # æ‰€æœ‰æƒé™
        'analyst': [
            'crawl:read', 'crawl:create',
            'report:read', 'report:create',
            'intelligence:query'
        ],
        'viewer': [
            'report:read',
            'intelligence:query'
        ]
    }
    
    @classmethod
    def check_permission(
        cls,
        user: User,
        resource: str,
        action: str
    ) -> bool:
        permission = f"{resource}:{action}"
        user_permissions = cls.ROLES.get(user.role, [])
        
        return '*' in user_permissions or permission in user_permissions

# APIæƒé™è£…é¥°å™¨
def require_permission(resource: str, action: str):
    async def permission_checker(
        current_user: User = Depends(get_current_user)
    ):
        if not RBACPermission.check_permission(
            current_user, resource, action
        ):
            raise HTTPException(
                status_code=403,
                detail="Insufficient permissions"
            )
        return current_user
    return permission_checker

# ä½¿ç”¨ç¤ºä¾‹
@router.post("/crawl/tasks")
async def create_task(
    request: CrawlRequest,
    user: User = Depends(require_permission("crawl", "create"))
):
    pass
```

### 5.3 å®¡è®¡æ—¥å¿—

```python
# å®¡è®¡æ—¥å¿—è®°å½•
class AuditLogger:
    async def log_operation(
        self,
        user_id: str,
        operation: str,
        resource: str,
        details: Dict,
        ip_address: str
    ):
        audit_log = {
            "timestamp": datetime.utcnow(),
            "user_id": user_id,
            "operation": operation,
            "resource": resource,
            "details": details,
            "ip_address": ip_address,
            "user_agent": details.get("user_agent"),
            "status": details.get("status", "success")
        }
        
        # å­˜å‚¨åˆ°MongoDB
        await self.audit_collection.insert_one(audit_log)
        
        # å…³é”®æ“ä½œå‘Šè­¦
        if operation in ['delete', 'export', 'admin_action']:
            await self.send_security_alert(audit_log)
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 ç¼“å­˜ç­–ç•¥

```python
# å¤šçº§ç¼“å­˜å®ç°
class CacheManager:
    def __init__(self):
        self.redis_client = Redis(
            host='localhost',
            decode_responses=True,
            connection_pool=BlockingConnectionPool(max_connections=50)
        )
        self.local_cache = TTLCache(maxsize=1000, ttl=300)
    
    async def get_with_cache(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int = 3600
    ):
        # 1. æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            return self.local_cache[key]
        
        # 2. Redisç¼“å­˜
        redis_value = await self.redis_client.get(key)
        if redis_value:
            value = json.loads(redis_value)
            self.local_cache[key] = value
            return value
        
        # 3. è·å–å¹¶ç¼“å­˜
        value = await fetch_func()
        await self.redis_client.setex(
            key,
            ttl,
            json.dumps(value)
        )
        self.local_cache[key] = value
        
        return value
```

### 6.2 æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†ä¼˜åŒ–
class BatchProcessor:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.queue = asyncio.Queue()
        self.processing = False
    
    async def add_item(self, item: Any):
        await self.queue.put(item)
        
        if not self.processing:
            asyncio.create_task(self.process_batch())
    
    async def process_batch(self):
        self.processing = True
        batch = []
        
        while True:
            try:
                # æ”¶é›†æ‰¹æ¬¡
                while len(batch) < self.batch_size:
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=1.0
                    )
                    batch.append(item)
                    
            except asyncio.TimeoutError:
                # è¶…æ—¶åˆ™å¤„ç†å½“å‰æ‰¹æ¬¡
                if batch:
                    await self.process_items(batch)
                    batch = []
                    
                if self.queue.empty():
                    self.processing = False
                    break
    
    async def process_items(self, items: List):
        """æ‰¹é‡å¤„ç†é€»è¾‘"""
        # æ‰¹é‡å‘é‡åŒ–
        embeddings = self.model.encode_batch(items)
        
        # æ‰¹é‡å­˜å‚¨
        await self.vector_db.upsert_batch(embeddings)
```

### 6.3 å¼‚æ­¥å¹¶å‘æ§åˆ¶

```python
# å¹¶å‘é™åˆ¶
class ConcurrencyLimiter:
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def run_with_limit(self, coro):
        async with self.semaphore:
            return await coro

# ä½¿ç”¨ç¤ºä¾‹
async def crawl_multiple_urls(urls: List[str]):
    limiter = ConcurrencyLimiter(max_concurrent=5)
    
    tasks = [
        limiter.run_with_limit(crawl_url(url))
        for url in urls
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

---

## 7. ç›‘æ§ä¸è¿ç»´

### 7.1 ç³»ç»Ÿç›‘æ§

```python
# PrometheusæŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
crawl_requests_total = Counter(
    'crawl_requests_total',
    'Total number of crawl requests',
    ['status', 'source']
)

crawl_duration_seconds = Histogram(
    'crawl_duration_seconds',
    'Time spent crawling',
    ['source_type']
)

active_crawl_tasks = Gauge(
    'active_crawl_tasks',
    'Number of active crawl tasks'
)

rag_query_latency = Histogram(
    'rag_query_latency_seconds',
    'RAG query latency',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

vector_db_size = Gauge(
    'vector_db_documents_total',
    'Total documents in vector database'
)

# ç›‘æ§ä¸­é—´ä»¶
@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    # è®°å½•æŒ‡æ ‡
    if request.url.path.startswith("/api/v1/crawl"):
        crawl_duration_seconds.labels(
            source_type=request.query_params.get("type", "web")
        ).observe(duration)
    
    return response
```

### 7.2 å¥åº·æ£€æŸ¥

```python
@router.get("/health", tags=["monitoring"])
async def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    checks = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": settings.APP_VERSION,
        "services": {}
    }
    
    # æ£€æŸ¥å„ä¸ªæœåŠ¡
    try:
        # MariaDB
        await mariadb_session.execute("SELECT 1")
        checks["services"]["mariadb"] = "healthy"
    except Exception as e:
        checks["services"]["mariadb"] = f"unhealthy: {str(e)}"
        checks["status"] = "degraded"
    
    try:
        # MongoDB
        await mongodb_client.admin.command("ping")
        checks["services"]["mongodb"] = "healthy"
    except Exception as e:
        checks["services"]["mongodb"] = f"unhealthy: {str(e)}"
        checks["status"] = "degraded"
    
    try:
        # Redis
        await redis_client.ping()
        checks["services"]["redis"] = "healthy"
    except Exception as e:
        checks["services"]["redis"] = f"unhealthy: {str(e)}"
        checks["status"] = "degraded"
    
    try:
        # Vector DB
        collections = await vector_client.get_collections()
        checks["services"]["vector_db"] = f"healthy ({len(collections)} collections)"
    except Exception as e:
        checks["services"]["vector_db"] = f"unhealthy: {str(e)}"
        checks["status"] = "degraded"
    
    # è¿”å›é€‚å½“çš„çŠ¶æ€ç 
    status_code = 200 if checks["status"] == "healthy" else 503
    return JSONResponse(content=checks, status_code=status_code)
```

### 7.3 æ—¥å¿—èšåˆ

```yaml
# ELK Stacké…ç½®
version: '3.8'

services:
  elasticsearch:
    image: elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - es_data:/usr/share/elasticsearch/data

  logstash:
    image: logstash:8.11.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:8.11.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

  filebeat:
    image: elastic/filebeat:8.11.0
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/log:/var/log:ro
      - ./logs:/app/logs:ro
```

---

## 8. éƒ¨ç½²æ–¹æ¡ˆ

### 8.1 å®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libxml2-dev \
    libxslt1-dev \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 8.2 Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: guanshan-api
  namespace: intelligence
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guanshan-api
  template:
    metadata:
      labels:
        app: guanshan-api
    spec:
      containers:
      - name: api
        image: guanshan/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: REDIS_URL
          value: redis://redis-service:6379
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: guanshan-api-service
  namespace: intelligence
spec:
  selector:
    app: guanshan-api
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: guanshan-api-hpa
  namespace: intelligence
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: guanshan-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 8.3 Docker Composeå¼€å‘ç¯å¢ƒ

```yaml
# docker-compose.yml
version: '3.8'

services:
  # APIæœåŠ¡
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=mysql://user:pass@mariadb:3306/guanshan
      - MONGODB_URL=mongodb://mongodb:27017/intelligence
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672
    depends_on:
      - mariadb
      - mongodb
      - redis
      - rabbitmq
      - qdrant
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    command: uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

  # Celery Worker
  celery:
    build: .
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    depends_on:
      - rabbitmq
      - redis
    command: celery -A src.tasks worker --loglevel=info

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build: .
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
    depends_on:
      - rabbitmq
    command: celery -A src.tasks beat --loglevel=info

  # MariaDB
  mariadb:
    image: mariadb:10.11
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: guanshan
      MYSQL_USER: user
      MYSQL_PASSWORD: pass
    volumes:
      - mariadb_data:/var/lib/mysql
    ports:
      - "3306:3306"

  # MongoDB
  mongodb:
    image: mongo:7.0
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    volumes:
      - mongodb_data:/data/db
    ports:
      - "27017:27017"

  # Redis
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

  # RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq

  # Qdrantå‘é‡æ•°æ®åº“
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage

  # MinIOå¯¹è±¡å­˜å‚¨
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  # Flower (Celeryç›‘æ§)
  flower:
    image: mher/flower
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
      - FLOWER_PORT=5555
    ports:
      - "5555:5555"
    depends_on:
      - rabbitmq

volumes:
  mariadb_data:
  mongodb_data:
  redis_data:
  rabbitmq_data:
  qdrant_data:
  minio_data:

networks:
  default:
    name: guanshan_network
```

---

## 9. æµ‹è¯•ç­–ç•¥

### 9.1 å•å…ƒæµ‹è¯•

```python
# tests/test_crawler.py
import pytest
from unittest.mock import Mock, AsyncMock

@pytest.mark.asyncio
async def test_firecrawl_engine():
    """æµ‹è¯•Firecrawlå¼•æ“"""
    engine = FirecrawlEngine()
    mock_response = Mock()
    mock_response.text = "<html><body>Test content</body></html>"
    
    engine.client.get = AsyncMock(return_value=mock_response)
    
    result = await engine.crawl("https://example.com")
    
    assert result.status == "success"
    assert "Test content" in result.content
    assert result.url == "https://example.com"

@pytest.mark.asyncio
async def test_text_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨"""
    processor = TextProcessor()
    
    raw_text = "<p>This is a <b>test</b> document.</p>"
    result = await processor.process_pipeline(raw_text)
    
    assert result.text == "This is a test document."
    assert result.language == "en"
    assert len(result.segments) > 0
```

### 9.2 é›†æˆæµ‹è¯•

```python
# tests/test_integration.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_crawl_to_report_pipeline(test_client: AsyncClient):
    """æµ‹è¯•å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""
    
    # 1. åˆ›å»ºé‡‡é›†ä»»åŠ¡
    crawl_response = await test_client.post(
        "/api/v1/crawl/tasks",
        json={
            "urls": ["https://example.com"],
            "config": {"depth": 1}
        }
    )
    assert crawl_response.status_code == 201
    task_id = crawl_response.json()["task_id"]
    
    # 2. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    await asyncio.sleep(5)
    
    # 3. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
    status_response = await test_client.get(
        f"/api/v1/tasks/{task_id}/status"
    )
    assert status_response.json()["status"] == "completed"
    
    # 4. æ‰§è¡ŒRAGæŸ¥è¯¢
    query_response = await test_client.post(
        "/api/v1/intelligence/query",
        json={
            "query": "What is the main topic?",
            "filters": {"task_id": task_id}
        }
    )
    assert query_response.status_code == 200
    assert len(query_response.json()["results"]) > 0
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report_response = await test_client.post(
        "/api/v1/reports/generate",
        json={
            "source_ids": [task_id],
            "template": "intelligence",
            "format": "pdf"
        }
    )
    assert report_response.status_code == 201
    assert report_response.json()["report_id"] is not None
```

### 9.3 æ€§èƒ½æµ‹è¯•

```python
# tests/test_performance.py
import asyncio
from locust import HttpUser, task, between

class IntelligenceSystemUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(1)
    def crawl_task(self):
        """æµ‹è¯•é‡‡é›†ä»»åŠ¡åˆ›å»º"""
        self.client.post(
            "/api/v1/crawl/tasks",
            json={
                "urls": ["https://example.com"],
                "config": {"depth": 1}
            }
        )
    
    @task(3)
    def query_intelligence(self):
        """æµ‹è¯•RAGæŸ¥è¯¢"""
        self.client.post(
            "/api/v1/intelligence/query",
            json={
                "query": "test query",
                "top_k": 5
            }
        )
    
    @task(2)
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        self.client.get("/health")
```

---

## 10. æˆæœ¬ä¼˜åŒ–

### 10.1 LLM APIæˆæœ¬æ§åˆ¶

```python
class LLMCostOptimizer:
    def __init__(self):
        self.cost_tracker = CostTracker()
        self.cache = LLMCache()
    
    async def optimize_llm_call(
        self,
        prompt: str,
        model: str = "gpt-3.5-turbo"
    ) -> str:
        # 1. ç¼“å­˜æ£€æŸ¥
        cached_result = await self.cache.get(prompt)
        if cached_result:
            return cached_result
        
        # 2. æ¨¡å‹é€‰æ‹©ä¼˜åŒ–
        optimal_model = self.select_optimal_model(prompt)
        
        # 3. Promptå‹ç¼©
        compressed_prompt = self.compress_prompt(prompt)
        
        # 4. æ‰§è¡Œè°ƒç”¨
        result = await self.llm.generate(compressed_prompt, model=optimal_model)
        
        # 5. è®°å½•æˆæœ¬
        cost = self.calculate_cost(len(compressed_prompt), len(result), optimal_model)
        await self.cost_tracker.record(cost, model=optimal_model)
        
        # 6. ç¼“å­˜ç»“æœ
        await self.cache.set(prompt, result, ttl=3600)
        
        return result
    
    def select_optimal_model(self, prompt: str) -> str:
        """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        complexity = self.assess_complexity(prompt)
        
        if complexity < 0.3:
            return "gpt-3.5-turbo"  # ç®€å•ä»»åŠ¡ç”¨ä¾¿å®œæ¨¡å‹
        elif complexity < 0.7:
            return "gpt-4-turbo"    # ä¸­ç­‰ä»»åŠ¡
        else:
            return "gpt-4"          # å¤æ‚ä»»åŠ¡ç”¨æœ€å¼ºæ¨¡å‹
```

### 10.2 å­˜å‚¨ä¼˜åŒ–

```python
# æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
class DataLifecycleManager:
    async def manage_data_lifecycle(self):
        """æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
        
        # 1. å½’æ¡£æ—§æ•°æ®
        old_documents = await self.find_old_documents(days=30)
        await self.archive_to_cold_storage(old_documents)
        
        # 2. å‹ç¼©å¤§æ–‡ä»¶
        large_files = await self.find_large_files(size_mb=10)
        await self.compress_files(large_files)
        
        # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = await self.find_temp_files()
        await self.cleanup_temp_files(temp_files)
        
        # 4. ä¼˜åŒ–å‘é‡ç´¢å¼•
        await self.vector_db.optimize_index()
        
        # 5. æ•°æ®åº“ç»´æŠ¤
        await self.mariadb.analyze_tables()
        await self.mongodb.compact_collections()
```

---

## é™„å½•A: é¡¹ç›®ç»“æ„

```
guanshan-intelligence/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # FastAPIåº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config.py                  # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ api/                       # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ crawl.py
â”‚   â”‚   â”‚   â”œâ”€â”€ intelligence.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reports.py
â”‚   â”‚   â”‚   â””â”€â”€ auth.py
â”‚   â”‚   â””â”€â”€ websocket.py
â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”‚   â”œâ”€â”€ crawler/              # çˆ¬è™«æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ processor/            # æ–‡æœ¬å¤„ç†
â”‚   â”‚   â”œâ”€â”€ translator/           # ç¿»è¯‘æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ rag/                  # RAG pipeline
â”‚   â”‚   â””â”€â”€ generator/            # æŠ¥å‘Šç”Ÿæˆ
â”‚   â”œâ”€â”€ models/                    # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ schemas/                   # Pydantic schemas
â”‚   â”œâ”€â”€ services/                  # ä¸šåŠ¡æœåŠ¡
â”‚   â”œâ”€â”€ tasks/                     # Celeryä»»åŠ¡
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ middleware/                # ä¸­é—´ä»¶
â”œâ”€â”€ tests/                         # æµ‹è¯•
â”œâ”€â”€ migrations/                    # æ•°æ®åº“è¿ç§»
â”œâ”€â”€ scripts/                       # è„šæœ¬å·¥å…·
â”œâ”€â”€ docker/                        # Dockeré…ç½®
â”œâ”€â”€ k8s/                          # Kubernetesé…ç½®
â”œâ”€â”€ docs/                         # æ–‡æ¡£
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ docker-compose.yml            # å¼€å‘ç¯å¢ƒé…ç½®
â”œâ”€â”€ Dockerfile                    # å®¹å™¨æ„å»º
â””â”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
```

## é™„å½•B: æŠ€æœ¯é€‰å‹ç†ç”±

| æŠ€æœ¯ | é€‰æ‹©ç†ç”± |
|------|---------|
| **FastAPI** | å¼‚æ­¥æ”¯æŒå¥½ï¼Œæ€§èƒ½é«˜ï¼Œè‡ªåŠ¨APIæ–‡æ¡£ï¼Œç±»å‹å®‰å…¨ |
| **Firecrawl** | å¼ºå¤§çš„çˆ¬è™«åŠŸèƒ½ï¼Œæ”¯æŒJSæ¸²æŸ“ï¼Œæ˜“äºé›†æˆ |
| **LangChain** | ç»Ÿä¸€çš„LLMæ¥å£ï¼Œä¸°å¯Œçš„å·¥å…·é“¾ï¼ŒRAGæ”¯æŒå®Œå–„ |
| **Qdrant** | ä¸“é—¨çš„å‘é‡æ•°æ®åº“ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œæ”¯æŒè¿‡æ»¤å’Œå…ƒæ•°æ® |
| **MongoDB** | çµæ´»çš„æ–‡æ¡£å­˜å‚¨ï¼Œé€‚åˆéç»“æ„åŒ–æ•°æ® |
| **MariaDB** | æˆç†Ÿç¨³å®šï¼Œäº‹åŠ¡æ”¯æŒï¼Œé€‚åˆç»“æ„åŒ–æ•°æ® |
| **RabbitMQ** | æ¶ˆæ¯å¯é æ€§é«˜ï¼Œæ”¯æŒä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œè¿ç»´æˆç†Ÿ |
| **Celery** | Pythonç”Ÿæ€æœ€æˆç†Ÿçš„ä»»åŠ¡é˜Ÿåˆ—ï¼ŒåŠŸèƒ½ä¸°å¯Œ |
| **Redis** | é«˜æ€§èƒ½ç¼“å­˜ï¼Œæ”¯æŒå¤šç§æ•°æ®ç»“æ„ |
| **MinIO** | S3å…¼å®¹ï¼Œç§æœ‰åŒ–éƒ¨ç½²ï¼Œæˆæœ¬å¯æ§ |

## é™„å½•C: å¸¸ç”¨å‘½ä»¤

```bash
# å¼€å‘ç¯å¢ƒå¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f api

# è¿è¡Œæµ‹è¯•
pytest tests/ -v --cov=src

# æ•°æ®åº“è¿ç§»
alembic upgrade head

# å¯åŠ¨Celery Worker
celery -A src.tasks worker --loglevel=info

# å¯åŠ¨Flowerç›‘æ§
celery -A src.tasks flower

# ç”Ÿäº§éƒ¨ç½²
kubectl apply -f k8s/

# æ€§èƒ½æµ‹è¯•
locust -f tests/test_performance.py --host=http://localhost:8000
```

---

## æ›´æ–°æ—¥å¿—

- 2025-01-10: v1.0.0 - åˆå§‹æ¶æ„è®¾è®¡
- 2025-01-10: v1.0.0 - æ·»åŠ RAG Pipelineè®¾è®¡
- 2025-01-10: v1.0.0 - å®Œå–„å®‰å…¨å’Œç›‘æ§æ–¹æ¡ˆ

---

## è”ç³»æ–¹å¼

- **é¡¹ç›®è´Ÿè´£äºº**: tech@guanshan.ai
- **æŠ€æœ¯æ”¯æŒ**: support@guanshan.ai
- **GitHub**: https://github.com/guanshan/intelligence-system
- **æ–‡æ¡£**: https://docs.guanshan.ai