# Map + Scrape å®Œæ•´ä½¿ç”¨æŒ‡å—

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-06
**æœ€åæ›´æ–°**: 2025-11-14
**é€‚ç”¨ç‰ˆæœ¬**: v2.1.0 - v2.1.2

---

## ğŸ“‹ ç›®å½•

1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [Map API è¯¦è§£](#map-api-è¯¦è§£)
3. [API ä½¿ç”¨ç¤ºä¾‹](#api-ä½¿ç”¨ç¤ºä¾‹)
4. [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
5. [é…ç½®å‚æ•°è¯´æ˜](#é…ç½®å‚æ•°è¯´æ˜)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## åŠŸèƒ½æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ Map + Scrape æ¨¡å¼

Map + Scrape æ˜¯ä¸€ç§åŸºäº Firecrawl Map API + Scrape API çš„æ–°å‹ç½‘ç«™çˆ¬å–æ¨¡å¼ï¼Œå®ƒå°† **URL å‘ç°** å’Œ **å†…å®¹è·å–** åˆ†ç¦»ï¼Œæä¾›æ¯”ä¼ ç»Ÿ Crawl API æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆã€æ›´ä½æˆæœ¬çš„å†…å®¹è·å–èƒ½åŠ›ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯´æ˜ | å¯¹æ¯”ä¼ ç»ŸCrawl |
|------|------|--------------|
| **ç²¾ç¡®æ§åˆ¶** | åªçˆ¬å–çœŸæ­£éœ€è¦çš„é¡µé¢ | èŠ‚çœ 35-92% ç§¯åˆ† |
| **æ—¶é—´è¿‡æ»¤** | æŒ‰å‘å¸ƒæ—¥æœŸè¿‡æ»¤å†…å®¹ | é¿å…çˆ¬å–å†å²å†…å®¹ |
| **URLè¿‡æ»¤** (v2.1.2) | æ™ºèƒ½è¿‡æ»¤æ— ç”¨é“¾æ¥ | å‡å°‘ 40% æ— æ•ˆçˆ¬å– |
| **æˆæœ¬é€æ˜** | Map (1 credit) + Scrape (N credits) | å¯é¢„æµ‹çš„æˆæœ¬ |
| **çµæ´»é…ç½®** | å¹¶å‘æ§åˆ¶ã€å»¶è¿Ÿæ§åˆ¶ã€é”™è¯¯å®¹å¿ | é€‚åº”ä¸åŒç½‘ç«™ |

### å·¥ä½œæµç¨‹

```
1. Map API å‘ç° URL
   â”œâ”€ ä½¿ç”¨sitemap
   â”œâ”€ æ™ºèƒ½çˆ¬å–è¡¥å……
   â””â”€ è¿”å›URLåˆ—è¡¨ (å›ºå®š1 credit)
   â†“
2. URL è¿‡æ»¤ (v2.1.2)
   â”œâ”€ è§„èŒƒåŒ–URL
   â”œâ”€ è¿‡æ»¤è·¯å¾„å…³é”®è¯ (login, admin, etc.)
   â”œâ”€ è¿‡æ»¤æ–‡ä»¶ç±»å‹ (pdf, jpg, etc.)
   â”œâ”€ è¿‡æ»¤å¤–éƒ¨é“¾æ¥
   â””â”€ URLå»é‡
   â†“
3. æ—¶é—´èŒƒå›´è¿‡æ»¤ (v2.1.0)
   â””â”€ æŒ‰ publishedDate å­—æ®µè¿‡æ»¤
   â†“
4. æ‰¹é‡å¹¶å‘ Scrape
   â”œâ”€ å¹¶å‘æ§åˆ¶ (max_concurrent_scrapes)
   â”œâ”€ è¯·æ±‚å»¶è¿Ÿ (scrape_delay)
   â””â”€ è·å–é¡µé¢å†…å®¹ (N credits)
   â†“
5. ä¿å­˜ç»“æœ
   â””â”€ è¿”å›çˆ¬å–ç»“æœ
```

---

## Map API è¯¦è§£

### Map API æ¦‚è¿°

Firecrawl **Map API** æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘ç°ç½‘ç«™æ‰€æœ‰å¯è®¿é—®URLçš„å·¥å…·ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**:

| ç‰¹ç‚¹ | è¯´æ˜ |
|------|------|
| **é€Ÿåº¦** | é€šå¸¸<5ç§’å®Œæˆæ•´ä¸ªç½‘ç«™çš„URLå‘ç° |
| **å‡†ç¡®æ€§** | ç»“åˆsitemapå’Œæ™ºèƒ½çˆ¬å–ï¼Œå‘ç°ç‡>95% |
| **æˆæœ¬** | å›ºå®š1 creditï¼Œæ— è®ºç½‘ç«™å¤§å° |
| **è¾“å‡º** | URLåˆ—è¡¨ + åŸºæœ¬å…ƒæ•°æ®ï¼ˆtitle, descriptionï¼‰ |
| **é™åˆ¶** | é»˜è®¤è¿”å›5000ä¸ªURL |

### ä¸ Crawl API å¯¹æ¯”

| ç»´åº¦ | Map API | Crawl API |
|------|---------|-----------|
| **ç›®çš„** | å‘ç°URL | çˆ¬å–å†…å®¹ |
| **é€Ÿåº¦** | æå¿«ï¼ˆ<5ç§’ï¼‰ | è¾ƒæ…¢ï¼ˆåˆ†é’Ÿçº§ï¼‰ |
| **è¾“å‡º** | URLåˆ—è¡¨ + å…ƒæ•°æ® | å®Œæ•´é¡µé¢å†…å®¹ |
| **å†…å®¹** | âŒ æ— é¡µé¢å†…å®¹ | âœ… Markdown + HTML |
| **æ—¶é—´ä¿¡æ¯** | âŒ æ— å‘å¸ƒæ—¶é—´ | âœ… å®Œæ•´metadata |
| **ç§¯åˆ†** | 1 credit | N creditsï¼ˆN=é¡µé¢æ•°ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | URLå‘ç° | å†…å®¹è·å– |

### ä½¿ç”¨å†³ç­–æ ‘

```
éœ€è¦è·å–é¡µé¢å†…å®¹ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨Crawl APIæˆ–Map+Scrape
â””â”€ å¦ â†’ ä½¿ç”¨Map API

éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–å“ªäº›é¡µé¢ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨Map API + Scrape API
â””â”€ å¦ â†’ ä½¿ç”¨Crawl API

éœ€è¦èŠ‚çœç§¯åˆ†ï¼Ÿ
â”œâ”€ æ˜¯ï¼Œåªéœ€è¦éƒ¨åˆ†é¡µé¢ â†’ Map API + Scrape
â””â”€ å¦ï¼Œéœ€è¦å…¨éƒ¨å†…å®¹ â†’ Crawl API

ç½‘ç«™æœ‰å¤§é‡æ— ç”¨é“¾æ¥ï¼Ÿ (v2.1.2)
â”œâ”€ æ˜¯ â†’ Map API + URLè¿‡æ»¤ + Scrape
â””â”€ å¦ â†’ Crawl APIæˆ–ç®€å•Map+Scrape
```

### Map API å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `url` | string | âœ… | - | ç½‘ç«™èµ·å§‹URL |
| `search` | string | âŒ | null | æœç´¢å…³é”®è¯ï¼ˆURLè¿‡æ»¤ï¼‰ |
| `limit` | integer | âŒ | 5000 | è¿”å›URLæ•°é‡é™åˆ¶ |

### Map API å“åº”æ ¼å¼

```json
{
  "success": true,
  "links": [
    {
      "url": "https://example.com/blog/post-1",
      "title": "First Blog Post",
      "description": "This is my first blog post about..."
    },
    {
      "url": "https://example.com/blog/post-2",
      "title": "Second Blog Post",
      "description": "In this post, I discuss..."
    }
  ]
}
```

---

## API ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»º Map+Scrape ä»»åŠ¡

#### åŸºç¡€é…ç½®

```bash
curl -X POST http://localhost:8000/api/v1/search-tasks \
  -H "Content-Type: application/json" \
  -d '{
    "name": "è¥¿è—é‚®æŠ¥æ–°é—»çˆ¬å–",
    "description": "ä½¿ç”¨ Map+Scrape æ¨¡å¼çˆ¬å–æ–°é—»å†…å®¹",
    "crawl_url": "https://www.thetibetpost.com/",
    "task_type": "map_scrape_website",
    "crawl_config": {
      "map_limit": 5000,
      "max_concurrent_scrapes": 5,
      "scrape_delay": 0.5,
      "only_main_content": false,
      "exclude_tags": [],
      "enable_dedup": true
    },
    "schedule_interval": "HOURLY_1",
    "is_active": true,
    "execute_immediately": true
  }'
```

#### å¸¦æ—¶é—´è¿‡æ»¤çš„é…ç½®

```bash
curl -X POST http://localhost:8000/api/v1/search-tasks \
  -H "Content-Type: application/json" \
  -d '{
    "name": "æœ€è¿‘30å¤©æ–°é—»çˆ¬å–",
    "crawl_url": "https://example.com",
    "task_type": "map_scrape_website",
    "crawl_config": {
      "map_limit": 5000,
      "start_date": "2025-10-15T00:00:00",
      "end_date": "2025-11-14T23:59:59",
      "max_concurrent_scrapes": 5,
      "scrape_delay": 0.5,
      "only_main_content": false,
      "exclude_tags": [],
      "enable_dedup": true
    }
  }'
```

### å“åº”ç¤ºä¾‹

```json
{
  "id": "244879584026255360",
  "name": "è¥¿è—é‚®æŠ¥æ–°é—»çˆ¬å–",
  "task_type": "map_scrape_website",
  "task_mode": "Map + Scrape ç»„åˆæ¨¡å¼",
  "crawl_url": "https://www.thetibetpost.com/",
  "crawl_config": {
    "only_main_content": false,
    "exclude_tags": [],
    "enable_dedup": true
  },
  "is_active": true,
  "status": "active",
  "execution_count": 0,
  "total_results": 0
}
```

### Python SDK ç¤ºä¾‹

```python
from firecrawl import FirecrawlApp
from datetime import datetime, timedelta

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# è®¾ç½®æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘30å¤©
end_date = datetime.utcnow()
start_date = end_date - timedelta(days=30)

# åˆ›å»ºMap+Scrapeä»»åŠ¡
task = {
    "name": "è¿‘æœŸæ–°é—»çˆ¬å–",
    "task_type": "map_scrape_website",
    "crawl_url": "https://example.com",
    "crawl_config": {
        # Map API é…ç½®
        "search": "news",
        "map_limit": 100,

        # æ—¶é—´è¿‡æ»¤
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),

        # Scrape API é…ç½®
        "max_concurrent_scrapes": 5,
        "scrape_delay": 0.5,
        "only_main_content": False,
        "wait_for": 3000,
        "timeout": 90,

        # é”™è¯¯å¤„ç†
        "allow_partial_failure": True,
        "min_success_rate": 0.8,

        # v2.1.1: å»é‡é…ç½®
        "enable_dedup": True
    }
}

# æ‰§è¡Œä»»åŠ¡
result = await execute_task(task)
print(f"çˆ¬å–æˆåŠŸ: {result['total_count']} ä¸ªé¡µé¢")
print(f"ç§¯åˆ†æ¶ˆè€—: {result['credits_used']}")
```

### ä»»åŠ¡æ‰§è¡Œæ—¥å¿—ç¤ºä¾‹

```
2025-11-07 01:44:02 - MapScrapeExecutor - INFO - ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: è¥¿è—é‚®æŠ¥ Map+Scrape
2025-11-07 01:44:02 - MapScrapeExecutor - INFO - ğŸ—ºï¸  Step 1: ä½¿ç”¨ Map API å‘ç° URL
2025-11-07 01:44:10 - MapScrapeExecutor - INFO - âœ… å‘ç° 195 ä¸ªURL

2025-11-07 01:44:10 - MapScrapeExecutor - INFO - ğŸ” å¼€å§‹URLè¿‡æ»¤ (v2.1.2)
2025-11-07 01:44:11 - FilterChain - INFO - âœ… URLè¿‡æ»¤å®Œæˆ: 195 â†’ 100 (è¿‡æ»¤ 95, 48.7%)

2025-11-07 01:44:11 - MapScrapeExecutor - INFO - ğŸ” æ£€æŸ¥å·²çˆ¬å–URLå»é‡ (v2.1.1)
2025-11-07 01:44:11 - MapScrapeExecutor - INFO - âœ… URLå»é‡: å‘ç°100ä¸ª, å·²å­˜åœ¨10ä¸ª, å¾…çˆ¬å–90ä¸ª

2025-11-07 01:44:30 - MapScrapeExecutor - INFO - ğŸ”¥ Step 2: æ‰¹é‡ Scrape è·å–å†…å®¹ï¼ˆ90ä¸ªURLï¼Œå¹¶å‘5ï¼‰
2025-11-07 01:45:30 - MapScrapeExecutor - INFO - âœ… Scrape å®Œæˆ: æˆåŠŸ88ä¸ª, å¤±è´¥2ä¸ª

2025-11-07 01:45:30 - SearchResultRepository - INFO - ä¿å­˜æœç´¢ç»“æœæˆåŠŸ: æ–°å¢88æ¡, è·³è¿‡é‡å¤0æ¡
2025-11-07 01:45:30 - MapScrapeExecutor - INFO - âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ | ç»“æœæ•°: 88 | è€—æ—¶: 88000ms
```

---

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: ç²¾ç¡®çˆ¬å–ç‰¹å®šå†…å®¹

**éœ€æ±‚**: åªçˆ¬å–åšå®¢æ–‡ç« ï¼Œä¸éœ€è¦å…¶ä»–é¡µé¢

**æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨Map APIå‘ç°æ‰€æœ‰URLï¼Œæœç´¢"blog"
map_result = app.map_url("https://example.com", params={"search": "blog"})

# 2. æ‰¹é‡scrapeè¿™äº›URL
for link in map_result['links']:
    content = app.scrape_url(link['url'])
    # ä¿å­˜å†…å®¹
```

**ä¼˜åŠ¿**:
- åªçˆ¬å–åšå®¢é¡µé¢ï¼ŒèŠ‚çœç§¯åˆ†
- å¿«é€Ÿå‘ç°æ‰€æœ‰åšå®¢URL
- é¿å…çˆ¬å–ä¸ç›¸å…³é¡µé¢

### åœºæ™¯2: æ—¶é—´èŒƒå›´çˆ¬å–

**éœ€æ±‚**: åªè·å–æœ€è¿‘30å¤©çš„æ–‡ç« 

**æ–¹æ¡ˆ**:
```python
from datetime import datetime, timedelta

# 1. Mapå‘ç°æ‰€æœ‰URL
map_result = app.map_url("https://example.com/blog")

# 2. Scrapeå¹¶è¿‡æ»¤
cutoff_date = datetime.now() - timedelta(days=30)
recent_articles = []

for link in map_result['links']:
    content = app.scrape_url(link['url'])

    # æ£€æŸ¥å‘å¸ƒæ—¶é—´
    pub_date_str = content['metadata'].get('publishedDate')
    if pub_date_str:
        pub_date = datetime.fromisoformat(pub_date_str)
        if pub_date >= cutoff_date:
            recent_articles.append(content)

print(f"å‘ç° {len(recent_articles)} ç¯‡æœ€è¿‘30å¤©çš„æ–‡ç« ")
```

**ç§¯åˆ†å¯¹æ¯”**:
- Crawlå…¨ç«™: 1000 credits
- Map+Scrape: 1 + 50 = 51 creditsï¼ˆå‡è®¾50ç¯‡ç¬¦åˆæ¡ä»¶ï¼‰
- **èŠ‚çœ**: 95%

### åœºæ™¯3: å¢é‡çˆ¬å–

**éœ€æ±‚**: å®šæœŸçˆ¬å–ï¼Œåªè·å–æ–°å¢é¡µé¢

**æ–¹æ¡ˆ**:
```python
# é¦–æ¬¡çˆ¬å–
initial_urls = set(link['url'] for link in app.map_url("https://example.com")['links'])
save_to_db(initial_urls)

# åç»­çˆ¬å–ï¼ˆ7å¤©åï¼‰
current_urls = set(link['url'] for link in app.map_url("https://example.com")['links'])
new_urls = current_urls - initial_urls

print(f"å‘ç° {len(new_urls)} ä¸ªæ–°é¡µé¢")

# åªscrapeæ–°é¡µé¢
for url in new_urls:
    content = app.scrape_url(url)
    save_to_db(content)
```

### åœºæ™¯4: ç½‘ç«™ç»“æ„åˆ†æ

**éœ€æ±‚**: åˆ†æç½‘ç«™çš„URLç»“æ„

**æ–¹æ¡ˆ**:
```python
from urllib.parse import urlparse
from collections import Counter

# è·å–æ‰€æœ‰URL
map_result = app.map_url("https://example.com")

# åˆ†æURLè·¯å¾„
paths = [urlparse(link['url']).path for link in map_result['links']]
path_segments = [p.split('/')[1] for p in paths if len(p.split('/')) > 1]

# ç»Ÿè®¡
counter = Counter(path_segments)
print("URLç»“æ„åˆ†æ:")
for segment, count in counter.most_common(10):
    print(f"  /{segment}/: {count} ä¸ªé¡µé¢")
```

**è¾“å‡ºç¤ºä¾‹**:
```
URLç»“æ„åˆ†æ:
  /blog/: 150 ä¸ªé¡µé¢
  /docs/: 80 ä¸ªé¡µé¢
  /products/: 30 ä¸ªé¡µé¢
  /about/: 5 ä¸ªé¡µé¢
```

---

## é…ç½®å‚æ•°è¯´æ˜

### Map API é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `search` | string | null | URL/æ ‡é¢˜è¿‡æ»¤å…³é”®è¯ |
| `map_limit` | integer | 5000 | è¿”å›URLæ•°é‡é™åˆ¶ (æœ€å¤§5000) |

### æ—¶é—´è¿‡æ»¤é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `start_date` | datetime | null | å¼€å§‹æ—¥æœŸ (ISO 8601æ ¼å¼) |
| `end_date` | datetime | null | ç»“æŸæ—¥æœŸ (ISO 8601æ ¼å¼) |

### Scrape API é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `max_concurrent_scrapes` | integer | 5 | æœ€å¤§å¹¶å‘Scrapeæ•°é‡ (1-10) |
| `scrape_delay` | float | 0.5 | Scrapeè¯·æ±‚é—´éš”(ç§’) |
| `only_main_content` | boolean | false | åªè·å–ä¸»è¦å†…å®¹ (v2.1.1: é»˜è®¤false) |
| `exclude_tags` | array | [] | æ’é™¤çš„HTMLæ ‡ç­¾ (v2.1.1: é»˜è®¤ç©º) |
| `wait_for` | integer | 500 | é¡µé¢ç­‰å¾…æ—¶é—´(æ¯«ç§’) (v2.1.1: 500ms) |
| `timeout` | integer | 90 | å•ä¸ªScrapeè¶…æ—¶(ç§’) |

### é”™è¯¯å¤„ç†é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `allow_partial_failure` | boolean | true | å…è®¸éƒ¨åˆ†Scrapeå¤±è´¥ |
| `min_success_rate` | float | 0.8 | æœ€ä½æˆåŠŸç‡è¦æ±‚ (0.0-1.0) |

### å»é‡é…ç½® (v2.1.1)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `enable_dedup` | boolean | true | å¯ç”¨URLå»é‡ |

### ä¸å…¶ä»–ä»»åŠ¡ç±»å‹çš„å¯¹æ¯”

| ä»»åŠ¡ç±»å‹ | task_type | ä½¿ç”¨çš„é…ç½®å­—æ®µ |
|---------|-----------|--------------|
| å…³é”®è¯æœç´¢ | `search_keyword` | `search_config` |
| ç½‘ç«™çˆ¬å– | `crawl_website` | `crawl_config` |
| å•é¡µé¢çˆ¬å– | `scrape_url` | `search_config` |
| **Map+Scrape** | `map_scrape_website` | `crawl_config` âœ… |

---

## æœ€ä½³å®è·µ

### 1. åˆç†ä½¿ç”¨ search å‚æ•°

**æ¨è**:
```python
# æ˜ç¡®çš„è¿‡æ»¤æ¡ä»¶
map_result = app.map_url("https://example.com", params={"search": "blog"})
```

**ä¸æ¨è**:
```python
# è¿‡äºå®½æ³›çš„æœç´¢
map_result = app.map_url("https://example.com", params={"search": "a"})
```

### 2. è®¾ç½®åˆç†çš„ limit

**åœºæ™¯åˆ¤æ–­**:
```python
# å°å‹ç½‘ç«™ï¼ˆ<1000é¡µï¼‰
params = {"limit": 1000}

# ä¸­å‹ç½‘ç«™ï¼ˆ<5000é¡µï¼‰
params = {"limit": 5000}  # é»˜è®¤å€¼

# å¤§å‹ç½‘ç«™ï¼ˆ>5000é¡µï¼‰
# åˆ†æ‰¹mapä¸åŒsection
params = {"limit": 5000, "search": "blog"}
params = {"limit": 5000, "search": "docs"}
```

### 3. ç¼“å­˜ Map ç»“æœ

```python
import json
from pathlib import Path
import time

def get_urls_with_cache(url: str, cache_file: str = "map_cache.json"):
    """ä½¿ç”¨ç¼“å­˜çš„Mapç»“æœ"""
    cache_path = Path(cache_file)

    # æ£€æŸ¥ç¼“å­˜
    if cache_path.exists():
        with open(cache_path) as f:
            cached = json.load(f)
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆä¾‹å¦‚ï¼š24å°æ—¶ï¼‰
            if time.time() - cached['timestamp'] < 86400:
                return cached['links']

    # é‡æ–°Map
    result = app.map_url(url)

    # ä¿å­˜ç¼“å­˜
    with open(cache_path, 'w') as f:
        json.dump({
            'timestamp': time.time(),
            'links': result['links']
        }, f)

    return result['links']
```

### 4. é”™è¯¯å¤„ç†

```python
from firecrawl import MapAPIError

try:
    result = app.map_url("https://example.com")
except MapAPIError as e:
    logger.error(f"Map APIå¤±è´¥: {e}")
    # Fallback: ä½¿ç”¨Crawl API
    result = app.crawl_url("https://example.com")
except Exception as e:
    logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
    raise
```

### 5. å¹¶å‘æ§åˆ¶ä¼˜åŒ–

**æ ¹æ®ç½‘ç«™å“åº”é€Ÿåº¦è°ƒæ•´**:
```python
# å¿«é€Ÿå“åº”çš„ç½‘ç«™
config = {
    "max_concurrent_scrapes": 10,
    "scrape_delay": 0.2
}

# ä¸€èˆ¬ç½‘ç«™
config = {
    "max_concurrent_scrapes": 5,
    "scrape_delay": 0.5
}

# æ…¢é€Ÿæˆ–æœ‰é™æµä¿æŠ¤çš„ç½‘ç«™
config = {
    "max_concurrent_scrapes": 2,
    "scrape_delay": 1.0
}
```

### 6. å®Œæ•´HTMLè·å– (v2.1.1)

**æ¨èé…ç½®**:
```json
{
  "only_main_content": false,
  "exclude_tags": []
}
```

è¿™æ ·å¯ä»¥è·å–å®Œæ•´çš„HTMLå†…å®¹ï¼Œä¸ºAIå¤„ç†æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚

---

## å¸¸è§é—®é¢˜

### Q1: Map APIä¸ºä»€ä¹ˆä¸è¿”å›é¡µé¢å†…å®¹ï¼Ÿ

**A**: Map APIçš„è®¾è®¡ç›®æ ‡æ˜¯**å¿«é€Ÿå‘ç°URL**ï¼Œè€Œä¸æ˜¯è·å–å†…å®¹ã€‚è¿™æ ·å¯ä»¥ï¼š
- æå¿«çš„å“åº”é€Ÿåº¦ï¼ˆ<5ç§’ï¼‰
- å›ºå®šçš„ä½æˆæœ¬ï¼ˆ1 creditï¼‰
- è®©ç”¨æˆ·ç²¾ç¡®æ§åˆ¶åç»­çˆ¬å–å“ªäº›é¡µé¢

å¦‚æœéœ€è¦å†…å®¹ï¼Œä½¿ç”¨**Map + Scrape**æˆ–**Crawl API**ã€‚

### Q2: Map APIèƒ½å‘ç°æ‰€æœ‰é¡µé¢å—ï¼Ÿ

**A**: Map APIç»“åˆäº†sitemapå’Œæ™ºèƒ½çˆ¬å–ï¼Œå‘ç°ç‡é€šå¸¸>95%ï¼Œä½†ä»¥ä¸‹æƒ…å†µå¯èƒ½é—æ¼ï¼š
- JavaScriptåŠ¨æ€ç”Ÿæˆçš„é“¾æ¥
- éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®çš„é¡µé¢
- éšè—åœ¨å¤æ‚äº¤äº’åçš„é“¾æ¥

å¯¹äºå®Œæ•´æ€§è¦æ±‚æé«˜çš„åœºæ™¯ï¼Œå»ºè®®ä½¿ç”¨**Crawl API**ã€‚

### Q3: search å‚æ•°å¦‚ä½•å·¥ä½œï¼Ÿ

**A**: `search`å‚æ•°ä¼šè¿‡æ»¤URLå’Œæ ‡é¢˜ä¸­åŒ…å«å…³é”®è¯çš„é¡µé¢ï¼š

```python
# åªè¿”å›URLæˆ–æ ‡é¢˜åŒ…å«"blog"çš„é¡µé¢
result = app.map_url("https://example.com", params={"search": "blog"})

# ç¤ºä¾‹ç»“æœ:
# âœ… https://example.com/blog/post-1
# âœ… https://example.com/about (æ ‡é¢˜åŒ…å«"blog")
# âŒ https://example.com/products
```

### Q4: limit å‚æ•°å¦‚ä½•è®¾ç½®ï¼Ÿ

**A**: æ ¹æ®ç½‘ç«™è§„æ¨¡è®¾ç½®ï¼š

| ç½‘ç«™è§„æ¨¡ | æ¨èlimit | è¯´æ˜ |
|----------|-----------|------|
| å°å‹ | 1000 | ä¸ªäººåšå®¢ã€å°ç½‘ç«™ |
| ä¸­å‹ | 5000 | ä¼ä¸šç½‘ç«™ã€ä¸­å‹åª’ä½“ |
| å¤§å‹ | åˆ†æ‰¹map | åˆ†sectionå¤šæ¬¡è°ƒç”¨ |

### Q5: Map APIçš„ç§¯åˆ†æˆæœ¬å¦‚ä½•è®¡ç®—ï¼Ÿ

**A**: éå¸¸ç®€å•ï¼š
```
æ¯æ¬¡Mapè°ƒç”¨ = 1 credit
```

æ— è®ºç½‘ç«™å¤§å°ï¼Œæ— è®ºè¿”å›å¤šå°‘URLï¼Œéƒ½æ˜¯å›ºå®š1 creditã€‚

### Q6: ä»€ä¹ˆæ—¶å€™ç”¨Map+Scrapeï¼Œä»€ä¹ˆæ—¶å€™ç”¨Crawlï¼Ÿ

**å†³ç­–è¡¨**:

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | åŸå›  |
|------|---------|------|
| åªéœ€è¦éƒ¨åˆ†é¡µé¢ | Map+Scrape | èŠ‚çœç§¯åˆ† |
| éœ€è¦æ—¶é—´è¿‡æ»¤ | Map+Scrape | ç²¾ç¡®æ§åˆ¶ |
| ç½‘ç«™æœ‰å¤§é‡æ— ç”¨é“¾æ¥ (v2.1.2) | Map+Scrape | æ™ºèƒ½è¿‡æ»¤ |
| å®Œæ•´ç½‘ç«™å½’æ¡£ | Crawl | ç®€å•ç›´æ¥ |
| ä¸ç¡®å®šéœ€è¦å“ªäº›é¡µé¢ | Crawl | å…¨é¢è¦†ç›– |
| ç½‘ç«™ç»“æ„è§„åˆ™ | Map+Scrape | é«˜æ•ˆå‡†ç¡® |
| ç½‘ç«™ç»“æ„å¤æ‚ | Crawl | æ›´å…¨é¢ |

### Q7: URLè¿‡æ»¤ç³»ç»Ÿ (v2.1.2) ä¼šè¿‡æ»¤æ‰å“ªäº›é“¾æ¥ï¼Ÿ

**A**: è¿‡æ»¤ç³»ç»Ÿä¼šè‡ªåŠ¨è¿‡æ»¤ä»¥ä¸‹ç±»å‹çš„æ— ç”¨é“¾æ¥ï¼š
- **ç”¨æˆ·æ“ä½œé¡µé¢**: login, signup, cart, checkout ç­‰
- **ç³»ç»ŸåŠŸèƒ½é¡µé¢**: admin, api, dashboard, search ç­‰
- **æ–‡ä»¶ä¸‹è½½**: PDF, å›¾ç‰‡, è§†é¢‘, å‹ç¼©åŒ…ç­‰
- **å¤–éƒ¨é“¾æ¥**: ä¸åœ¨åŒä¸€åŸŸåä¸‹çš„é“¾æ¥
- **é‡å¤URL**: å®Œå…¨ç›¸åŒæˆ–è§„èŒƒåŒ–åç›¸åŒçš„URL

**é¢„è®¡è¿‡æ»¤ç‡**: 35-65% (ä¿å®ˆä¼°è®¡ 40%)

### Q8: å¦‚ä½•éªŒè¯å®Œæ•´HTMLè·å–ï¼Ÿ

**A**: åˆ›å»ºä»»åŠ¡åï¼Œæ£€æŸ¥ç»“æœçš„HTMLå†…å®¹ï¼š

```bash
# è·å–ä»»åŠ¡ç»“æœ
curl http://localhost:8000/api/v1/search-tasks/{task_id}/results?page=1&page_size=1

# æ£€æŸ¥å“åº”ä¸­çš„ html_content å­—æ®µ
# åº”è¯¥åŒ…å«å®Œæ•´çš„ HTMLï¼ŒåŒ…æ‹¬ <nav>, <footer>, <header> ç­‰æ ‡ç­¾
```

**éªŒè¯ç‚¹**:
1. âœ… `html_content` å­—æ®µé•¿åº¦åº”è¯¥æ¯”è¿‡æ»¤ç‰ˆæœ¬æ›´é•¿
2. âœ… åŒ…å« `<nav>`, `<footer>`, `<header>` ç­‰æ ‡ç­¾
3. âœ… åŒ…å«å®Œæ•´çš„é¡µé¢ç»“æ„
4. âœ… `content_hash` å­—æ®µå·²ç”Ÿæˆï¼ˆv2.1.1 å»é‡åŠŸèƒ½ï¼‰

### Q9: ä»»åŠ¡åˆ›å»ºå¤±è´¥ï¼Œæç¤º 422 é”™è¯¯ï¼Ÿ

**A**: v2.1.1ä¹‹å‰ç‰ˆæœ¬å­˜åœ¨APIéªŒè¯bugï¼Œå·²ä¿®å¤ã€‚ç¡®ä¿ï¼š
- ä½¿ç”¨ `task_type: "map_scrape_website"`
- ä½¿ç”¨ `crawl_config` è€Œä¸æ˜¯ `search_config`
- æä¾›å¿…éœ€çš„ `crawl_url` å­—æ®µ

### Q10: Scrape å…¨éƒ¨å¤±è´¥ï¼Œæç¤º waitFor é”™è¯¯ï¼Ÿ

**A**: v2.1.1 å·²ä¿®å¤ timeout å‚æ•°é—®é¢˜ã€‚ç¡®ä¿ï¼š
- `wait_for` ä½¿ç”¨é»˜è®¤å€¼ 500ms
- `timeout` ä½¿ç”¨é»˜è®¤å€¼ 90ç§’
- ç³»ç»Ÿä¼šè‡ªåŠ¨è½¬æ¢å•ä½ï¼ˆç§’â†’æ¯«ç§’ï¼‰

---

## æ€»ç»“

### Map + Scrape çš„æ ¸å¿ƒä»·å€¼

1. **å¿«é€Ÿå‘ç°**: å‡ ç§’å†…è·å–æ‰€æœ‰URL
2. **æˆæœ¬å›ºå®š**: 1 creditæ— è®ºç½‘ç«™å¤§å°
3. **ç²¾ç¡®æ§åˆ¶**: ä¸Scrapeç»„åˆå®ç°ç²¾ç¡®çˆ¬å–
4. **èŠ‚çœç§¯åˆ†**: é¿å…ä¸å¿…è¦çš„é¡µé¢çˆ¬å–
5. **æ™ºèƒ½è¿‡æ»¤** (v2.1.2): è‡ªåŠ¨è¿‡æ»¤æ— ç”¨é“¾æ¥

### æœ€ä½³ä½¿ç”¨æ¨¡å¼

```
Map API (å‘ç°) â†’ URLè¿‡æ»¤ (v2.1.2) â†’ æ—¶é—´/å†…å®¹è¿‡æ»¤ â†’ Scrape API (è·å–)
```

è¿™ç§æ¨¡å¼åœ¨ä»¥ä¸‹åœºæ™¯æœ€æœ‰ä»·å€¼ï¼š
- å®šæœŸç›‘æ§ç½‘ç«™æ›´æ–°
- åªéœ€è¦ç‰¹å®šæ—¶é—´èŒƒå›´çš„å†…å®¹
- ç½‘ç«™åŒ…å«å¤§é‡æ— ç”¨é“¾æ¥
- éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡
- å…³æ³¨APIç§¯åˆ†æˆæœ¬

### æˆæœ¬ä¼˜åŒ–æ•ˆæœ

| ä¼˜åŒ–é˜¶æ®µ | ç§¯åˆ†èŠ‚çœ | ç´¯è®¡èŠ‚çœ |
|---------|---------|---------|
| v2.1.0 åŸºç¡€ | æ—¶é—´è¿‡æ»¤: æœ€é«˜84% | 84% |
| v2.1.2 URLè¿‡æ»¤ | å†èŠ‚çœ40% | æœ€é«˜92% |

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Development Team
**æœ€åæ›´æ–°**: 2025-11-14
**ç‰ˆæœ¬**: v2.0.0
