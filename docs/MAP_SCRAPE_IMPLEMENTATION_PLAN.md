# Map+Scrape æ‰§è¡Œå™¨å®ç°è®¡åˆ’

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-06
**çŠ¶æ€**: å¾…å®æ–½

---

## ğŸ“‹ å®æ–½æ¦‚è§ˆ

### æ ¸å¿ƒç›®æ ‡

âœ… ä½¿ç”¨Firecrawlçš„**Map API + Scrape API**å®ç°æŒ‡å®šURLåŠ æ—¶é—´èŒƒå›´çš„ç²¾ç¡®çˆ¬å–
âœ… **æ›¿æ¢**/crawlæ¥å£çš„ä½¿ç”¨åœºæ™¯ï¼ˆä½œä¸ºæ–°é€‰é¡¹ï¼‰
âœ… **ä¿ç•™**/crawlæ¨¡å—åŠŸèƒ½ä½œä¸ºå¤‡ç”¨
âœ… **æ•°æ®åº“å­—æ®µç»“æ„å®Œå…¨ä¸å˜**

### é¢„æœŸæ”¶ç›Š

- **ç§¯åˆ†èŠ‚çœ**: 80-90%ï¼ˆåªçˆ¬å–éœ€è¦çš„é¡µé¢ï¼‰
- **ç²¾ç¡®æ§åˆ¶**: URLçº§åˆ«çš„çˆ¬å–æ§åˆ¶
- **æ—¶é—´è¿‡æ»¤**: æ”¯æŒæŒ‰å‘å¸ƒæ—¶é—´èŒƒå›´è¿‡æ»¤
- **æ€§èƒ½æå‡**: å¹¶å‘scrape + Map APIå¿«é€Ÿå‘ç°

---

## ğŸ—‚ï¸ æ–‡ä»¶å˜æ›´æ¸…å•

### æ–°å¢æ–‡ä»¶

```
src/services/firecrawl/executors/map_scrape_executor.py    [æ–°å»º]
src/services/firecrawl/config/map_scrape_config.py         [æ–°å»º]
tests/test_map_scrape_executor.py                          [æ–°å»º]
docs/MAP_SCRAPE_EXECUTOR_DESIGN.md                         [å·²åˆ›å»º]
docs/FIRECRAWL_MAP_API_GUIDE.md                            [å·²åˆ›å»º]
```

### ä¿®æ”¹æ–‡ä»¶

```
src/infrastructure/crawlers/firecrawl_adapter.py           [æ‰©å±•]
  + async def map(url, search, limit) -> List[Dict]

src/core/domain/entities/search_task.py                    [æ‰©å±•]
  + TaskType.MAP_SCRAPE_WEBSITE = "map_scrape_website"

src/services/firecrawl/factory.py                          [æ‰©å±•]
  + æ³¨å†ŒMapScrapeExecutor

src/services/firecrawl/config/__init__.py                  [æ‰©å±•]
  + from .map_scrape_config import MapScrapeConfig

src/services/firecrawl/credits_calculator.py               [æ‰©å±•]
  + calculate_map_scrape_credits()

docs/FIRECRAWL_ARCHITECTURE_V2.md                          [æ›´æ–°]
  + æ·»åŠ MapScrapeExecutorè¯´æ˜
```

---

## ğŸ”§ è¯¦ç»†å®æ–½æ­¥éª¤

### Phase 1: åŸºç¡€è®¾æ–½ï¼ˆç¬¬1å¤©ï¼‰

#### 1.1 æ‰©å±•FirecrawlAdapter

**æ–‡ä»¶**: `src/infrastructure/crawlers/firecrawl_adapter.py`

**æ–°å¢æ–¹æ³•**ï¼š

```python
async def map(
    self,
    url: str,
    search: Optional[str] = None,
    limit: int = 5000
) -> List[Dict[str, Any]]:
    """è°ƒç”¨Firecrawl Map API

    Args:
        url: èµ·å§‹URL
        search: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›URLæ•°é‡é™åˆ¶

    Returns:
        List[Dict]: [
            {"url": "...", "title": "...", "description": "..."},
            ...
        ]

    Raises:
        MapAPIError: Map APIè°ƒç”¨å¤±è´¥
    """
    payload = {"url": url, "limit": limit}

    if search:
        payload["search"] = search

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {self.api_key}"
    }

    try:
        response = await self.client.post(
            f"{self.base_url}/v2/map",
            json=payload,
            headers=headers,
            timeout=30
        )
        response.raise_for_status()

        data = response.json()
        if not data.get("success"):
            raise MapAPIError(f"Map APIè¿”å›å¤±è´¥: {data}")

        links = data.get("links", [])
        self.logger.info(f"âœ… Map APIè¿”å› {len(links)} ä¸ªURL")

        return links

    except Exception as e:
        self.logger.error(f"âŒ Map APIè°ƒç”¨å¤±è´¥: {e}")
        raise MapAPIError(f"Map APIè°ƒç”¨å¤±è´¥: {str(e)}")
```

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
python scripts/test_map_api.py
```

#### 1.2 åˆ›å»ºé…ç½®ç±»

**æ–‡ä»¶**: `src/services/firecrawl/config/map_scrape_config.py`

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, List

@dataclass
class MapScrapeConfig:
    """Map + Scrape æ‰§è¡Œå™¨é…ç½®

    ç¤ºä¾‹:
        config = MapScrapeConfig(
            search="blog",
            start_date=datetime(2025, 1, 1),
            end_date=datetime(2025, 12, 31),
            max_concurrent_scrapes=5
        )
    """

    # Map API é…ç½®
    search: Optional[str] = None          # æœç´¢å…³é”®è¯
    map_limit: int = 5000                 # Mapè¿”å›URLé™åˆ¶

    # æ—¶é—´è¿‡æ»¤
    start_date: Optional[datetime] = None # å¼€å§‹æ—¥æœŸ
    end_date: Optional[datetime] = None   # ç»“æŸæ—¥æœŸ

    # Scrape API é…ç½®
    max_concurrent_scrapes: int = 5       # æœ€å¤§å¹¶å‘æ•°
    scrape_delay: float = 0.5             # scrapeé—´éš”ï¼ˆç§’ï¼‰
    only_main_content: bool = True        # åªæå–ä¸»è¦å†…å®¹
    exclude_tags: List[str] = field(
        default_factory=lambda: ["nav", "footer", "header", "aside"]
    )
    timeout: int = 90                     # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    # é”™è¯¯å¤„ç†
    allow_partial_failure: bool = True    # å…è®¸éƒ¨åˆ†å¤±è´¥
    min_success_rate: float = 0.8         # æœ€ä½æˆåŠŸç‡
```

**æ›´æ–°**: `src/services/firecrawl/config/__init__.py`

```python
from .task_config import SearchConfig, CrawlConfig, ScrapeConfig, ConfigFactory
from .map_scrape_config import MapScrapeConfig

# æ‰©å±•ConfigFactory
class ConfigFactory:
    # ... ç°æœ‰æ–¹æ³• ...

    @staticmethod
    def create_map_scrape_config(config_dict: Dict[str, Any]) -> MapScrapeConfig:
        """ä»å­—å…¸åˆ›å»ºMapScrapeConfig"""
        # å¤„ç†æ—¥æœŸå­—ç¬¦ä¸²
        if 'start_date' in config_dict and isinstance(config_dict['start_date'], str):
            config_dict['start_date'] = datetime.fromisoformat(config_dict['start_date'])

        if 'end_date' in config_dict and isinstance(config_dict['end_date'], str):
            config_dict['end_date'] = datetime.fromisoformat(config_dict['end_date'])

        return MapScrapeConfig(**config_dict)
```

#### 1.3 æ›´æ–°TaskTypeæšä¸¾

**æ–‡ä»¶**: `src/core/domain/entities/search_task.py`

```python
class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    SEARCH_KEYWORD = "search_keyword"      # å…³é”®è¯æœç´¢æ¨¡å¼
    CRAWL_WEBSITE = "crawl_website"        # ç½‘ç«™çˆ¬å–æ¨¡å¼ï¼ˆCrawl APIï¼‰
    SCRAPE_URL = "scrape_url"              # å•é¡µé¢çˆ¬å–æ¨¡å¼
    MAP_SCRAPE_WEBSITE = "map_scrape_website"  # Map+Scrapeæ¨¡å¼ï¼ˆæ–°å¢ï¼‰
```

**æ‰©å±•SearchTaskç±»**ï¼š

```python
def is_map_scrape_mode(self) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºMap+Scrapeæ¨¡å¼"""
    return self.get_task_type() == TaskType.MAP_SCRAPE_WEBSITE
```

---

### Phase 2: æ ¸å¿ƒæ‰§è¡Œå™¨ï¼ˆç¬¬2å¤©ï¼‰

#### 2.1 åˆ›å»ºMapScrapeExecutor

**æ–‡ä»¶**: `src/services/firecrawl/executors/map_scrape_executor.py`

**å®Œæ•´ä»£ç ** (è§é™„å½•A)

**å…³é”®æ–¹æ³•**ï¼š

1. `execute()` - ä¸»æ‰§è¡Œæµç¨‹
2. `_execute_map()` - Map APIè°ƒç”¨
3. `_batch_scrape()` - æ‰¹é‡Scrape
4. `_filter_by_date()` - æ—¶é—´è¿‡æ»¤
5. `_convert_to_search_results()` - ç»“æœè½¬æ¢
6. `_save_raw_responses()` - ä¿å­˜åŸå§‹å“åº”

#### 2.2 æ³¨å†Œæ‰§è¡Œå™¨

**æ–‡ä»¶**: `src/services/firecrawl/factory.py`

```python
from .executors.map_scrape_executor import MapScrapeExecutor

class ExecutorFactory:
    """æ‰§è¡Œå™¨å·¥å‚ç±»"""

    _executors = {
        TaskType.SEARCH_KEYWORD: SearchExecutor,
        TaskType.CRAWL_WEBSITE: CrawlExecutor,
        TaskType.SCRAPE_URL: ScrapeExecutor,
        TaskType.MAP_SCRAPE_WEBSITE: MapScrapeExecutor,  # æ–°å¢
    }

    # ... å…¶ä»–æ–¹æ³•ä¸å˜ ...
```

#### 2.3 æ›´æ–°ç§¯åˆ†è®¡ç®—å™¨

**æ–‡ä»¶**: `src/services/firecrawl/credits_calculator.py`

```python
class FirecrawlCreditsCalculator:
    """Firecrawlç§¯åˆ†æ¶ˆè€—è®¡ç®—å™¨"""

    # ... ç°æœ‰æ–¹æ³• ...

    @staticmethod
    def calculate_map_scrape_credits(
        map_calls: int,
        urls_scraped: int
    ) -> int:
        """è®¡ç®—Map + Scrapeæ“ä½œçš„ç§¯åˆ†æ¶ˆè€—

        Args:
            map_calls: Map APIè°ƒç”¨æ¬¡æ•°
            urls_scraped: Scrapeçš„URLæ•°é‡

        Returns:
            int: æ€»ç§¯åˆ†æ¶ˆè€—
        """
        map_cost = map_calls * 1    # Map: 1 credit/call
        scrape_cost = urls_scraped * 1  # Scrape: 1 credit/URL

        return map_cost + scrape_cost
```

---

### Phase 3: æµ‹è¯•éªŒè¯ï¼ˆç¬¬3å¤©ï¼‰

#### 3.1 å•å…ƒæµ‹è¯•

**æ–‡ä»¶**: `tests/test_map_scrape_executor.py`

```python
import pytest
from datetime import datetime, timedelta
from src.services.firecrawl.executors.map_scrape_executor import MapScrapeExecutor
from src.services.firecrawl.config.map_scrape_config import MapScrapeConfig
from src.core.domain.entities.search_task import SearchTask, TaskType

class TestMapScrapeExecutor:

    @pytest.fixture
    def executor(self):
        return MapScrapeExecutor()

    @pytest.fixture
    def test_task(self):
        return SearchTask(
            name="æµ‹è¯•ä»»åŠ¡",
            crawl_url="https://example.com",
            task_type=TaskType.MAP_SCRAPE_WEBSITE,
            crawl_config={
                "search": "blog",
                "start_date": "2025-01-01",
                "end_date": "2025-12-31",
                "max_concurrent_scrapes": 3
            }
        )

    @pytest.mark.asyncio
    async def test_execute_map(self, executor):
        """æµ‹è¯•Map APIè°ƒç”¨"""
        config = MapScrapeConfig(search="blog")

        urls = await executor._execute_map("https://example.com", config)

        assert len(urls) > 0
        assert all('url' in link for link in urls)
        assert all('title' in link for link in urls)

    @pytest.mark.asyncio
    async def test_batch_scrape(self, executor):
        """æµ‹è¯•æ‰¹é‡Scrape"""
        config = MapScrapeConfig(max_concurrent_scrapes=2)
        urls = [
            {"url": "https://example.com/page1"},
            {"url": "https://example.com/page2"}
        ]

        results = await executor._batch_scrape(urls, config)

        assert len(results) == 2
        assert all(r.markdown is not None for r in results)

    def test_filter_by_date(self, executor):
        """æµ‹è¯•æ—¶é—´è¿‡æ»¤"""
        config = MapScrapeConfig(
            start_date=datetime(2025, 1, 1),
            end_date=datetime(2025, 12, 31)
        )

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        from src.core.domain.interfaces.crawler_interface import CrawlResult

        results = [
            CrawlResult(
                url="https://example.com/1",
                metadata={"publishedDate": "2025-06-15"}
            ),
            CrawlResult(
                url="https://example.com/2",
                metadata={"publishedDate": "2024-06-15"}  # è¶…å‡ºèŒƒå›´
            ),
        ]

        filtered = executor._filter_by_date(results, config)

        assert len(filtered) == 1
        assert filtered[0].url == "https://example.com/1"

    @pytest.mark.asyncio
    async def test_full_execute(self, executor, test_task):
        """æµ‹è¯•å®Œæ•´æ‰§è¡Œæµç¨‹"""
        batch = await executor.execute(test_task)

        assert batch.returned_count > 0
        assert batch.credits_used > 0
        assert all(r.source == "map_scrape" for r in batch.results)
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
pytest tests/test_map_scrape_executor.py -v
```

#### 3.2 é›†æˆæµ‹è¯•

**åˆ›å»ºæµ‹è¯•è„šæœ¬**: `scripts/test_map_scrape_integration.py`

```python
#!/usr/bin/env python3
"""Map+Scrapeæ‰§è¡Œå™¨é›†æˆæµ‹è¯•"""

import asyncio
import sys
from datetime import datetime, timedelta

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.core.domain.entities.search_task import SearchTask, TaskType
from src.services.firecrawl import ExecutorFactory

async def test_map_scrape():
    """æµ‹è¯•Map+Scrapeå®Œæ•´æµç¨‹"""

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = SearchTask(
        name="æµ‹è¯•Map+Scrape",
        crawl_url="https://news.ycombinator.com",  # Hacker News
        task_type=TaskType.MAP_SCRAPE_WEBSITE,
        crawl_config={
            "search": "show",  # åªè¦Show HNçš„å¸–å­
            "start_date": (datetime.now() - timedelta(days=7)).isoformat(),
            "end_date": datetime.now().isoformat(),
            "max_concurrent_scrapes": 5,
            "map_limit": 100
        }
    )

    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)

    print("=== å¼€å§‹æ‰§è¡ŒMap+Scrape ===\n")

    # æ‰§è¡Œ
    batch = await executor.execute(task)

    # è¾“å‡ºç»“æœ
    print(f"\n=== æ‰§è¡Œç»“æœ ===")
    print(f"å‘ç°URLæ•°é‡: {batch.total_count}")
    print(f"è¿”å›ç»“æœæ•°é‡: {batch.returned_count}")
    print(f"ç§¯åˆ†æ¶ˆè€—: {batch.credits_used}")
    print(f"æ‰§è¡Œæ—¶é—´: {batch.execution_time_ms}ms")

    print(f"\n=== å‰5æ¡ç»“æœ ===")
    for i, result in enumerate(batch.results[:5], 1):
        print(f"\n[{i}] {result.title}")
        print(f"    URL: {result.url}")
        print(f"    å‘å¸ƒæ—¶é—´: {result.published_date}")
        print(f"    å†…å®¹é¢„è§ˆ: {result.snippet}")

    return batch

if __name__ == "__main__":
    batch = asyncio.run(test_map_scrape())
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
```

**è¿è¡Œé›†æˆæµ‹è¯•**ï¼š
```bash
python scripts/test_map_scrape_integration.py
```

---

### Phase 4: æ–‡æ¡£æ›´æ–°ï¼ˆç¬¬4å¤©ï¼‰

#### 4.1 æ›´æ–°æ¶æ„æ–‡æ¡£

**æ–‡ä»¶**: `docs/FIRECRAWL_ARCHITECTURE_V2.md`

**æ–°å¢ç« èŠ‚**ï¼š

```markdown
### 5. MapScrapeExecutor (Map+Scrapeæ‰§è¡Œå™¨)

**ä½ç½®**: `src/services/firecrawl/executors/map_scrape_executor.py`

**èŒè´£**:
- ä½¿ç”¨Map APIå¿«é€Ÿå‘ç°URL
- æ‰¹é‡ScrapeæŒ‡å®šURL
- æŒ‰å‘å¸ƒæ—¶é—´è¿‡æ»¤ç»“æœ
- ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡

**å·¥ä½œæµç¨‹**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Map API - å‘ç°URL                                 â”‚
â”‚  è¾“å…¥: èµ·å§‹URL, searchå‚æ•°                                â”‚
â”‚  è¾“å‡º: URLåˆ—è¡¨                                            â”‚
â”‚  æ—¶é—´: ~5ç§’                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: Batch Scrape - çˆ¬å–å†…å®¹                           â”‚
â”‚  è¾“å…¥: URLåˆ—è¡¨                                             â”‚
â”‚  è¾“å‡º: é¡µé¢å†…å®¹ + metadata                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: æ—¶é—´è¿‡æ»¤                                           â”‚
â”‚  è¿‡æ»¤: æ ¹æ®publishedDate                                  â”‚
â”‚  è¾“å‡º: ç¬¦åˆæ¡ä»¶çš„ç»“æœ                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®é…ç½®**:

```python
MapScrapeConfig(
    search="blog",                    # æœç´¢å…³é”®è¯
    map_limit=5000,                   # Mapè¿”å›é™åˆ¶
    start_date=datetime(2025, 1, 1),  # å¼€å§‹æ—¥æœŸ
    end_date=datetime(2025, 12, 31),  # ç»“æŸæ—¥æœŸ
    max_concurrent_scrapes=5,         # å¹¶å‘æ•°
)
```

**é€‚ç”¨åœºæ™¯**:
- åªéœ€è¦ç‰¹å®šæ—¶é—´èŒƒå›´çš„å†…å®¹
- éœ€è¦ç²¾ç¡®æ§åˆ¶çˆ¬å–ç›®æ ‡
- å…³æ³¨APIç§¯åˆ†æˆæœ¬
- å®šæœŸç›‘æ§ç½‘ç«™æ›´æ–°
```

**æ›´æ–°ä»»åŠ¡ç±»å‹è¡¨æ ¼**ï¼š

| ä»»åŠ¡ç±»å‹ | è¾“å…¥ | APIä½¿ç”¨ | è¾“å‡º | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|------|----------|
| **SEARCH_KEYWORD** | å…³é”®è¯ | Search + Scrape | æœç´¢ç»“æœ + è¯¦æƒ… | è¡Œä¸šèµ„è®¯ |
| **CRAWL_WEBSITE** | èµ·å§‹URL | Crawl | æ•´ç«™å†…å®¹ | å®Œæ•´å½’æ¡£ |
| **SCRAPE_URL** | å•ä¸ªURL | Scrape | å•é¡µå†…å®¹ | é¡µé¢ç›‘æ§ |
| **MAP_SCRAPE_WEBSITE** | èµ·å§‹URL + æ—¶é—´ | Map + Scrape | è¿‡æ»¤åå†…å®¹ | ç²¾ç¡®çˆ¬å– |

#### 4.2 åˆ›å»ºä½¿ç”¨ç¤ºä¾‹

**æ–‡ä»¶**: `docs/MAP_SCRAPE_USAGE_EXAMPLES.md`

```markdown
# Map+Scrape ä½¿ç”¨ç¤ºä¾‹

## ç¤ºä¾‹1: çˆ¬å–æœ€è¿‘30å¤©çš„åšå®¢æ–‡ç« 

```python
from datetime import datetime, timedelta
from src.core.domain.entities.search_task import SearchTask, TaskType

# åˆ›å»ºä»»åŠ¡
task = SearchTask(
    name="æŠ€æœ¯åšå®¢æœ€è¿‘æ–‡ç« ",
    crawl_url="https://example.com/blog",
    task_type=TaskType.MAP_SCRAPE_WEBSITE,
    crawl_config={
        "search": "python",  # åªè¦åŒ…å«pythonçš„æ–‡ç« 
        "start_date": (datetime.now() - timedelta(days=30)).isoformat(),
        "end_date": datetime.now().isoformat(),
        "max_concurrent_scrapes": 5
    }
)

# æ‰§è¡Œ
executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
batch = await executor.execute(task)

print(f"è·å– {batch.returned_count} ç¯‡æœ€è¿‘æ–‡ç« ")
print(f"æ¶ˆè€— {batch.credits_used} ç§¯åˆ†")
```

## ç¤ºä¾‹2: åªçˆ¬å–ç‰¹å®šåˆ†ç±»çš„é¡µé¢

```python
task = SearchTask(
    name="äº§å“æ–‡æ¡£çˆ¬å–",
    crawl_url="https://docs.example.com",
    task_type=TaskType.MAP_SCRAPE_WEBSITE,
    crawl_config={
        "search": "/api/",  # åªè¦APIæ–‡æ¡£
        "map_limit": 1000,
        "max_concurrent_scrapes": 10
    }
)
```
```

---

## ğŸ“Š å®æ–½æ£€æŸ¥æ¸…å•

### âœ… ä»£ç å®ç°

- [ ] FirecrawlAdapter.map()æ–¹æ³•
- [ ] MapScrapeConfigé…ç½®ç±»
- [ ] MapScrapeExecutoræ‰§è¡Œå™¨
- [ ] TaskType.MAP_SCRAPE_WEBSITEæšä¸¾
- [ ] ExecutorFactoryæ³¨å†Œ
- [ ] ç§¯åˆ†è®¡ç®—å™¨æ›´æ–°

### âœ… æµ‹è¯•éªŒè¯

- [ ] å•å…ƒæµ‹è¯•ï¼ˆtest_map_scrape_executor.pyï¼‰
- [ ] é›†æˆæµ‹è¯•ï¼ˆtest_map_scrape_integration.pyï¼‰
- [ ] Map APIæµ‹è¯•
- [ ] æ—¶é—´è¿‡æ»¤æµ‹è¯•
- [ ] å¹¶å‘Scrapeæµ‹è¯•
- [ ] çœŸå®åœºæ™¯éªŒè¯

### âœ… æ–‡æ¡£å®Œå–„

- [x] MAP_SCRAPE_EXECUTOR_DESIGN.mdï¼ˆè¯¦ç»†è®¾è®¡ï¼‰
- [x] FIRECRAWL_MAP_API_GUIDE.mdï¼ˆAPIæŒ‡å—ï¼‰
- [x] MAP_SCRAPE_IMPLEMENTATION_PLAN.mdï¼ˆæœ¬æ–‡æ¡£ï¼‰
- [ ] FIRECRAWL_ARCHITECTURE_V2.mdï¼ˆæ¶æ„æ›´æ–°ï¼‰
- [ ] MAP_SCRAPE_USAGE_EXAMPLES.mdï¼ˆä½¿ç”¨ç¤ºä¾‹ï¼‰

### âœ… éƒ¨ç½²å‡†å¤‡

- [ ] ä»£ç å®¡æŸ¥
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] é”™è¯¯å¤„ç†éªŒè¯
- [ ] æ—¥å¿—å®Œå–„
- [ ] ç›‘æ§é…ç½®

---

## ğŸ“ˆ é¢„æœŸæˆæœ

### åŠŸèƒ½éªŒæ”¶æ ‡å‡†

1. âœ… Map APIæ­£å¸¸è°ƒç”¨å¹¶è¿”å›URLåˆ—è¡¨
2. âœ… Batch ScrapeæˆåŠŸçˆ¬å–æ‰€æœ‰URL
3. âœ… æ—¶é—´è¿‡æ»¤æ­£ç¡®ç­›é€‰ç»“æœ
4. âœ… æ•°æ®åº“å­—æ®µå®Œå…¨å…¼å®¹
5. âœ… ç§¯åˆ†æ¶ˆè€—è®¡ç®—å‡†ç¡®
6. âœ… é”™è¯¯å¤„ç†å¥å£®

### æ€§èƒ½æŒ‡æ ‡

- **Map APIå“åº”**: <5ç§’
- **Scrapeå¹¶å‘**: 5ä¸ª/æ¬¡
- **æˆåŠŸç‡**: >80%
- **ç§¯åˆ†èŠ‚çœ**: 80-90%ï¼ˆvs Crawl APIï¼‰

### æ–‡æ¡£å®Œæ•´æ€§

- [x] è®¾è®¡æ–‡æ¡£å®Œæ•´
- [x] APIæŒ‡å—æ¸…æ™°
- [x] ä½¿ç”¨ç¤ºä¾‹ä¸°å¯Œ
- [ ] æ¶æ„æ–‡æ¡£æ›´æ–°
- [ ] æ•…éšœæ’æŸ¥æŒ‡å—

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œ

1. **å®ç°Map APIè°ƒç”¨**
   ```bash
   # ç¼–è¾‘ firecrawl_adapter.py
   # å®ç° map() æ–¹æ³•
   # è¿è¡Œæµ‹è¯•: python scripts/test_map_api.py
   ```

2. **åˆ›å»ºé…ç½®ç±»**
   ```bash
   # åˆ›å»º map_scrape_config.py
   # å®šä¹‰ MapScrapeConfig
   ```

3. **å®ç°æ‰§è¡Œå™¨**
   ```bash
   # åˆ›å»º map_scrape_executor.py
   # å®ç°æ ¸å¿ƒé€»è¾‘
   ```

### åç»­è®¡åˆ’

- **Week 1**: æ ¸å¿ƒåŠŸèƒ½å®ç° + å•å…ƒæµ‹è¯•
- **Week 2**: é›†æˆæµ‹è¯• + æ–‡æ¡£å®Œå–„
- **Week 3**: çœŸå®åœºæ™¯éªŒè¯ + æ€§èƒ½ä¼˜åŒ–
- **Week 4**: ä»£ç å®¡æŸ¥ + ç”Ÿäº§éƒ¨ç½²

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Development Team
**æœ€åæ›´æ–°**: 2025-11-06
**å®æ–½çŠ¶æ€**: è®¾è®¡å®Œæˆï¼Œå¾…å¼€å‘
