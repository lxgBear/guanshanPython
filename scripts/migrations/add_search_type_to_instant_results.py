#!/usr/bin/env python3
"""
æ•°æ®åº“è¿ç§»è„šæœ¬ï¼šä¸º instant_search_results æ·»åŠ  search_type å­—æ®µ

v2.1.0 å³æ—¶+æ™ºèƒ½æœç´¢ç»Ÿä¸€æ¶æ„è¿ç§» - Phase 1

ç›®æ ‡ï¼š
1. ä¸ºæ‰€æœ‰ç°æœ‰ instant_search_results è®°å½•æ·»åŠ  search_type="instant"
2. åˆ›å»ºå¤åˆç´¢å¼• (search_type, task_id, created_at)
3. éªŒè¯è¿ç§»ç»“æœ

æ‰§è¡Œæ–¹å¼ï¼š
    python scripts/migrations/add_search_type_to_instant_results.py --execute

å®‰å…¨æ£€æŸ¥ï¼š
    python scripts/migrations/add_search_type_to_instant_results.py --dry-run
"""

import asyncio
import argparse
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from motor.motor_asyncio import AsyncIOMotorClient
from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def check_existing_search_type_field():
    """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ search_type å­—æ®µ"""
    try:
        db = await get_mongodb_database()
        collection = db["instant_search_results"]

        # æ£€æŸ¥æ˜¯å¦æœ‰è®°å½•å·²ç»æœ‰ search_type å­—æ®µ
        sample = await collection.find_one({"search_type": {"$exists": True}})

        if sample:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°å·²æœ‰è®°å½•åŒ…å« search_type å­—æ®µï¼Œå¯èƒ½å·²ç»æ‰§è¡Œè¿‡è¿ç§»")
            return True

        logger.info("âœ… æœªæ£€æµ‹åˆ° search_type å­—æ®µï¼Œå¯ä»¥å®‰å…¨æ‰§è¡Œè¿ç§»")
        return False

    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥ search_type å­—æ®µå¤±è´¥: {e}")
        raise


async def get_migration_stats():
    """è·å–è¿ç§»å‰çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        db = await get_mongodb_database()
        collection = db["instant_search_results"]

        # æ€»è®°å½•æ•°
        total_count = await collection.count_documents({})

        # å·²æœ‰ search_type çš„è®°å½•æ•°
        with_search_type = await collection.count_documents({"search_type": {"$exists": True}})

        # éœ€è¦è¿ç§»çš„è®°å½•æ•°
        needs_migration = total_count - with_search_type

        stats = {
            "total_count": total_count,
            "with_search_type": with_search_type,
            "needs_migration": needs_migration
        }

        logger.info(f"""
ğŸ“Š è¿ç§»å‰ç»Ÿè®¡:
   - æ€»è®°å½•æ•°: {total_count}
   - å·²æœ‰ search_type: {with_search_type}
   - éœ€è¦è¿ç§»: {needs_migration}
        """)

        return stats

    except Exception as e:
        logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise


async def add_search_type_field_batch(batch_size: int = 1000, dry_run: bool = False):
    """æ‰¹é‡æ·»åŠ  search_type å­—æ®µ"""
    try:
        db = await get_mongodb_database()
        collection = db["instant_search_results"]

        # æŸ¥æ‰¾æ‰€æœ‰æ²¡æœ‰ search_type å­—æ®µçš„è®°å½•
        query = {"search_type": {"$exists": False}}

        total_updated = 0
        batch_count = 0

        while True:
            # åˆ†æ‰¹è·å–éœ€è¦æ›´æ–°çš„è®°å½•ID
            cursor = collection.find(query, {"_id": 1}).limit(batch_size)
            ids = [doc["_id"] async for doc in cursor]

            if not ids:
                break  # æ²¡æœ‰æ›´å¤šè®°å½•éœ€è¦æ›´æ–°

            batch_count += 1

            if dry_run:
                logger.info(f"ğŸ” [DRY-RUN] Batch {batch_count}: å°†æ›´æ–° {len(ids)} æ¡è®°å½•")
                total_updated += len(ids)
            else:
                # æ‰¹é‡æ›´æ–°
                result = await collection.update_many(
                    {"_id": {"$in": ids}},
                    {"$set": {"search_type": "instant"}}
                )

                total_updated += result.modified_count
                logger.info(f"âœ… Batch {batch_count}: æˆåŠŸæ›´æ–° {result.modified_count} æ¡è®°å½•")

            # å¦‚æœæœ¬æ‰¹æ¬¡è®°å½•å°‘äº batch_sizeï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€æ‰¹
            if len(ids) < batch_size:
                break

        logger.info(f"{'ğŸ” [DRY-RUN]' if dry_run else 'âœ…'} æ€»è®¡æ›´æ–°: {total_updated} æ¡è®°å½•")
        return total_updated

    except Exception as e:
        logger.error(f"âŒ æ·»åŠ  search_type å­—æ®µå¤±è´¥: {e}")
        raise


async def create_search_type_index(dry_run: bool = False):
    """åˆ›å»º search_type å¤åˆç´¢å¼•"""
    try:
        db = await get_mongodb_database()
        collection = db["instant_search_results"]

        # ç´¢å¼•åç§°
        index_name = "idx_search_type_task_created"

        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
        existing_indexes = await collection.index_information()

        if index_name in existing_indexes:
            logger.warning(f"âš ï¸ ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
            return

        if dry_run:
            logger.info(f"ğŸ” [DRY-RUN] å°†åˆ›å»ºç´¢å¼•: {index_name} on (search_type, task_id, created_at)")
        else:
            # åˆ›å»ºå¤åˆç´¢å¼•ï¼šsearch_type + task_id + created_atï¼ˆé™åºï¼‰
            await collection.create_index(
                [
                    ("search_type", 1),
                    ("task_id", 1),
                    ("created_at", -1)
                ],
                name=index_name,
                background=True  # åå°åˆ›å»ºï¼Œä¸é˜»å¡æ•°æ®åº“æ“ä½œ
            )
            logger.info(f"âœ… æˆåŠŸåˆ›å»ºç´¢å¼•: {index_name}")

    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        raise


async def verify_migration():
    """éªŒè¯è¿ç§»ç»“æœ"""
    try:
        db = await get_mongodb_database()
        collection = db["instant_search_results"]

        # æ€»è®°å½•æ•°
        total_count = await collection.count_documents({})

        # æœ‰ search_type="instant" çš„è®°å½•æ•°
        instant_count = await collection.count_documents({"search_type": "instant"})

        # æ²¡æœ‰ search_type å­—æ®µçš„è®°å½•æ•°
        missing_count = await collection.count_documents({"search_type": {"$exists": False}})

        # æ£€æŸ¥ç´¢å¼•
        indexes = await collection.index_information()
        index_exists = "idx_search_type_task_created" in indexes

        logger.info(f"""
ğŸ” è¿ç§»éªŒè¯ç»“æœ:
   - æ€»è®°å½•æ•°: {total_count}
   - search_type="instant": {instant_count}
   - ç¼ºå°‘ search_type: {missing_count}
   - ç´¢å¼•å·²åˆ›å»º: {'âœ…' if index_exists else 'âŒ'}
        """)

        if missing_count > 0:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: è¿˜æœ‰ {missing_count} æ¡è®°å½•ç¼ºå°‘ search_type å­—æ®µ")
            return False

        if not index_exists:
            logger.error("âŒ éªŒè¯å¤±è´¥: ç´¢å¼•æœªåˆ›å»º")
            return False

        logger.info("âœ… è¿ç§»éªŒè¯é€šè¿‡ï¼")
        return True

    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿ç§»ç»“æœå¤±è´¥: {e}")
        raise


async def run_migration(dry_run: bool = False, batch_size: int = 1000):
    """æ‰§è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
    try:
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹è¿ç§»: ä¸º instant_search_results æ·»åŠ  search_type å­—æ®µ")
        logger.info(f"æ¨¡å¼: {'DRY-RUN (ä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®)' if dry_run else 'EXECUTE (å°†å®é™…ä¿®æ”¹æ•°æ®)'}")
        logger.info(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        logger.info("=" * 60)

        # Step 1: æ£€æŸ¥æ˜¯å¦å·²è¿ç§»
        logger.info("\nğŸ“‹ Step 1: æ£€æŸ¥ç°æœ‰çŠ¶æ€...")
        already_migrated = await check_existing_search_type_field()

        # Step 2: è·å–ç»Ÿè®¡ä¿¡æ¯
        logger.info("\nğŸ“‹ Step 2: è·å–ç»Ÿè®¡ä¿¡æ¯...")
        stats = await get_migration_stats()

        if stats["needs_migration"] == 0:
            logger.info("âœ… æ‰€æœ‰è®°å½•å·²æœ‰ search_type å­—æ®µï¼Œæ— éœ€è¿ç§»")
            if not dry_run:
                # å³ä½¿è®°å½•å·²è¿ç§»ï¼Œä¹Ÿè¦ç¡®ä¿ç´¢å¼•å­˜åœ¨
                logger.info("\nğŸ“‹ ç¡®ä¿ç´¢å¼•å­˜åœ¨...")
                await create_search_type_index(dry_run=False)
            return

        # Step 3: æ·»åŠ  search_type å­—æ®µ
        logger.info(f"\nğŸ“‹ Step 3: æ‰¹é‡æ·»åŠ  search_type å­—æ®µ (batch_size={batch_size})...")
        updated_count = await add_search_type_field_batch(batch_size=batch_size, dry_run=dry_run)

        # Step 4: åˆ›å»ºç´¢å¼•
        logger.info("\nğŸ“‹ Step 4: åˆ›å»ºå¤åˆç´¢å¼•...")
        await create_search_type_index(dry_run=dry_run)

        # Step 5: éªŒè¯ï¼ˆä»…åœ¨é dry-run æ¨¡å¼ä¸‹ï¼‰
        if not dry_run:
            logger.info("\nğŸ“‹ Step 5: éªŒè¯è¿ç§»ç»“æœ...")
            success = await verify_migration()

            if success:
                logger.info("\n" + "=" * 60)
                logger.info("âœ… è¿ç§»æˆåŠŸå®Œæˆï¼")
                logger.info("=" * 60)
            else:
                logger.error("\n" + "=" * 60)
                logger.error("âŒ è¿ç§»éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
                logger.error("=" * 60)
                sys.exit(1)
        else:
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ” DRY-RUN å®Œæˆï¼Œæœªå®é™…ä¿®æ”¹æ•°æ®")
            logger.info("æ‰§è¡Œå®é™…è¿ç§»è¯·è¿è¡Œ: python scripts/migrations/add_search_type_to_instant_results.py --execute")
            logger.info("=" * 60)

    except Exception as e:
        logger.error(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        sys.exit(1)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸º instant_search_results æ·»åŠ  search_type å­—æ®µ (v2.1.0 Phase 1)"
    )
    parser.add_argument(
        "--execute",
        action="store_true",
        help="æ‰§è¡Œå®é™…è¿ç§»ï¼ˆé»˜è®¤ä¸º dry-run æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤1000æ¡ï¼‰"
    )

    args = parser.parse_args()

    dry_run = not args.execute

    await run_migration(dry_run=dry_run, batch_size=args.batch_size)


if __name__ == "__main__":
    asyncio.run(main())
