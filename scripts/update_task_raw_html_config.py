#!/usr/bin/env python3
"""
æ›´æ–°ä»»åŠ¡é…ç½®ä»¥è·å–åŸå§‹ HTML

ç›®çš„ï¼š
- å°† only_main_content è®¾ç½®ä¸º False
- å°† exclude_tags è®¾ç½®ä¸ºç©ºæ•°ç»„ []
- ç¡®ä¿è·å–å®Œæ•´çš„åŸå§‹ HTMLï¼Œä¸åšä»»ä½•è¿‡æ»¤
"""

import asyncio
import sys

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def update_task_for_raw_html(task_id: str):
    """æ›´æ–°æŒ‡å®šä»»åŠ¡çš„é…ç½®ä»¥è·å–åŸå§‹ HTML"""

    try:
        db = await get_mongodb_database()

        # æŸ¥è¯¢ä»»åŠ¡
        task = await db.search_tasks.find_one({"_id": task_id})
        if not task:
            logger.error(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return False

        logger.info("=" * 80)
        logger.info(f"ğŸ“‹ æ›´æ–°ä»»åŠ¡é…ç½®: {task.get('name')}")
        logger.info("=" * 80)

        # è·å–å½“å‰é…ç½®
        current_config = task.get('crawl_config', {})

        logger.info(f"\nğŸ” å½“å‰é…ç½®:")
        logger.info(f"  only_main_content: {current_config.get('only_main_content')}")
        logger.info(f"  exclude_tags: {current_config.get('exclude_tags')}")

        # æ›´æ–°é…ç½®ä¸ºåŸå§‹ HTML æ¨¡å¼
        current_config['only_main_content'] = False
        current_config['exclude_tags'] = []

        logger.info(f"\nâœ… æ–°é…ç½® (åŸå§‹ HTML æ¨¡å¼):")
        logger.info(f"  only_main_content: {current_config['only_main_content']}")
        logger.info(f"  exclude_tags: {current_config['exclude_tags']}")

        # ä¿ç•™å…¶ä»–é…ç½®é¡¹
        logger.info(f"\nğŸ“ ä¿ç•™çš„å…¶ä»–é…ç½®:")
        logger.info(f"  limit: {current_config.get('limit')}")
        logger.info(f"  max_depth: {current_config.get('max_depth')}")
        logger.info(f"  wait_for: {current_config.get('wait_for')}")
        logger.info(f"  prompt: {current_config.get('prompt')}")

        # æ›´æ–°æ•°æ®åº“
        result = await db.search_tasks.update_one(
            {"_id": task_id},
            {"$set": {"crawl_config": current_config}}
        )

        if result.modified_count > 0:
            logger.info(f"\nâœ… ä»»åŠ¡é…ç½®å·²æ›´æ–°")
            logger.info(f"\nğŸ’¡ è¯´æ˜:")
            logger.info(f"  â€¢ only_main_content=false - ä¿ç•™å®Œæ•´é¡µé¢ç»“æ„")
            logger.info(f"  â€¢ exclude_tags=[] - ä¸ç§»é™¤ä»»ä½• HTML æ ‡ç­¾")
            logger.info(f"  â€¢ çˆ¬å–ç»“æœå°†åŒ…å«å¯¼èˆªã€é¡µè„šã€å¹¿å‘Šç­‰æ‰€æœ‰å…ƒç´ ")
            logger.info(f"  â€¢ HTML æ–‡ä»¶å¤§å°å¯èƒ½å¢åŠ  2-5 å€")
            return True
        else:
            logger.warning(f"\nâš ï¸  ä»»åŠ¡é…ç½®æœªå‘ç”Ÿå˜åŒ–")
            return False

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def update_all_crawl_tasks():
    """æ›´æ–°æ‰€æœ‰ crawl_website ç±»å‹ä»»åŠ¡çš„é…ç½®"""

    try:
        db = await get_mongodb_database()

        # æŸ¥è¯¢æ‰€æœ‰ crawl_website ä»»åŠ¡
        tasks = await db.search_tasks.find({"task_type": "crawl_website"}).to_list(length=100)

        logger.info("=" * 80)
        logger.info(f"ğŸ“Š æ‰¹é‡æ›´æ–°æ‰€æœ‰çˆ¬å–ä»»åŠ¡é…ç½®")
        logger.info("=" * 80)
        logger.info(f"\næ‰¾åˆ° {len(tasks)} ä¸ªçˆ¬å–ä»»åŠ¡\n")

        success_count = 0

        for task in tasks:
            task_id = task['_id']
            task_name = task.get('name', 'æœªå‘½å')

            logger.info(f"å¤„ç†ä»»åŠ¡: {task_name} ({task_id})")

            # è·å–å½“å‰é…ç½®
            current_config = task.get('crawl_config', {})

            # æ›´æ–°é…ç½®
            current_config['only_main_content'] = False
            current_config['exclude_tags'] = []

            # æ›´æ–°æ•°æ®åº“
            result = await db.search_tasks.update_one(
                {"_id": task_id},
                {"$set": {"crawl_config": current_config}}
            )

            if result.modified_count > 0:
                logger.info(f"  âœ… å·²æ›´æ–°\n")
                success_count += 1
            else:
                logger.info(f"  âš ï¸  æœªå˜åŒ–\n")

        logger.info("=" * 80)
        logger.info(f"ğŸ“ˆ æ›´æ–°å®Œæˆ: {success_count}/{len(tasks)} ä¸ªä»»åŠ¡")
        logger.info("=" * 80)

        return True

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""

    if len(sys.argv) > 1:
        # æ›´æ–°æŒ‡å®šä»»åŠ¡
        task_id = sys.argv[1]
        logger.info(f"\nğŸ¯ æ›´æ–°å•ä¸ªä»»åŠ¡: {task_id}\n")
        success = await update_task_for_raw_html(task_id)
    else:
        # æ›´æ–°æ‰€æœ‰ä»»åŠ¡
        logger.info(f"\nğŸ¯ æ‰¹é‡æ›´æ–°æ‰€æœ‰çˆ¬å–ä»»åŠ¡\n")
        success = await update_all_crawl_tasks()

    logger.info("\n" + "=" * 80)
    if success:
        logger.info("âœ… æ“ä½œå®Œæˆ")
        logger.info("\nä¸‹ä¸€æ­¥:")
        logger.info("  1. é‡æ–°æ‰§è¡Œçˆ¬å–ä»»åŠ¡")
        logger.info("  2. å¯¹æ¯” HTML å†…å®¹å·®å¼‚")
        logger.info("  3. éªŒè¯æ˜¯å¦åŒ…å«å¯¼èˆªã€é¡µè„šç­‰å…ƒç´ ")
    else:
        logger.info("âŒ æ“ä½œå¤±è´¥")
    logger.info("=" * 80)

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
