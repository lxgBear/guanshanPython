#!/usr/bin/env python3
"""
æ¸…ç†æ—§çš„ processed_results é›†åˆ

å½“å‰æƒ…å†µ:
- processed_results_new å·²ç»å­˜åœ¨å¹¶ä¸”æœ‰æ•°æ®ï¼ˆä»£ç å·²æŒ‡å‘æ–°é›†åˆï¼‰
- processed_results æ—§é›†åˆè¿˜åœ¨ï¼Œä½†ä»£ç å·²ä¸å†ä½¿ç”¨
- éœ€è¦åˆ é™¤æ—§é›†åˆä»¥é¿å…æ··æ·†

åŠŸèƒ½:
1. éªŒè¯ä»£ç å·²æŒ‡å‘ processed_results_new
2. å¤‡ä»½æ—§é›†åˆç»Ÿè®¡ä¿¡æ¯
3. åˆ é™¤æ—§é›†åˆ processed_results
4. ä¸ºæ–°é›†åˆåˆ›å»ºå¿…è¦çš„ç´¢å¼•

å®‰å…¨ä¿éšœ:
- åˆ é™¤å‰äºŒæ¬¡ç¡®è®¤
- æ˜¾ç¤ºè¯¦ç»†çš„é›†åˆä¿¡æ¯
- æä¾›æ•°æ®å¤‡ä»½å»ºè®®

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-11-04
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def get_collection_info(db, collection_name: str) -> dict:
    """è·å–é›†åˆè¯¦ç»†ä¿¡æ¯"""
    try:
        stats = await db.command("collStats", collection_name)

        # è·å–ç´¢å¼•ä¿¡æ¯
        collection = db[collection_name]
        indexes = await collection.index_information()

        return {
            "exists": True,
            "count": stats.get("count", 0),
            "size": stats.get("size", 0),
            "storage_size": stats.get("storageSize", 0),
            "indexes": list(indexes.keys()),
            "index_count": len(indexes)
        }
    except Exception as e:
        return {"exists": False, "error": str(e)}


async def verify_code_points_to_new_collection():
    """éªŒè¯ä»£ç æ˜¯å¦å·²æŒ‡å‘æ–°é›†åˆ"""
    print("\nğŸ“ éªŒè¯ä»£ç é…ç½®...")

    # è¯»å– Repository æ–‡ä»¶æ£€æŸ¥é›†åˆå
    repo_file = project_root / "src/infrastructure/database/processed_result_repositories.py"

    if not repo_file.exists():
        print("âŒ æ— æ³•æ‰¾åˆ° Repository æ–‡ä»¶")
        return False

    content = repo_file.read_text()

    if 'self.collection_name = "processed_results_new"' in content:
        print("âœ… ä»£ç å·²æ­£ç¡®æŒ‡å‘ processed_results_new")
        return True
    elif 'self.collection_name = "processed_results"' in content:
        print("âŒ ä»£ç ä»æŒ‡å‘æ—§é›†åˆ processed_results")
        print("â„¹ï¸  è¯·å…ˆè¿è¡Œä»£ç ä¿®æ”¹ï¼Œå°†é›†åˆåæ”¹ä¸º processed_results_new")
        return False
    else:
        print("âš ï¸ æ— æ³•ç¡®è®¤é›†åˆåé…ç½®")
        return False


async def backup_collection_stats(db, collection_name: str):
    """å¤‡ä»½é›†åˆç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“ å¤‡ä»½ {collection_name} ç»Ÿè®¡ä¿¡æ¯...")

    info = await get_collection_info(db, collection_name)

    if not info.get("exists"):
        print(f"âš ï¸ é›†åˆ {collection_name} ä¸å­˜åœ¨")
        return

    # ä¿å­˜åˆ°æ–‡ä»¶
    backup_file = project_root / f"claudedocs/backup_{collection_name}_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    backup_data = {
        "collection_name": collection_name,
        "backup_time": datetime.now().isoformat(),
        "stats": info
    }

    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(backup_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {backup_file}")
    print(f"  - è®°å½•æ•°: {info['count']}")
    print(f"  - æ•°æ®å¤§å°: {info['size'] / 1024:.2f} KB")
    print(f"  - ç´¢å¼•: {', '.join(info['indexes'])}")


async def create_indexes_for_new_collection(db):
    """ä¸ºæ–°é›†åˆåˆ›å»ºå¿…è¦çš„ç´¢å¼•"""
    print("\nğŸ“ ä¸º processed_results_new åˆ›å»ºç´¢å¼•...")

    collection = db['processed_results_new']

    # è·å–ç°æœ‰ç´¢å¼•
    existing_indexes = await collection.index_information()
    existing_index_names = set(existing_indexes.keys())

    print(f"ç°æœ‰ç´¢å¼•: {', '.join(existing_index_names)}")

    # å®šä¹‰éœ€è¦çš„ç´¢å¼•
    indexes_to_create = [
        {
            "name": "task_id_index",
            "keys": [("task_id", 1)],
            "description": "ä»»åŠ¡IDç´¢å¼•ï¼ˆæŸ¥è¯¢æŸä»»åŠ¡çš„æ‰€æœ‰ç»“æœï¼‰"
        },
        {
            "name": "status_index",
            "keys": [("status", 1)],
            "description": "çŠ¶æ€ç´¢å¼•ï¼ˆç­›é€‰ä¸åŒçŠ¶æ€çš„ç»“æœï¼‰"
        },
        {
            "name": "task_id_status_index",
            "keys": [("task_id", 1), ("status", 1)],
            "description": "ä»»åŠ¡ID+çŠ¶æ€å¤åˆç´¢å¼•ï¼ˆå¸¸ç”¨ç»„åˆæŸ¥è¯¢ï¼‰"
        },
        {
            "name": "created_at_index",
            "keys": [("created_at", -1)],
            "description": "åˆ›å»ºæ—¶é—´ç´¢å¼•ï¼ˆæ—¶é—´æ’åºï¼Œé™åºï¼‰"
        }
    ]

    created_count = 0
    skipped_count = 0

    for index_config in indexes_to_create:
        index_name = index_config["name"]

        if index_name in existing_index_names:
            print(f"  â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„ç´¢å¼•: {index_name}")
            skipped_count += 1
            continue

        try:
            await collection.create_index(
                index_config["keys"],
                name=index_name
            )
            print(f"  âœ… åˆ›å»ºç´¢å¼•: {index_name}")
            print(f"     è¯´æ˜: {index_config['description']}")
            created_count += 1
        except Exception as e:
            print(f"  âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {index_name} - {e}")

    print(f"\nğŸ“Š ç´¢å¼•åˆ›å»ºæ€»ç»“:")
    print(f"  âœ… æ–°åˆ›å»º: {created_count} ä¸ª")
    print(f"  â­ï¸  è·³è¿‡: {skipped_count} ä¸ª")


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("æ¸…ç†æ—§çš„ processed_results é›†åˆ")
    print("="*60)
    print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().isoformat()}\n")

    try:
        # æ­¥éª¤ 1: éªŒè¯ä»£ç é…ç½®
        if not await verify_code_points_to_new_collection():
            print("\nâŒ ä»£ç é…ç½®éªŒè¯å¤±è´¥ï¼Œåœæ­¢æ¸…ç†æ“ä½œ")
            return 1

        # æ­¥éª¤ 2: è¿æ¥æ•°æ®åº“
        print("\nğŸ“ è¿æ¥æ•°æ®åº“...")
        db = await get_mongodb_database()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")

        # æ­¥éª¤ 3: æ£€æŸ¥é›†åˆçŠ¶æ€
        print("ğŸ“ æ£€æŸ¥é›†åˆçŠ¶æ€...")
        old_info = await get_collection_info(db, "processed_results")
        new_info = await get_collection_info(db, "processed_results_new")

        if not old_info.get("exists"):
            print("â„¹ï¸  æ—§é›†åˆ processed_results ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")

            # ä½†ä»éœ€æ£€æŸ¥æ–°é›†åˆçš„ç´¢å¼•
            if new_info.get("exists"):
                print(f"\nâœ… processed_results_new å­˜åœ¨:")
                print(f"  - è®°å½•æ•°: {new_info['count']}")
                print(f"  - ç´¢å¼•æ•°: {new_info['index_count']}")

                if new_info['index_count'] < 5:
                    print("\nâš ï¸  ç´¢å¼•æ•°é‡ä¸è¶³ï¼Œéœ€è¦åˆ›å»ºç´¢å¼•")
                    await create_indexes_for_new_collection(db)

            return 0

        if not new_info.get("exists"):
            print("âŒ æ–°é›†åˆ processed_results_new ä¸å­˜åœ¨ï¼")
            print("â„¹ï¸  è¯·å…ˆç¡®ä¿æ–°é›†åˆå·²åˆ›å»ºå¹¶æœ‰æ•°æ®")
            return 1

        # æ˜¾ç¤ºé›†åˆå¯¹æ¯”
        print("\nğŸ“Š é›†åˆå¯¹æ¯”:")
        print(f"\næ—§é›†åˆ (processed_results):")
        print(f"  - è®°å½•æ•°: {old_info['count']}")
        print(f"  - æ•°æ®å¤§å°: {old_info['size'] / 1024:.2f} KB")
        print(f"  - ç´¢å¼•æ•°: {old_info['index_count']}")
        print(f"  - ç´¢å¼•: {', '.join(old_info['indexes'])}")

        print(f"\næ–°é›†åˆ (processed_results_new):")
        print(f"  - è®°å½•æ•°: {new_info['count']}")
        print(f"  - æ•°æ®å¤§å°: {new_info['size'] / 1024:.2f} KB")
        print(f"  - ç´¢å¼•æ•°: {new_info['index_count']}")
        print(f"  - ç´¢å¼•: {', '.join(new_info['indexes'])}")

        # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        if old_info['count'] != new_info['count']:
            print(f"\nâš ï¸ è­¦å‘Š: ä¸¤ä¸ªé›†åˆçš„è®°å½•æ•°ä¸ä¸€è‡´!")
            print(f"  æ—§é›†åˆ: {old_info['count']} æ¡")
            print(f"  æ–°é›†åˆ: {new_info['count']} æ¡")
            print(f"\nå»ºè®®å…ˆæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼Œå†ç»§ç»­åˆ é™¤æ“ä½œ")

            confirm = input("\næ˜¯å¦ä»è¦ç»§ç»­åˆ é™¤æ—§é›†åˆ? (yes/no): ").strip().lower()
            if confirm != "yes":
                print("\nâ¹ï¸  å–æ¶ˆåˆ é™¤æ“ä½œ")
                return 0

        # æ­¥éª¤ 4: å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯
        await backup_collection_stats(db, "processed_results")

        # æ­¥éª¤ 5: ç¡®è®¤åˆ é™¤
        print("\n" + "="*60)
        print("âš ï¸  å³å°†åˆ é™¤æ—§é›†åˆ processed_results")
        print("="*60)
        print(f"å°†åˆ é™¤ {old_info['count']} æ¡è®°å½•")
        print(f"æ•°æ®å¤§å°: {old_info['size'] / 1024:.2f} KB")
        print("\nâš ï¸ æ­¤æ“ä½œä¸å¯é€†ï¼")
        print("å»ºè®®:")
        print("  1. ç¡®ä¿å·²æœ‰æ•°æ®å¤‡ä»½")
        print("  2. ç¡®ä¿æœåŠ¡å·²åœæ­¢æˆ–ä»£ç å·²æŒ‡å‘æ–°é›†åˆ")
        print("  3. ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ° claudedocs/ ç›®å½•")

        confirm = input("\nç¡®è®¤åˆ é™¤æ—§é›†åˆ? è¯·è¾“å…¥é›†åˆå 'processed_results' ä»¥ç¡®è®¤: ").strip()

        if confirm != "processed_results":
            print("\nâ¹ï¸  å–æ¶ˆåˆ é™¤æ“ä½œ")
            return 0

        # æ­¥éª¤ 6: åˆ é™¤æ—§é›†åˆ
        print("\nğŸ“ åˆ é™¤æ—§é›†åˆ...")
        await db.drop_collection("processed_results")
        print("âœ… æ—§é›†åˆå·²åˆ é™¤")

        # æ­¥éª¤ 7: éªŒè¯åˆ é™¤
        print("\nğŸ“ éªŒè¯åˆ é™¤ç»“æœ...")
        verify_info = await get_collection_info(db, "processed_results")

        if verify_info.get("exists"):
            print("âŒ éªŒè¯å¤±è´¥: æ—§é›†åˆä»ç„¶å­˜åœ¨")
            return 1

        print("âœ… ç¡®è®¤æ—§é›†åˆå·²åˆ é™¤")

        # æ­¥éª¤ 8: ä¸ºæ–°é›†åˆåˆ›å»ºç´¢å¼•
        if new_info['index_count'] < 5:
            await create_indexes_for_new_collection(db)
        else:
            print("\nâœ… æ–°é›†åˆç´¢å¼•å·²å®Œæ•´ï¼Œæ— éœ€åˆ›å»º")

        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ‰ æ¸…ç†å®Œæˆ!")
        print("="*60)
        print(f"âœ… å·²åˆ é™¤æ—§é›†åˆ: processed_results ({old_info['count']} æ¡è®°å½•)")
        print(f"âœ… å½“å‰ä½¿ç”¨é›†åˆ: processed_results_new ({new_info['count']} æ¡è®°å½•)")
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²å¤‡ä»½åˆ° claudedocs/ ç›®å½•")
        print(f"\nâ„¹ï¸  ç°åœ¨å¯ä»¥é‡å¯æœåŠ¡è¿›è¡Œæµ‹è¯•\n")

        return 0

    except Exception as e:
        print(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
