#!/usr/bin/env python3
"""
ä½¿ç”¨ prompt å‚æ•°çˆ¬å– news é¡µé¢ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° news_results è¡¨

URL: https://www.thetibetpost.com/news
Prompt: "åªçˆ¬å–è¿‘æœŸä¸€ä¸ªæœˆçš„æ•°æ® å¿½ç•¥æ—§ç‰ˆå­˜æ¡£"
å­˜å‚¨ä½ç½®: news_results é›†åˆ
"""

import asyncio
import sys
import re
from datetime import datetime, timedelta
from typing import List

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter
from src.infrastructure.id_generator.snowflake import SnowflakeGenerator
from src.utils.logger import get_logger

logger = get_logger(__name__)

# åˆå§‹åŒ– ID ç”Ÿæˆå™¨
id_generator = SnowflakeGenerator()


def extract_dates_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸä¿¡æ¯"""
    if not text:
        return []

    patterns = [
        r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?',
        r'\d{1,2}[-/]\d{1,2}[-/]\d{4}',
        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}',
        r'\d{4}\.\d{1,2}\.\d{1,2}',
    ]

    dates = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        dates.extend(matches)

    return dates


def extract_year_from_path(text: str) -> List[str]:
    """ä»å›¾ç‰‡è·¯å¾„ä¸­æå–å¹´ä»½"""
    if not text:
        return []

    # åŒ¹é… Pics-YYYY æ ¼å¼
    pattern = r'Pics-(\d{4})'
    matches = re.findall(pattern, text)
    return matches


async def crawl_news_page():
    """çˆ¬å– news é¡µé¢å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""

    url = "https://www.thetibetpost.com/news"
    prompt = "åªçˆ¬å–è¿‘æœŸä¸€ä¸ªæœˆçš„æ•°æ® å¿½ç•¥æ—§ç‰ˆå­˜æ¡£"

    try:
        # è¿æ¥æ•°æ®åº“
        db = await get_mongodb_database()

        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– News é¡µé¢")
        logger.info("=" * 80)
        logger.info(f"\nç›®æ ‡ URL: {url}")
        logger.info(f"Prompt: {prompt}")
        logger.info(f"å­˜å‚¨ä½ç½®: news_results é›†åˆ\n")

        # åˆå§‹åŒ– Firecrawl é€‚é…å™¨
        adapter = FirecrawlAdapter()

        # æ‰§è¡Œçˆ¬å–
        logger.info("ğŸ”„ æ­£åœ¨çˆ¬å–...")
        start_time = datetime.now()

        results = await adapter.crawl(
            url=url,
            limit=10,
            max_depth=2,
            only_main_content=True,
            wait_for=1000,
            exclude_tags=['nav', 'footer', 'header'],
            prompt=prompt
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        logger.info(f"\nâœ… çˆ¬å–å®Œæˆ")
        logger.info(f"   è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"   ç»“æœæ•°: {len(results)} é¡µ\n")

        # åˆ†æå¹¶ä¿å­˜æ¯ä¸ªç»“æœ
        logger.info("ğŸ’¾ ä¿å­˜ç»“æœåˆ° news_results è¡¨...")
        logger.info("=" * 80)

        saved_count = 0
        analysis_summary = []

        for i, result in enumerate(results, 1):
            # å¤„ç† metadata
            metadata = result.metadata
            if hasattr(metadata, 'model_dump'):
                metadata_dict = metadata.model_dump()
            elif isinstance(metadata, dict):
                metadata_dict = metadata
            else:
                metadata_dict = {}

            # æå–åŸºæœ¬ä¿¡æ¯
            title = metadata_dict.get('title', f'æœªå‘½åé¡µé¢ {i}')
            page_url = metadata_dict.get('url') or metadata_dict.get('og_url') or ''
            content = result.markdown or result.content or ''

            logger.info(f"\n[{i}] åˆ†æé¡µé¢: {title[:60]}...")
            logger.info(f"    URL: {page_url[:70]}...")

            # ä»å†…å®¹ä¸­æå–æ—¥æœŸ
            dates_in_content = extract_dates_from_text(content[:3000])
            years_in_path = extract_year_from_path(content[:3000])

            logger.info(f"    å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"    æå–æ—¥æœŸ: {len(dates_in_content)} ä¸ª")
            if years_in_path:
                logger.info(f"    å›¾ç‰‡è·¯å¾„å¹´ä»½: {set(years_in_path)}")

            # åˆ¤æ–­æ˜¯å¦ä¸ºè¿‘æœŸå†…å®¹ï¼ˆåŒ…å« 2025 å¹´æ ‡è®°ï¼‰
            is_recent = '2025' in str(years_in_path) or '2025' in content[:1000]

            logger.info(f"    åˆ¤æ–­: {'âœ… 2025å¹´å†…å®¹' if is_recent else 'âš ï¸ æœªç¡®å®š'}")

            # ç”Ÿæˆå”¯ä¸€ ID
            result_id = id_generator.generate()

            # æ„å»ºä¿å­˜å¯¹è±¡
            news_result = {
                "_id": result_id,
                "url": page_url,
                "title": title,
                "markdown": result.markdown,
                "html": result.html,
                "metadata": metadata_dict,
                "extracted_dates": dates_in_content[:10],
                "path_years": list(set(years_in_path)),
                "is_recent_content": is_recent,
                "content_length": len(content),
                "crawl_config": {
                    "source_url": url,
                    "prompt": prompt,
                    "limit": 10,
                    "max_depth": 2
                },
                "created_at": datetime.now(),
                "status": "active"
            }

            # ä¿å­˜åˆ°æ•°æ®åº“
            await db.news_results.insert_one(news_result)
            saved_count += 1

            # è®°å½•åˆ†æç»“æœ
            analysis_summary.append({
                "title": title,
                "url": page_url,
                "is_recent": is_recent,
                "years": years_in_path,
                "dates": dates_in_content[:3]
            })

        logger.info(f"\nâœ… å·²ä¿å­˜ {saved_count} æ¡ç»“æœåˆ° news_results è¡¨")

        # ç”Ÿæˆæ€»ç»“
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š å†…å®¹åˆ†ææ€»ç»“")
        logger.info("=" * 80)

        recent_count = sum(1 for item in analysis_summary if item['is_recent'])

        logger.info(f"\næ€»è®¡çˆ¬å–: {len(analysis_summary)} é¡µ")
        logger.info(f"  âœ… 2025å¹´å†…å®¹: {recent_count} é¡µ ({recent_count/len(analysis_summary)*100:.1f}%)")
        logger.info(f"  âš ï¸  å…¶ä»–/æœªç¡®å®š: {len(analysis_summary) - recent_count} é¡µ")

        # æ˜¾ç¤º 2025 å¹´å†…å®¹è¯¦æƒ…
        if recent_count > 0:
            logger.info(f"\nâœ… åŒ…å« 2025 å¹´æ ‡è®°çš„é¡µé¢:")
            for item in analysis_summary:
                if item['is_recent']:
                    logger.info(f"\n  â€¢ {item['title'][:60]}...")
                    logger.info(f"    URL: {item['url'][:60]}...")
                    if item['years']:
                        logger.info(f"    å¹´ä»½: {set(item['years'])}")
                    if item['dates']:
                        logger.info(f"    æ—¥æœŸ: {item['dates']}")

        # Prompt æ•ˆæœè¯„ä¼°
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ Prompt å‚æ•°æ•ˆæœè¯„ä¼°")
        logger.info("=" * 80)
        logger.info(f"\nPrompt: \"{prompt}\"")

        if recent_count > 0:
            logger.info(f"\nâœ… å‘ç° {recent_count} é¡µåŒ…å« 2025 å¹´æ ‡è®°")
            logger.info(f"âœ… å æ¯” {recent_count/len(analysis_summary)*100:.1f}%")
            logger.info(f"âœ… Prompt å¯èƒ½èµ·åˆ°äº†å¼•å¯¼ä½œç”¨")
        else:
            logger.info(f"\nâš ï¸  æœªæ‰¾åˆ° 2025 å¹´æ ‡è®°")
            logger.info(f"âš ï¸  éœ€è¦è¿›ä¸€æ­¥åˆ†æå…·ä½“å†…å®¹")

        return True

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("\nğŸš€ å¼€å§‹æ‰§è¡Œï¼šçˆ¬å– News é¡µé¢ + æ•°æ®åº“ä¿å­˜ + å†…å®¹åˆ†æ\n")

    success = await crawl_news_page()

    logger.info("\n" + "=" * 80)
    if success:
        logger.info("âœ… ä»»åŠ¡å®Œæˆ")
        logger.info("\næŸ¥çœ‹ç»“æœ:")
        logger.info("  æ•°æ®åº“æŸ¥è¯¢: db.news_results.find().sort({created_at: -1}).limit(10)")
        logger.info("  Python æŸ¥è¯¢: python scripts/analyze_news_results.py")
    else:
        logger.info("âŒ ä»»åŠ¡å¤±è´¥")
    logger.info("=" * 80)

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
