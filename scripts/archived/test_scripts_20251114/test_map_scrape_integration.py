#!/usr/bin/env python3
"""
Map + Scrape é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯• Map + Scrape åŠŸèƒ½çš„ç«¯åˆ°ç«¯é›†æˆï¼š
1. åˆ›å»º MAP_SCRAPE_WEBSITE ç±»å‹ä»»åŠ¡
2. æ‰§è¡Œä»»åŠ¡è·å–ç»“æœ
3. éªŒè¯ç»“æœæ•°æ®å®Œæ•´æ€§
4. æµ‹è¯•æ—¶é—´è¿‡æ»¤åŠŸèƒ½
"""
import asyncio
import sys
from datetime import datetime, timedelta

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.core.domain.entities.search_task import SearchTask, TaskType, TaskStatus
from src.services.firecrawl.factory import ExecutorFactory
from src.services.firecrawl.credits_calculator import FirecrawlCreditsCalculator
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def test_basic_map_scrape():
    """æµ‹è¯•åŸºç¡€ Map + Scrape åŠŸèƒ½ï¼ˆä¸å¸¦æ—¶é—´è¿‡æ»¤ï¼‰"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•1: åŸºç¡€ Map + Scrape åŠŸèƒ½")
    logger.info("=" * 80)

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = SearchTask(
        name="Map + Scrape é›†æˆæµ‹è¯•",
        description="æµ‹è¯• Map API + Scrape API ç»„åˆåŠŸèƒ½",
        task_type=TaskType.MAP_SCRAPE_WEBSITE.value,
        crawl_url="https://firecrawl.dev",
        crawl_config={
            "search": "docs",  # åªæœç´¢åŒ…å«docsçš„URL
            "map_limit": 20,   # é™åˆ¶20ä¸ªURLç”¨äºæµ‹è¯•
            "max_concurrent_scrapes": 3,
            "scrape_delay": 0.5,
            "only_main_content": True,
            "exclude_tags": ["nav", "footer", "header", "aside"],
            "timeout": 90,
            "allow_partial_failure": True,
            "min_success_rate": 0.7
        },
        status=TaskStatus.ACTIVE
    )

    try:
        # åˆ›å»ºæ‰§è¡Œå™¨
        executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
        logger.info(f"âœ… æ‰§è¡Œå™¨åˆ›å»ºæˆåŠŸ: {type(executor).__name__}\n")

        # æ‰§è¡Œä»»åŠ¡
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œ Map + Scrape ä»»åŠ¡...")
        batch = await executor.execute(task)

        # éªŒè¯ç»“æœ
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š æ‰§è¡Œç»“æœç»Ÿè®¡")
        logger.info("=" * 80)
        logger.info(f"  ç»“æœæ•°é‡: {batch.total_count}")
        logger.info(f"  ç§¯åˆ†æ¶ˆè€—: {batch.credits_used}")
        logger.info(f"  æ‰§è¡Œæ—¶é—´: {batch.execution_time_ms}ms")
        logger.info(f"  æŸ¥è¯¢å†…å®¹: {batch.query}")

        # æ£€æŸ¥ç»“æœåˆ—è¡¨
        if batch.results:
            logger.info(f"\nâœ… è·å¾— {len(batch.results)} ä¸ªç»“æœ")
            logger.info("\nå‰3ä¸ªç»“æœç¤ºä¾‹:")
            for i, result in enumerate(batch.results[:3], 1):
                logger.info(f"\n  [{i}] {result.title}")
                logger.info(f"      URL: {result.url}")
                logger.info(f"      æ¥æº: {result.source}")
                logger.info(f"      å‘å¸ƒæ—¥æœŸ: {result.published_date}")
                logger.info(f"      Markdowné•¿åº¦: {len(result.markdown_content or '')} å­—ç¬¦")
        else:
            logger.warning("âš ï¸  ç»“æœåˆ—è¡¨ä¸ºç©º")

        logger.info("\nâœ… åŸºç¡€ Map + Scrape åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼\n")
        return True

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_map_scrape_with_time_filter():
    """æµ‹è¯•å¸¦æ—¶é—´è¿‡æ»¤çš„ Map + Scrape åŠŸèƒ½"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•2: Map + Scrape with æ—¶é—´è¿‡æ»¤")
    logger.info("=" * 80)

    # è®¾ç½®æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘7å¤©
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=7)

    logger.info(f"  æ—¶é—´èŒƒå›´: {start_date.date()} è‡³ {end_date.date()}")

    task = SearchTask(
        name="Map + Scrape æ—¶é—´è¿‡æ»¤æµ‹è¯•",
        description="æµ‹è¯•æ—¶é—´èŒƒå›´è¿‡æ»¤åŠŸèƒ½",
        task_type=TaskType.MAP_SCRAPE_WEBSITE.value,
        crawl_url="https://firecrawl.dev",
        crawl_config={
            "search": "blog",
            "map_limit": 20,
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "max_concurrent_scrapes": 3,
            "scrape_delay": 0.5,
            "only_main_content": True,
            "timeout": 90
        },
        status=TaskStatus.ACTIVE
    )

    try:
        # åˆ›å»ºæ‰§è¡Œå™¨å¹¶æ‰§è¡Œ
        executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
        batch = await executor.execute(task)

        # éªŒè¯æ—¶é—´è¿‡æ»¤
        logger.info("\nğŸ“Š æ—¶é—´è¿‡æ»¤ç»“æœ:")
        logger.info(f"  è¿‡æ»¤åç»“æœæ•°: {batch.total_count}")
        logger.info(f"  ç§¯åˆ†æ¶ˆè€—: {batch.credits_used}")

        if batch.results:
            # æ£€æŸ¥æ¯ä¸ªç»“æœçš„å‘å¸ƒæ—¥æœŸ
            within_range = 0
            outside_range = 0
            no_date = 0

            for result in batch.results:
                if result.published_date:
                    if start_date <= result.published_date <= end_date:
                        within_range += 1
                    else:
                        outside_range += 1
                else:
                    no_date += 1

            logger.info(f"\n  æ—¶é—´èŒƒå›´å†…: {within_range} ä¸ª")
            logger.info(f"  æ—¶é—´èŒƒå›´å¤–: {outside_range} ä¸ª")
            logger.info(f"  æ— å‘å¸ƒæ—¥æœŸ: {no_date} ä¸ª")

            if outside_range > 0:
                logger.warning(f"âš ï¸  å‘ç° {outside_range} ä¸ªæ—¶é—´èŒƒå›´å¤–çš„ç»“æœ")
        else:
            logger.info("  æœªæ‰¾åˆ°ç¬¦åˆæ—¶é—´èŒƒå›´çš„ç»“æœ")

        logger.info("\nâœ… æ—¶é—´è¿‡æ»¤åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼\n")
        return True

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_credits_calculation():
    """æµ‹è¯•ç§¯åˆ†è®¡ç®—åŠŸèƒ½"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•3: ç§¯åˆ†è®¡ç®—åŠŸèƒ½")
    logger.info("=" * 80)

    # æµ‹è¯•ä¼°ç®—
    estimate = FirecrawlCreditsCalculator.estimate_map_scrape_credits(
        estimated_urls=100,
        estimated_scraped=50
    )

    logger.info("ğŸ“Š ç§¯åˆ†ä¼°ç®—:")
    logger.info(f"  {estimate.description}")
    logger.info(f"  æ˜ç»†: {estimate.breakdown}")

    # æµ‹è¯•å®é™…è®¡ç®—
    actual_credits = FirecrawlCreditsCalculator.calculate_map_scrape_credits(
        urls_discovered=100,
        pages_scraped=50
    )

    logger.info(f"\nğŸ’° å®é™…ç§¯åˆ†è®¡ç®—:")
    logger.info(f"  å‘ç°URL: 100")
    logger.info(f"  çˆ¬å–é¡µé¢: 50")
    logger.info(f"  æ€»ç§¯åˆ†: {actual_credits}")

    # éªŒè¯
    expected = 1 + 50  # Map (1) + Scrape (50)
    if actual_credits == expected:
        logger.info(f"  âœ… è®¡ç®—æ­£ç¡®: {actual_credits} == {expected}")
    else:
        logger.error(f"  âŒ è®¡ç®—é”™è¯¯: {actual_credits} != {expected}")
        return False

    # å¯¹æ¯”æˆæœ¬ä¼˜åŠ¿
    logger.info(f"\nğŸ’¡ æˆæœ¬å¯¹æ¯”ï¼ˆçˆ¬å–50ä¸ªé¡µé¢ï¼‰:")
    crawl_credits = 50  # Crawl API: 50ç§¯åˆ†
    map_scrape_credits = actual_credits  # Map + Scrape: 51ç§¯åˆ†
    logger.info(f"  Crawl API: {crawl_credits} ç§¯åˆ†")
    logger.info(f"  Map + Scrape: {map_scrape_credits} ç§¯åˆ†")

    # å¦‚æœæœ‰æ—¶é—´è¿‡æ»¤ï¼Œæˆæœ¬ä¼˜åŠ¿æ›´æ˜æ˜¾
    filtered_pages = 20  # å‡è®¾è¿‡æ»¤ååªæœ‰20é¡µç¬¦åˆæ¡ä»¶
    map_scrape_filtered = 1 + filtered_pages
    savings = crawl_credits - map_scrape_filtered
    savings_pct = (savings / crawl_credits) * 100

    logger.info(f"\n  å¦‚æœæ—¶é—´è¿‡æ»¤ååªéœ€{filtered_pages}é¡µ:")
    logger.info(f"  Map + Scrape: {map_scrape_filtered} ç§¯åˆ†")
    logger.info(f"  èŠ‚çœ: {savings} ç§¯åˆ† ({savings_pct:.1f}%)")

    logger.info("\nâœ… ç§¯åˆ†è®¡ç®—åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼\n")
    return True


async def test_executor_factory():
    """æµ‹è¯•æ‰§è¡Œå™¨å·¥å‚æ³¨å†Œ"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•4: ExecutorFactory æ³¨å†Œ")
    logger.info("=" * 80)

    # æµ‹è¯•é€šè¿‡æšä¸¾åˆ›å»º
    try:
        executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
        logger.info(f"  âœ… é€šè¿‡TaskTypeåˆ›å»º: {type(executor).__name__}")
    except Exception as e:
        logger.error(f"  âŒ åˆ›å»ºå¤±è´¥: {e}")
        return False

    # æµ‹è¯•é€šè¿‡å­—ç¬¦ä¸²åˆ›å»º
    try:
        executor = ExecutorFactory.create_from_string("map_scrape_website")
        logger.info(f"  âœ… é€šè¿‡å­—ç¬¦ä¸²åˆ›å»º: {type(executor).__name__}")
    except Exception as e:
        logger.error(f"  âŒ åˆ›å»ºå¤±è´¥: {e}")
        return False

    # è·å–æ”¯æŒçš„ç±»å‹
    supported_types = ExecutorFactory.get_supported_types()
    logger.info(f"\nğŸ“‹ æ”¯æŒçš„ä»»åŠ¡ç±»å‹:")
    for task_type in supported_types:
        logger.info(f"  - {task_type.value}")

    # éªŒè¯ MAP_SCRAPE_WEBSITE åœ¨åˆ—è¡¨ä¸­
    if TaskType.MAP_SCRAPE_WEBSITE in supported_types:
        logger.info(f"\n  âœ… MAP_SCRAPE_WEBSITE å·²æ­£ç¡®æ³¨å†Œ")
    else:
        logger.error(f"\n  âŒ MAP_SCRAPE_WEBSITE æœªæ‰¾åˆ°")
        return False

    logger.info("\nâœ… ExecutorFactory æµ‹è¯•é€šè¿‡ï¼\n")
    return True


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("\n")
    logger.info("ğŸš€ å¼€å§‹ Map + Scrape é›†æˆæµ‹è¯•")
    logger.info("=" * 80)
    logger.info("\n")

    results = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ExecutorFactory æ³¨å†Œ", test_executor_factory),
        ("ç§¯åˆ†è®¡ç®—åŠŸèƒ½", test_credits_calculation),
        ("åŸºç¡€ Map + Scrape", test_basic_map_scrape),
        ("æ—¶é—´è¿‡æ»¤åŠŸèƒ½", test_map_scrape_with_time_filter),
    ]

    for test_name, test_func in tests:
        try:
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯• '{test_name}' å¼‚å¸¸: {e}")
            results.append((test_name, False))

    # æ€»ç»“
    logger.info("\n")
    logger.info("=" * 80)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("=" * 80)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"  {status}: {test_name}")

    logger.info(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    logger.info("=" * 80)

    return 0 if passed == total else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
