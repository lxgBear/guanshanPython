#!/usr/bin/env python3
"""
éªŒè¯ Map + Scrape åŠŸèƒ½æ˜¯å¦åœ¨ API ä¸­å¯ç”¨

æµ‹è¯•å†…å®¹:
1. éªŒè¯ TaskType æšä¸¾åŒ…å« MAP_SCRAPE_WEBSITE
2. æµ‹è¯•åˆ›å»º Map + Scrape ä»»åŠ¡
3. éªŒè¯ä»»åŠ¡é…ç½®æ­£ç¡®
"""
import sys
import requests
from datetime import datetime, timedelta

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.core.domain.entities.search_task import TaskType
from src.utils.logger import get_logger

logger = get_logger(__name__)

BASE_URL = "http://localhost:8000/api/v1"


def test_task_type_enum():
    """æµ‹è¯• TaskType æšä¸¾"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯• 1: TaskType æšä¸¾éªŒè¯")
    logger.info("=" * 80)

    # éªŒè¯æšä¸¾å­˜åœ¨
    assert hasattr(TaskType, 'MAP_SCRAPE_WEBSITE'), "âŒ MAP_SCRAPE_WEBSITE æšä¸¾ä¸å­˜åœ¨"

    # éªŒè¯æšä¸¾å€¼
    assert TaskType.MAP_SCRAPE_WEBSITE.value == "map_scrape_website", "âŒ æšä¸¾å€¼ä¸æ­£ç¡®"

    logger.info("âœ… TaskType.MAP_SCRAPE_WEBSITE æšä¸¾å­˜åœ¨")
    logger.info(f"âœ… æšä¸¾å€¼: {TaskType.MAP_SCRAPE_WEBSITE.value}")

    # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ç±»å‹
    logger.info("\nğŸ“‹ æ‰€æœ‰ä»»åŠ¡ç±»å‹:")
    for task_type in TaskType:
        logger.info(f"  - {task_type.value}")

    logger.info("\nâœ… æµ‹è¯• 1 é€šè¿‡ï¼\n")
    return True


def test_create_map_scrape_task():
    """æµ‹è¯•åˆ›å»º Map + Scrape ä»»åŠ¡"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯• 2: åˆ›å»º Map + Scrape ä»»åŠ¡")
    logger.info("=" * 80)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=30)

    task_data = {
        "name": "Map + Scrape API æµ‹è¯•ä»»åŠ¡",
        "description": "éªŒè¯ Map + Scrape åŠŸèƒ½å·²æ­£ç¡®é›†æˆåˆ° API",
        "crawl_url": "https://firecrawl.dev",
        "task_type": "map_scrape_website",
        "crawl_config": {
            "search": "docs",
            "map_limit": 50,
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "max_concurrent_scrapes": 3,
            "scrape_delay": 0.5,
            "only_main_content": True,
            "wait_for": 3000,
            "timeout": 90,
            "allow_partial_failure": True,
            "min_success_rate": 0.8
        },
        "schedule_interval": "DAILY",
        "is_active": False,  # ä¸å¯ç”¨ï¼Œåªæµ‹è¯•åˆ›å»º
        "execute_immediately": False
    }

    logger.info(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ°: {BASE_URL}/search-tasks")
    logger.info(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_data['task_type']}")
    logger.info(f"ğŸŒ çˆ¬å–URL: {task_data['crawl_url']}")

    # å‘é€è¯·æ±‚
    try:
        response = requests.post(
            f"{BASE_URL}/search-tasks",
            json=task_data,
            timeout=30
        )

        logger.info(f"\nğŸ“¥ å“åº”çŠ¶æ€ç : {response.status_code}")

        if response.status_code == 201:
            result = response.json()
            logger.info("âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸï¼")
            logger.info(f"\nğŸ“Š ä»»åŠ¡ä¿¡æ¯:")
            logger.info(f"  ID: {result.get('id')}")
            logger.info(f"  åç§°: {result.get('name')}")
            logger.info(f"  ç±»å‹: {result.get('task_type')}")
            logger.info(f"  æ¨¡å¼: {result.get('task_mode')}")
            logger.info(f"  çŠ¶æ€: {result.get('status')}")
            logger.info(f"  è°ƒåº¦é—´éš”: {result.get('schedule_display')}")

            # éªŒè¯é…ç½®
            crawl_config = result.get('crawl_config', {})
            logger.info(f"\nğŸ“‹ Map + Scrape é…ç½®:")
            logger.info(f"  search: {crawl_config.get('search')}")
            logger.info(f"  map_limit: {crawl_config.get('map_limit')}")
            logger.info(f"  max_concurrent_scrapes: {crawl_config.get('max_concurrent_scrapes')}")
            logger.info(f"  allow_partial_failure: {crawl_config.get('allow_partial_failure')}")

            logger.info("\nâœ… æµ‹è¯• 2 é€šè¿‡ï¼")

            # æ¸…ç†æµ‹è¯•ä»»åŠ¡
            task_id = result.get('id')
            if task_id:
                delete_response = requests.delete(f"{BASE_URL}/search-tasks/{task_id}")
                if delete_response.status_code == 200:
                    logger.info(f"ğŸ—‘ï¸  æµ‹è¯•ä»»åŠ¡å·²æ¸…ç† (ID: {task_id})")

            return True
        else:
            logger.error(f"âŒ ä»»åŠ¡åˆ›å»ºå¤±è´¥: {response.status_code}")
            logger.error(f"å“åº”å†…å®¹: {response.text}")
            return False

    except Exception as e:
        logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_executor_factory():
    """æµ‹è¯• ExecutorFactory æ˜¯å¦æ³¨å†Œäº† MapScrapeExecutor"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯• 3: ExecutorFactory æ³¨å†ŒéªŒè¯")
    logger.info("=" * 80)

    try:
        from src.services.firecrawl.factory import ExecutorFactory

        # æµ‹è¯•é€šè¿‡æšä¸¾åˆ›å»º
        executor = ExecutorFactory.create(TaskType.MAP_SCRAPE_WEBSITE)
        logger.info(f"âœ… é€šè¿‡ TaskType åˆ›å»ºæ‰§è¡Œå™¨: {type(executor).__name__}")

        # æµ‹è¯•é€šè¿‡å­—ç¬¦ä¸²åˆ›å»º
        executor = ExecutorFactory.create_from_string("map_scrape_website")
        logger.info(f"âœ… é€šè¿‡å­—ç¬¦ä¸²åˆ›å»ºæ‰§è¡Œå™¨: {type(executor).__name__}")

        # è·å–æ”¯æŒçš„ç±»å‹
        supported_types = ExecutorFactory.get_supported_types()
        logger.info(f"\nğŸ“‹ æ”¯æŒçš„ä»»åŠ¡ç±»å‹ ({len(supported_types)} ä¸ª):")
        for task_type in supported_types:
            logger.info(f"  - {task_type.value}")

        # éªŒè¯ MAP_SCRAPE_WEBSITE åœ¨åˆ—è¡¨ä¸­
        assert TaskType.MAP_SCRAPE_WEBSITE in supported_types, "âŒ MAP_SCRAPE_WEBSITE æœªæ³¨å†Œ"
        logger.info("\nâœ… MAP_SCRAPE_WEBSITE å·²æ­£ç¡®æ³¨å†Œ")

        logger.info("\nâœ… æµ‹è¯• 3 é€šè¿‡ï¼\n")
        return True

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ Map + Scrape API éªŒè¯æµ‹è¯•")
    logger.info("=" * 80)
    logger.info("\n")

    results = []

    # è¿è¡Œæµ‹è¯•
    tests = [
        ("TaskType æšä¸¾éªŒè¯", test_task_type_enum),
        ("ExecutorFactory æ³¨å†ŒéªŒè¯", test_executor_factory),
        ("åˆ›å»º Map + Scrape ä»»åŠ¡", test_create_map_scrape_task),
    ]

    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯• '{test_name}' å¼‚å¸¸: {e}")
            results.append((test_name, False))

    # æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("=" * 80)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"  {status}: {test_name}")

    logger.info(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Map + Scrape åŠŸèƒ½å·²æˆåŠŸé›†æˆåˆ° APIï¼")
    else:
        logger.error(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")

    logger.info("=" * 80)

    return 0 if passed == total else 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
