"""
æ‰§è¡Œæµ‹è¯•ä»»åŠ¡å¹¶éªŒè¯metadataå­—æ®µæå–ä¼˜åŒ–

ä»»åŠ¡1 (244376860577325056): URLçˆ¬å–ä»»åŠ¡ - 5é¡µï¼Œmax_depth=2
ä»»åŠ¡2 (244383648711102464): å…³é”®è¯æœç´¢ä»»åŠ¡ - 10æ¡ç»“æœ
"""

import asyncio
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.database.repositories import SearchTaskRepository, SearchResultRepository
from src.services.task_scheduler import TaskSchedulerService
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def execute_and_analyze_task(task_id: str, task_name: str):
    """æ‰§è¡Œä»»åŠ¡å¹¶åˆ†æç»“æœ"""

    print(f"\n{'='*80}")
    print(f"æ‰§è¡Œä»»åŠ¡: {task_name} (ID: {task_id})")
    print(f"{'='*80}")

    try:
        # æ‰§è¡Œä»»åŠ¡
        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡...")
        start_time = datetime.utcnow()

        # åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹å¹¶æ‰§è¡Œä»»åŠ¡
        scheduler = TaskSchedulerService()
        result = await scheduler.execute_task_now(task_id)

        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()

        if result:
            print(f"âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ (è€—æ—¶: {execution_time:.2f}ç§’)")
        else:
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’)")
            return False

        # åˆ†æç»“æœ
        print(f"\nğŸ“Š åˆ†ææ‰§è¡Œç»“æœ...")
        db = await get_mongodb_database()
        results_collection = db['search_results']

        # æŸ¥è¯¢æœ€æ–°çš„ç»“æœ
        cursor = results_collection.find({"task_id": task_id}).sort("created_at", -1).limit(20)
        results = await cursor.to_list(length=20)

        if not results:
            print(f"âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœ")
            return False

        print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡æœç´¢ç»“æœ")

        # åˆ†æå­—æ®µæå–æƒ…å†µ
        print(f"\n{'='*80}")
        print(f"å­—æ®µæå–åˆ†æ")
        print(f"{'='*80}")

        # ç»Ÿè®¡å­—æ®µ
        field_stats = {
            'title': {'count': 0, 'non_empty': 0},
            'url': {'count': 0, 'non_empty': 0},
            'published_date': {'count': 0, 'non_empty': 0},
            'author': {'count': 0, 'non_empty': 0},
            'language': {'count': 0, 'non_empty': 0},
            'article_tag': {'count': 0, 'non_empty': 0},
            'article_published_time': {'count': 0, 'non_empty': 0},
            'source_url': {'count': 0, 'non_empty': 0},
            'http_status_code': {'count': 0, 'non_empty': 0},
            'search_position': {'count': 0, 'non_empty': 0},
            'markdown_content': {'count': 0, 'non_empty': 0},
            'html_content': {'count': 0, 'non_empty': 0},
            'metadata': {'count': 0, 'non_empty': 0, 'size_bytes': []}
        }

        # åˆ†ææ¯æ¡è®°å½•
        for idx, result in enumerate(results[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡è¯¦æƒ…
            print(f"\nğŸ“„ è®°å½• #{idx}")
            print(f"   ID: {result.get('_id')}")
            print(f"   Title: {result.get('title', 'N/A')[:60]}...")
            print(f"   URL: {result.get('url', 'N/A')[:60]}...")
            print(f"   Source: {result.get('source', 'N/A')}")

            # æ£€æŸ¥æ¯ä¸ªå­—æ®µ
            for field in field_stats.keys():
                if field in result:
                    field_stats[field]['count'] += 1
                    value = result[field]

                    if field == 'metadata':
                        # ç‰¹æ®Šå¤„ç†metadataå­—æ®µ
                        if value:  # éç©º
                            field_stats[field]['non_empty'] += 1
                            import json
                            size = len(json.dumps(value))
                            field_stats[field]['size_bytes'].append(size)
                            print(f"   âš ï¸  metadata: å­˜åœ¨ (å¤§å°: {size} å­—èŠ‚)")
                        else:
                            print(f"   âœ… metadata: ç©º (å·²ä¼˜åŒ–)")
                    else:
                        # å…¶ä»–å­—æ®µ
                        if value is not None and value != '':
                            field_stats[field]['non_empty'] += 1

        # ç»Ÿè®¡æ‰€æœ‰è®°å½•
        for result in results:
            for field in field_stats.keys():
                if field in result:
                    value = result[field]
                    if field == 'metadata':
                        if value:
                            import json
                            size = len(json.dumps(value))
                            field_stats[field]['size_bytes'].append(size)

        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"\n{'='*80}")
        print(f"å­—æ®µæå–ç»Ÿè®¡ (å…± {len(results)} æ¡è®°å½•)")
        print(f"{'='*80}")

        print(f"\n{'å­—æ®µå':<25} {'å­˜åœ¨ç‡':<15} {'æœ‰å€¼ç‡':<15} {'çŠ¶æ€'}")
        print("-"*80)

        for field, stats in field_stats.items():
            if field == 'metadata':
                continue

            exist_rate = (stats['count'] / len(results)) * 100
            non_empty_rate = (stats['non_empty'] / len(results)) * 100 if stats['count'] > 0 else 0

            status = "âœ…" if non_empty_rate >= 80 else "âš ï¸" if non_empty_rate >= 50 else "âŒ"

            print(f"{field:<25} {exist_rate:>5.1f}% ({stats['count']}/{len(results)}) "
                  f"{non_empty_rate:>5.1f}% ({stats['non_empty']}/{len(results)}) {status}")

        # metadataå­—æ®µç‰¹æ®Šç»Ÿè®¡
        print(f"\n{'-'*80}")
        print("metadataå­—æ®µå­˜å‚¨æ£€æŸ¥:")
        metadata_stats = field_stats['metadata']

        if metadata_stats['non_empty'] == 0:
            print(f"   âœ… æ‰€æœ‰è®°å½•çš„metadataéƒ½ä¸ºç©º (v2.1.0ä¼˜åŒ–ç”Ÿæ•ˆ)")
        else:
            print(f"   âš ï¸  å‘ç° {metadata_stats['non_empty']}/{len(results)} æ¡è®°å½•ä»æœ‰metadata")
            if metadata_stats['size_bytes']:
                avg_size = sum(metadata_stats['size_bytes']) / len(metadata_stats['size_bytes'])
                total_size = sum(metadata_stats['size_bytes'])
                print(f"   å¹³å‡å¤§å°: {avg_size:.1f} å­—èŠ‚")
                print(f"   æ€»å¤§å°: {total_size} å­—èŠ‚ ({total_size/1024:.2f} KB)")

        return True

    except Exception as e:
        print(f"âŒ æ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""

    print("\n" + "="*80)
    print("æµ‹è¯•ä»»åŠ¡æ‰§è¡Œä¸éªŒè¯")
    print("="*80)

    # ä»»åŠ¡1ï¼šURLçˆ¬å–ä»»åŠ¡
    print("\n" + "="*80)
    print("æµ‹è¯•1: URLçˆ¬å–ä»»åŠ¡ (crawl_url)")
    print("="*80)
    success1 = await execute_and_analyze_task(
        "244376860577325056",
        "æ— å£°ä¹‹å£° - URLçˆ¬å–"
    )

    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    print("\nâ³ ç­‰å¾…5ç§’åæ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡...")
    await asyncio.sleep(5)

    # ä»»åŠ¡2ï¼šå…³é”®è¯æœç´¢ä»»åŠ¡
    print("\n" + "="*80)
    print("æµ‹è¯•2: å…³é”®è¯æœç´¢ä»»åŠ¡ (search_keyword)")
    print("="*80)
    success2 = await execute_and_analyze_task(
        "244383648711102464",
        "test1 - å…³é”®è¯æœç´¢"
    )

    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)
    print(f"URLçˆ¬å–ä»»åŠ¡: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
    print(f"å…³é”®è¯æœç´¢ä»»åŠ¡: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")
    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
