#!/usr/bin/env python3
"""
æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡ 244746288889929728 æµ‹è¯• prompt å‚æ•°

ä»»åŠ¡é…ç½®:
- ç›®æ ‡ç½‘ç«™: https://www.thetibetpost.com/
- çˆ¬å–é¡µæ•°: 10 é¡µ
- çˆ¬å–æ·±åº¦: 2
- Prompt: "åªçˆ¬å–è¿‘æœŸä¸€ä¸ªæœˆçš„æ•°æ® å¿½ç•¥æ—§ç‰ˆå­˜æ¡£"
"""

import asyncio
import sys
import json
from datetime import datetime

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def execute_task():
    """æ‰‹åŠ¨æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""

    task_id = "244746288889929728"

    try:
        # 1. æŸ¥è¯¢ä»»åŠ¡é…ç½®
        db = await get_mongodb_database()
        task = await db.search_tasks.find_one({"_id": task_id})

        if not task:
            logger.error(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return False

        logger.info("=" * 60)
        logger.info(f"ä»»åŠ¡ä¿¡æ¯:")
        logger.info(f"  ID: {task_id}")
        logger.info(f"  åç§°: {task.get('name')}")
        logger.info(f"  URL: {task.get('crawl_url')}")
        logger.info(f"  ç±»å‹: {task.get('task_type')}")
        logger.info("=" * 60)

        # 2. æå–é…ç½®
        crawl_config = task.get('crawl_config', {})
        url = task.get('crawl_url')

        logger.info(f"\nçˆ¬å–é…ç½®:")
        for key, value in crawl_config.items():
            logger.info(f"  {key}: {value}")

        # 3. åˆå§‹åŒ– Firecrawl é€‚é…å™¨
        logger.info(f"\nåˆå§‹åŒ– Firecrawl é€‚é…å™¨...")
        adapter = FirecrawlAdapter()

        # 4. å‡†å¤‡å‚æ•°
        exclude_tags = crawl_config.get('exclude_tags', ['nav', 'footer', 'header'])
        # å¤„ç†æ•°æ®åº“ä¸­å­˜å‚¨çš„å­—ç¬¦ä¸²æ ¼å¼
        if isinstance(exclude_tags, str):
            logger.warning(f"   exclude_tags æ˜¯å­—ç¬¦ä¸²æ ¼å¼: {exclude_tags}, ä½¿ç”¨é»˜è®¤å€¼")
            exclude_tags = ['nav', 'footer', 'header']

        # æ‰§è¡Œçˆ¬å–
        logger.info(f"\nğŸš€ å¼€å§‹çˆ¬å–...")
        logger.info(f"   ç›®æ ‡: {url}")
        logger.info(f"   Prompt: {crawl_config.get('prompt', 'æ— ')}")

        start_time = datetime.now()

        results = await adapter.crawl(
            url=url,
            limit=int(crawl_config.get('limit', 10)),
            max_depth=int(crawl_config.get('max_depth', 2)),
            only_main_content=crawl_config.get('only_main_content', True),
            wait_for=int(crawl_config.get('wait_for', 1000)),
            exclude_tags=exclude_tags,
            prompt=crawl_config.get('prompt')  # ä¼ é€’ prompt å‚æ•°
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # 5. åˆ†æç»“æœ
        logger.info(f"\nâœ… çˆ¬å–å®Œæˆ")
        logger.info(f"   è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"   ç»“æœæ•°: {len(results)} é¡µ")

        if results:
            logger.info(f"\nğŸ“Š ç»“æœé¢„è§ˆ:")
            for i, result in enumerate(results[:3], 1):
                logger.info(f"\n   [{i}] {result.url}")

                # å¤„ç† metadata (å¯èƒ½æ˜¯ dict æˆ– Pydantic å¯¹è±¡)
                metadata = result.metadata
                if hasattr(metadata, 'model_dump'):
                    # Pydantic å¯¹è±¡
                    metadata_dict = metadata.model_dump()
                elif isinstance(metadata, dict):
                    metadata_dict = metadata
                else:
                    metadata_dict = {}

                title = metadata_dict.get('title', 'æ— æ ‡é¢˜')
                logger.info(f"       æ ‡é¢˜: {title[:50]}...")

                # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                pub_time = metadata_dict.get('article_published_time') or \
                          metadata_dict.get('og:article:published_time') or \
                          metadata_dict.get('published_time')

                if pub_time:
                    logger.info(f"       å‘å¸ƒæ—¶é—´: {pub_time}")
                else:
                    logger.info(f"       å‘å¸ƒæ—¶é—´: æœªæ‰¾åˆ°")

                # æ˜¾ç¤ºå†…å®¹é•¿åº¦
                content_length = len(result.markdown or result.content or '')
                logger.info(f"       å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")

            if len(results) > 3:
                logger.info(f"\n   ... è¿˜æœ‰ {len(results) - 3} æ¡ç»“æœ")

        # 6. ä¿å­˜ç»“æœç¤ºä¾‹
        logger.info(f"\nğŸ’¾ ä¿å­˜ç»“æœç¤ºä¾‹åˆ°æ–‡ä»¶...")
        output_file = f"crawl_result_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        result_data = []
        for result in results:
            # å¤„ç† metadata
            metadata = result.metadata
            if hasattr(metadata, 'model_dump'):
                metadata_dict = metadata.model_dump()
            elif isinstance(metadata, dict):
                metadata_dict = metadata
            else:
                metadata_dict = {}

            result_data.append({
                "url": result.url,
                "title": metadata_dict.get('title', ''),
                "published_time": metadata_dict.get('article_published_time') or
                                 metadata_dict.get('og:article:published_time') or
                                 metadata_dict.get('published_time'),
                "content_length": len(result.markdown or result.content or ''),
                "metadata_keys": list(metadata_dict.keys())
            })

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)

        logger.info(f"   å·²ä¿å­˜åˆ°: {output_file}")

        # 7. åˆ†ææ—¶é—´åˆ†å¸ƒ
        logger.info(f"\nğŸ“… æ—¶é—´åˆ†å¸ƒåˆ†æ:")
        with_time = 0
        without_time = 0

        for result in results:
            metadata = result.metadata
            if hasattr(metadata, 'model_dump'):
                metadata_dict = metadata.model_dump()
            elif isinstance(metadata, dict):
                metadata_dict = metadata
            else:
                metadata_dict = {}

            pub_time = metadata_dict.get('article_published_time') or \
                      metadata_dict.get('og:article:published_time') or \
                      metadata_dict.get('published_time')

            if pub_time:
                with_time += 1
            else:
                without_time += 1

        logger.info(f"   åŒ…å«å‘å¸ƒæ—¶é—´: {with_time} é¡µ")
        logger.info(f"   æ— å‘å¸ƒæ—¶é—´: {without_time} é¡µ")

        return True

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• Firecrawl v2 API prompt å‚æ•° - æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡")
    logger.info("=" * 60 + "\n")

    success = await execute_task()

    logger.info("\n" + "=" * 60)
    if success:
        logger.info("âœ… æµ‹è¯•å®Œæˆ")
        logger.info("\nåˆ†æ prompt å‚æ•°æ•ˆæœ:")
        logger.info("1. æ£€æŸ¥ç»“æœæ–‡ä»¶ä¸­çš„å‘å¸ƒæ—¶é—´åˆ†å¸ƒ")
        logger.info("2. ç¡®è®¤æ˜¯å¦ä¸»è¦åŒ…å«è¿‘æœŸä¸€ä¸ªæœˆçš„æ•°æ®")
        logger.info("3. éªŒè¯æ˜¯å¦è¿‡æ»¤äº†æ—§ç‰ˆå­˜æ¡£é¡µé¢")
    else:
        logger.info("âŒ æµ‹è¯•å¤±è´¥")
    logger.info("=" * 60)

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
