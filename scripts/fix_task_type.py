"""
ä¿®å¤ä»»åŠ¡ç±»å‹

ä»»åŠ¡ 244376860577325056 çš„ task_type åº”è¯¥æ˜¯ crawl_url è€Œä¸æ˜¯ search_keyword
"""

import asyncio
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def fix_task_type():
    """ä¿®å¤ä»»åŠ¡ç±»å‹"""

    print(f"\n{'='*80}")
    print(f"ä¿®å¤ä»»åŠ¡ç±»å‹")
    print(f"{'='*80}")

    try:
        db = await get_mongodb_database()
        tasks_collection = db['search_tasks']

        task_id = "244376860577325056"

        # æŸ¥è¯¢ä»»åŠ¡
        task = await tasks_collection.find_one({"_id": task_id})

        if not task:
            print(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
            return False

        print(f"\nğŸ“‹ ä»»åŠ¡ä¿¡æ¯:")
        print(f"   åç§°: {task.get('name')}")
        print(f"   å½“å‰ç±»å‹: {task.get('task_type')}")
        print(f"   URL: {task.get('crawl_url')}")

        print(f"\nğŸ”§ ä¿®å¤æ“ä½œ:")
        print(f"   task_type: {task.get('task_type')} â†’ crawl_url")

        # æ›´æ–°ä»»åŠ¡ç±»å‹
        result = await tasks_collection.update_one(
            {"_id": task_id},
            {
                "$set": {
                    "task_type": "crawl_url",
                    "updated_at": datetime.utcnow()
                }
            }
        )

        if result.modified_count > 0:
            print(f"\nâœ… ä»»åŠ¡ç±»å‹ä¿®å¤æˆåŠŸ")
            return True
        else:
            print(f"\nâš ï¸ ä»»åŠ¡æœªä¿®æ”¹ï¼ˆå¯èƒ½å·²ç»æ˜¯æ­£ç¡®ç±»å‹ï¼‰")
            return False

    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    await fix_task_type()


if __name__ == "__main__":
    asyncio.run(main())
