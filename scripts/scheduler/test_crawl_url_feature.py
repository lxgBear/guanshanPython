#!/usr/bin/env python3
"""æµ‹è¯• crawl_url å­—æ®µå’Œä¼˜å…ˆçº§é€»è¾‘"""

import asyncio
import httpx
import json

BASE_URL = "http://localhost:8000/api/v1"
INTERNAL_URL = "http://localhost:8000/api/v1/internal"


async def test_create_with_crawl_url():
    """æµ‹è¯•åœºæ™¯1ï¼šåˆ›å»ºå¸¦ crawl_url çš„ä»»åŠ¡"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•åœºæ™¯1: åˆ›å»ºå¸¦ crawl_url çš„ä»»åŠ¡")
    print("=" * 60)

    task_data = {
        "name": "æµ‹è¯•ä»»åŠ¡ - ç½‘å€çˆ¬å–",
        "description": "æµ‹è¯• crawl_url ä¼˜å…ˆçº§é€»è¾‘",
        "query": "dummy query",  # è¿™ä¸ªä¼šè¢«å¿½ç•¥
        "crawl_url": "https://www.anthropic.com",  # è¿™ä¸ªä¼šè¢«ä¼˜å…ˆä½¿ç”¨
        "search_config": {
            "wait_for": 2000,
            "exclude_tags": ["nav", "footer"]
        },
        "schedule_interval": "DAILY",
        "is_active": True
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(f"{BASE_URL}/search-tasks", json=task_data)

        if response.status_code == 201:
            result = response.json()
            print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ!")
            print(f"   ä»»åŠ¡ID: {result['id']}")
            print(f"   ä»»åŠ¡åç§°: {result['name']}")
            print(f"   çˆ¬å–URL: {result['crawl_url']}")
            print(f"   æŸ¥è¯¢å…³é”®è¯: {result['query']}")

            assert result['crawl_url'] == "https://www.anthropic.com", "crawl_url åº”è¯¥æ­£ç¡®ä¿å­˜"
            print(f"âœ… éªŒè¯é€šè¿‡: crawl_url å­—æ®µæ­£ç¡®ä¿å­˜")
            return result['id']
        else:
            print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯: {response.text}")
            return None


async def test_execute_crawl_task(task_id: str):
    """æµ‹è¯•åœºæ™¯2ï¼šæ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ŒéªŒè¯ä¼˜å…ˆçº§é€»è¾‘"""
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•åœºæ™¯2: æ‰§è¡Œä»»åŠ¡ {task_id}ï¼ŒéªŒè¯ä½¿ç”¨çˆ¬å–æ¨¡å¼")
    print("=" * 60)

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(f"{INTERNAL_URL}/search-tasks/{task_id}/execute")

        if response.status_code == 200:
            result = response.json()
            print(f"âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ!")
            print(f"   ä»»åŠ¡ID: {result['task_id']}")
            print(f"   ä»»åŠ¡åç§°: {result['task_name']}")
            print(f"   æ‰§è¡Œæ—¶é—´: {result['execution_time_ms']}ms")
            print(f"   ç»“æœæ•°é‡: {result['total_results']}")
            print(f"   æ¶ˆè€—ç§¯åˆ†: {result['credits_used']}")

            if result['results_preview']:
                print(f"\n   ç»“æœé¢„è§ˆ:")
                for idx, preview in enumerate(result['results_preview'][:1]):
                    print(f"      ç»“æœ {idx+1}:")
                    print(f"         æ ‡é¢˜: {preview.get('title', 'N/A')[:50]}...")
                    print(f"         URL: {preview.get('url', 'N/A')}")
                    print(f"         æ¥æº: {preview.get('source', 'N/A')}")

            # éªŒè¯ä½¿ç”¨äº†çˆ¬å–æ¨¡å¼ï¼ˆsource åº”è¯¥æ˜¯ "crawl"ï¼‰
            if result['results_preview'] and result['results_preview'][0].get('source') == 'crawl':
                print(f"\nâœ… éªŒè¯é€šè¿‡: ä½¿ç”¨äº†ç½‘å€çˆ¬å–æ¨¡å¼ï¼ˆsource=crawlï¼‰")
            else:
                print(f"\nâš ï¸  æ³¨æ„: ç»“æœæ¥æºä¸æ˜¯ 'crawl'")

            return result['success']
        else:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯: {response.text}")
            return False


async def test_create_without_crawl_url():
    """æµ‹è¯•åœºæ™¯3ï¼šåˆ›å»ºä¸å¸¦ crawl_url çš„ä»»åŠ¡ï¼ˆåº”è¯¥ä½¿ç”¨å…³é”®è¯æœç´¢ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•åœºæ™¯3: åˆ›å»ºä¸å¸¦ crawl_url çš„ä»»åŠ¡")
    print("=" * 60)

    task_data = {
        "name": "æµ‹è¯•ä»»åŠ¡ - å…³é”®è¯æœç´¢",
        "description": "éªŒè¯æœªæä¾› crawl_url æ—¶ä½¿ç”¨æœç´¢æ¨¡å¼",
        "query": "AI news",  # åº”è¯¥ä½¿ç”¨è¿™ä¸ªå…³é”®è¯æœç´¢
        "search_config": {
            "limit": 5,
            "language": "en"
        },
        "schedule_interval": "DAILY",
        "is_active": False  # ä¸æ‰§è¡Œï¼Œåªæµ‹è¯•åˆ›å»º
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(f"{BASE_URL}/search-tasks", json=task_data)

        if response.status_code == 201:
            result = response.json()
            print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ!")
            print(f"   ä»»åŠ¡ID: {result['id']}")
            print(f"   ä»»åŠ¡åç§°: {result['name']}")
            print(f"   çˆ¬å–URL: {result.get('crawl_url', 'None')}")
            print(f"   æŸ¥è¯¢å…³é”®è¯: {result['query']}")

            assert result.get('crawl_url') is None, "crawl_url åº”è¯¥ä¸º None"
            assert result['query'] == "AI news", "query åº”è¯¥æ­£ç¡®ä¿å­˜"
            print(f"âœ… éªŒè¯é€šè¿‡: æœªæä¾› crawl_urlï¼Œquery æ­£å¸¸ä¿å­˜")
            return result['id']
        else:
            print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯: {response.text}")
            return None


async def test_update_crawl_url(task_id: str):
    """æµ‹è¯•åœºæ™¯4ï¼šæ›´æ–°ä»»åŠ¡çš„ crawl_url"""
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•åœºæ™¯4: æ›´æ–°ä»»åŠ¡ {task_id} çš„ crawl_url")
    print("=" * 60)

    update_data = {
        "crawl_url": "https://www.example.com"
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.put(f"{BASE_URL}/search-tasks/{task_id}", json=update_data)

        if response.status_code == 200:
            result = response.json()
            print(f"âœ… ä»»åŠ¡æ›´æ–°æˆåŠŸ!")
            print(f"   ä»»åŠ¡ID: {result['id']}")
            print(f"   æ›´æ–°åçš„ crawl_url: {result['crawl_url']}")

            assert result['crawl_url'] == "https://www.example.com", "crawl_url åº”è¯¥æ›´æ–°ä¸ºæ–°å€¼"
            print(f"âœ… éªŒè¯é€šè¿‡: crawl_url æ›´æ–°æˆåŠŸ")
            return True
        else:
            print(f"âŒ æ›´æ–°å¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯: {response.text}")
            return False


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "=" * 60)
    print("ğŸ§ª crawl_url å­—æ®µå’Œä¼˜å…ˆçº§é€»è¾‘æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•1: åˆ›å»ºå¸¦ crawl_url çš„ä»»åŠ¡
    task_id_1 = await test_create_with_crawl_url()
    if not task_id_1:
        print("\nâŒ æµ‹è¯•1å¤±è´¥ï¼Œä¸­æ­¢æµ‹è¯•")
        return

    await asyncio.sleep(1)

    # æµ‹è¯•2: æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ˆéªŒè¯ä¼˜å…ˆçº§é€»è¾‘ï¼‰
    if task_id_1:
        success = await test_execute_crawl_task(task_id_1)
        if not success:
            print("\nâš ï¸ æµ‹è¯•2å¤±è´¥ï¼Œä½†ç»§ç»­åç»­æµ‹è¯•")

    await asyncio.sleep(1)

    # æµ‹è¯•3: åˆ›å»ºä¸å¸¦ crawl_url çš„ä»»åŠ¡
    task_id_3 = await test_create_without_crawl_url()
    if not task_id_3:
        print("\nâš ï¸ æµ‹è¯•3å¤±è´¥ï¼Œä½†ç»§ç»­åç»­æµ‹è¯•")

    await asyncio.sleep(1)

    # æµ‹è¯•4: æ›´æ–° crawl_url
    if task_id_1:
        success = await test_update_crawl_url(task_id_1)
        if not success:
            print("\nâš ï¸ æµ‹è¯•4å¤±è´¥")

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
