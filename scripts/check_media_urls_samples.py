"""
æ£€æŸ¥ media_urls å­—æ®µçš„æ ·æœ¬æ•°æ®

äº†è§£ media_urls çš„å®é™…å†…å®¹å’Œæ•°æ®ç±»å‹
"""

import asyncio
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def check_media_urls_samples():
    """æ£€æŸ¥ media_urls å­—æ®µçš„æ ·æœ¬"""

    print(f"\n{'='*80}")
    print(f"media_urls å­—æ®µæ ·æœ¬æ•°æ®åˆ†æ")
    print(f"{'='*80}")

    try:
        db = await get_mongodb_database()
        collection = db['news_results']

        # æŸ¥è¯¢æœ‰ media_urls çš„è®°å½•
        cursor = collection.find(
            {"news_results.media_urls": {"$exists": True}}
        ).limit(5)

        results = await cursor.to_list(length=5)

        if not results:
            print(f"âŒ æœªæ‰¾åˆ°åŒ…å« media_urls çš„è®°å½•")
            return

        print(f"âœ… æ‰¾åˆ° {len(results)} æ¡åŒ…å« media_urls çš„è®°å½•\n")

        for idx, result in enumerate(results, 1):
            news_results = result.get('news_results', {})
            media_urls = news_results.get('media_urls', [])

            print(f"{'='*80}")
            print(f"æ ·æœ¬ #{idx}")
            print(f"{'='*80}")
            print(f"ID: {result.get('_id')}")
            print(f"Title: {news_results.get('title', 'N/A')[:60]}...")
            print(f"\nmedia_urls å­—æ®µ:")
            print(f"  ç±»å‹: {type(media_urls)}")
            print(f"  é•¿åº¦: {len(media_urls) if isinstance(media_urls, list) else 'N/A'}")

            if isinstance(media_urls, list):
                print(f"\n  å†…å®¹:")
                for i, url in enumerate(media_urls[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                    url_str = str(url)[:100]
                    print(f"    [{i}] {url_str}")
                    print(f"        ç±»å‹: {type(url).__name__}")

                if len(media_urls) > 10:
                    print(f"    ... è¿˜æœ‰ {len(media_urls) - 10} ä¸ªURL")
            else:
                print(f"  âš ï¸  ä¸æ˜¯åˆ—è¡¨ç±»å‹")

            print()

        # ç»Ÿè®¡åˆ†æ
        print(f"{'='*80}")
        print(f"ğŸ“Š ç»Ÿè®¡åˆ†æ")
        print(f"{'='*80}")

        all_urls = []
        for result in results:
            news_results = result.get('news_results', {})
            media_urls = news_results.get('media_urls', [])
            if isinstance(media_urls, list):
                all_urls.extend(media_urls)

        print(f"\næ€»URLæ•°: {len(all_urls)}")
        print(f"å¹³å‡æ¯æ¡è®°å½•URLæ•°: {len(all_urls) / len(results):.1f}")

        # URLç±»å‹ç»Ÿè®¡
        if all_urls:
            url_types = {}
            for url in all_urls[:20]:  # åˆ†æå‰20ä¸ª
                if isinstance(url, str):
                    # æå–æ–‡ä»¶æ‰©å±•å
                    if '.' in url:
                        ext = url.split('.')[-1].split('?')[0].split('#')[0].lower()
                        url_types[ext] = url_types.get(ext, 0) + 1

            if url_types:
                print(f"\næ–‡ä»¶ç±»å‹åˆ†å¸ƒï¼ˆå‰20ä¸ªURLï¼‰:")
                for ext, count in sorted(url_types.items(), key=lambda x: -x[1]):
                    print(f"  - .{ext}: {count}")

    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ä¸»å‡½æ•°"""
    await check_media_urls_samples()


if __name__ == "__main__":
    asyncio.run(main())
