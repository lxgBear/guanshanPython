#!/usr/bin/env python3
"""
é‡æ–°æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼Œä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¹¶åˆ†æå…·ä½“å†…å®¹åˆ¤æ–­æ—¶é—´è¿‡æ»¤æ•ˆæœ

åŠŸèƒ½ï¼š
1. æ‰§è¡Œå¸¦ prompt çš„çˆ¬å–ä»»åŠ¡
2. å°†ç»“æœä¿å­˜åˆ° processed_results é›†åˆ
3. æå–å†…å®¹ä¸­çš„æ—¶é—´ä¿¡æ¯
4. åˆ†æå†…å®¹åˆ¤æ–­æ˜¯å¦ä¸ºè¿‘æœŸæ•°æ®
"""

import asyncio
import sys
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any

sys.path.insert(0, '/Users/lanxionggao/Documents/guanshanPython')

from src.infrastructure.database.connection import get_mongodb_database
from src.infrastructure.crawlers.firecrawl_adapter import FirecrawlAdapter
from src.utils.logger import get_logger
from src.infrastructure.id_generator.snowflake import SnowflakeGenerator

logger = get_logger(__name__)

# åˆå§‹åŒ– ID ç”Ÿæˆå™¨
id_generator = SnowflakeGenerator()


def extract_dates_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸä¿¡æ¯"""
    if not text:
        return []

    # åŒ¹é…å„ç§æ—¥æœŸæ ¼å¼
    patterns = [
        r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?',  # 2025-11-06, 2025å¹´11æœˆ6æ—¥
        r'\d{1,2}[-/]\d{1,2}[-/]\d{4}',  # 11/06/2025
        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}',  # November 6, 2025
        r'\d{4}\.\d{1,2}\.\d{1,2}',  # 2025.11.06
    ]

    dates = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        dates.extend(matches)

    return dates


def is_recent_date(date_str: str, days: int = 30) -> bool:
    """åˆ¤æ–­æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘Nå¤©å†…"""
    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼è§£æ
    formats = [
        '%Y-%m-%d', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥',
        '%m/%d/%Y', '%d/%m/%Y',
        '%Y.%m.%d',
        '%B %d, %Y', '%b %d, %Y'
    ]

    for fmt in formats:
        try:
            parsed_date = datetime.strptime(date_str.strip(), fmt)
            cutoff_date = datetime.now() - timedelta(days=days)
            return parsed_date >= cutoff_date
        except ValueError:
            continue

    return False


async def crawl_and_save():
    """æ‰§è¡Œçˆ¬å–å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""

    task_id = "244746288889929728"

    try:
        # 1. è¿æ¥æ•°æ®åº“
        db = await get_mongodb_database()

        # 2. æŸ¥è¯¢ä»»åŠ¡é…ç½®
        task = await db.search_tasks.find_one({"_id": task_id})
        if not task:
            logger.error(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return False

        logger.info("=" * 60)
        logger.info(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯:")
        logger.info(f"  ID: {task_id}")
        logger.info(f"  åç§°: {task.get('name')}")
        logger.info(f"  URL: {task.get('crawl_url')}")
        logger.info("=" * 60)

        # 3. æå–é…ç½®
        crawl_config = task.get('crawl_config', {})
        url = task.get('crawl_url')

        # å¤„ç† exclude_tags
        exclude_tags = crawl_config.get('exclude_tags', ['nav', 'footer', 'header'])
        if isinstance(exclude_tags, str):
            exclude_tags = ['nav', 'footer', 'header']

        logger.info(f"\nğŸ”§ çˆ¬å–é…ç½®:")
        logger.info(f"  limit: {crawl_config.get('limit')}")
        logger.info(f"  max_depth: {crawl_config.get('max_depth')}")
        logger.info(f"  prompt: {crawl_config.get('prompt')}")

        # 4. æ‰§è¡Œçˆ¬å–
        logger.info(f"\nğŸš€ å¼€å§‹çˆ¬å–...")
        adapter = FirecrawlAdapter()

        start_time = datetime.now()

        results = await adapter.crawl(
            url=url,
            limit=int(crawl_config.get('limit', 10)),
            max_depth=int(crawl_config.get('max_depth', 2)),
            only_main_content=crawl_config.get('only_main_content', True),
            wait_for=int(crawl_config.get('wait_for', 1000)),
            exclude_tags=exclude_tags,
            prompt=crawl_config.get('prompt')
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        logger.info(f"\nâœ… çˆ¬å–å®Œæˆ")
        logger.info(f"  è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"  ç»“æœæ•°: {len(results)} é¡µ")

        # 5. åˆ†æå¹¶ä¿å­˜æ¯ä¸ªç»“æœ
        logger.info(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“...")

        saved_count = 0
        content_analysis = []

        for i, result in enumerate(results, 1):
            # å¤„ç† metadata
            metadata = result.metadata
            if hasattr(metadata, 'model_dump'):
                metadata_dict = metadata.model_dump()
            elif isinstance(metadata, dict):
                metadata_dict = metadata
            else:
                metadata_dict = {}

            # æå–å†…å®¹
            content = result.markdown or result.content or ''
            title = metadata_dict.get('title', f'æœªå‘½åé¡µé¢ {i}')
            page_url = metadata_dict.get('url') or metadata_dict.get('og_url') or ''

            logger.info(f"\n  [{i}] åˆ†æé¡µé¢: {title[:50]}...")

            # ä»å†…å®¹ä¸­æå–æ—¥æœŸ
            dates_in_content = extract_dates_from_text(content[:5000])  # åªåˆ†æå‰5000å­—ç¬¦
            dates_in_title = extract_dates_from_text(title)
            all_dates = dates_in_title + dates_in_content

            # åˆ¤æ–­æ˜¯å¦ä¸ºè¿‘æœŸå†…å®¹
            is_recent = False
            recent_dates = []
            old_dates = []

            for date_str in all_dates[:10]:  # åªæ£€æŸ¥å‰10ä¸ªæ—¥æœŸ
                if is_recent_date(date_str, days=30):
                    is_recent = True
                    recent_dates.append(date_str)
                else:
                    old_dates.append(date_str)

            logger.info(f"      URL: {page_url[:60]}...")
            logger.info(f"      å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"      æå–æ—¥æœŸæ•°: {len(all_dates)}")
            if recent_dates:
                logger.info(f"      è¿‘æœŸæ—¥æœŸ: {recent_dates[:3]}")
            if old_dates:
                logger.info(f"      æ—§æ—¥æœŸ: {old_dates[:3]}")
            logger.info(f"      åˆ¤æ–­: {'âœ… è¿‘æœŸå†…å®¹' if is_recent else 'âš ï¸ æœªç¡®å®šæˆ–æ—§å†…å®¹'}")

            # ä¿å­˜åˆ°æ•°æ®åº“
            result_id = id_generator.generate()

            processed_result = {
                "_id": result_id,
                "task_id": task_id,
                "url": page_url,
                "title": title,
                "markdown_content": result.markdown,
                "content": result.content,
                "html": result.html,
                "metadata": metadata_dict,
                "extracted_dates": all_dates[:10],
                "is_recent_content": is_recent,
                "recent_dates": recent_dates,
                "old_dates": old_dates,
                "content_length": len(content),
                "created_at": datetime.now(),
                "status": "active"
            }

            await db.processed_results.insert_one(processed_result)
            saved_count += 1

            # è®°å½•åˆ†æç»“æœ
            content_analysis.append({
                "title": title,
                "url": page_url,
                "is_recent": is_recent,
                "dates_found": len(all_dates),
                "recent_dates": recent_dates,
                "old_dates": old_dates
            })

        logger.info(f"\nâœ… å·²ä¿å­˜ {saved_count} æ¡ç»“æœåˆ°æ•°æ®åº“")

        # 6. ç”Ÿæˆæ€»ä½“åˆ†æ
        logger.info(f"\n" + "=" * 60)
        logger.info(f"ğŸ“Š å†…å®¹åˆ†ææ€»ç»“")
        logger.info(f"=" * 60)

        recent_count = sum(1 for item in content_analysis if item['is_recent'])
        unknown_count = len(content_analysis) - recent_count

        logger.info(f"\næ€»è®¡çˆ¬å–: {len(content_analysis)} é¡µ")
        logger.info(f"  âœ… è¿‘æœŸå†…å®¹: {recent_count} é¡µ ({recent_count/len(content_analysis)*100:.1f}%)")
        logger.info(f"  âš ï¸  æœªç¡®å®š/æ—§å†…å®¹: {unknown_count} é¡µ ({unknown_count/len(content_analysis)*100:.1f}%)")

        # æ˜¾ç¤ºè¿‘æœŸå†…å®¹è¯¦æƒ…
        if recent_count > 0:
            logger.info(f"\nâœ… è¿‘æœŸå†…å®¹è¯¦æƒ…:")
            for item in content_analysis:
                if item['is_recent']:
                    logger.info(f"\n  - {item['title'][:60]}...")
                    logger.info(f"    URL: {item['url'][:60]}...")
                    logger.info(f"    è¿‘æœŸæ—¥æœŸ: {item['recent_dates'][:3]}")

        # æ˜¾ç¤ºæœªç¡®å®šå†…å®¹
        if unknown_count > 0:
            logger.info(f"\nâš ï¸  æœªç¡®å®š/æ—§å†…å®¹:")
            for item in content_analysis:
                if not item['is_recent']:
                    logger.info(f"\n  - {item['title'][:60]}...")
                    logger.info(f"    URL: {item['url'][:60]}...")
                    if item['old_dates']:
                        logger.info(f"    æ—§æ—¥æœŸ: {item['old_dates'][:3]}")
                    else:
                        logger.info(f"    æœªæ‰¾åˆ°æ—¥æœŸä¿¡æ¯")

        # 7. Prompt æ•ˆæœè¯„ä¼°
        logger.info(f"\n" + "=" * 60)
        logger.info(f"ğŸ¯ Prompt å‚æ•°æ•ˆæœè¯„ä¼°")
        logger.info(f"=" * 60)
        logger.info(f"\nPrompt: \"{crawl_config.get('prompt')}\"")
        logger.info(f"\nè¯„ä¼°ç»“æœ:")

        if recent_count > 0:
            logger.info(f"  âœ… æ‰¾åˆ° {recent_count} é¡µè¿‘æœŸå†…å®¹")
            logger.info(f"  âœ… Prompt å¯èƒ½èµ·åˆ°äº†è¿‡æ»¤ä½œç”¨")
        else:
            logger.info(f"  âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®çš„è¿‘æœŸæ—¥æœŸä¿¡æ¯")
            logger.info(f"  âš ï¸  å¯èƒ½åŸå› :")
            logger.info(f"     1. ç½‘ç«™å†…å®¹ç¡®å®æ˜¯è¿‘æœŸçš„ï¼Œä½†æ²¡æœ‰æ˜ç¡®æ ‡æ³¨æ—¥æœŸ")
            logger.info(f"     2. éœ€è¦æ›´æ·±å…¥çš„å†…å®¹åˆ†æ")
            logger.info(f"     3. Prompt æ•ˆæœéœ€è¦å¯¹æ¯”æµ‹è¯•éªŒè¯")

        logger.info(f"\nå»ºè®®:")
        logger.info(f"  1. æ‰‹åŠ¨è®¿é—®éƒ¨åˆ† URL éªŒè¯å†…å®¹æ—¶æ•ˆæ€§")
        logger.info(f"  2. æ‰§è¡Œä¸å¸¦ prompt çš„å¯¹æ¯”æµ‹è¯•")
        logger.info(f"  3. ä½¿ç”¨æ›´æ˜ç¡®æ ‡æ³¨æ—¥æœŸçš„ç½‘ç«™æµ‹è¯•")

        return True

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("\nğŸš€ å¼€å§‹æ‰§è¡Œï¼šçˆ¬å– + æ•°æ®åº“ä¿å­˜ + å†…å®¹åˆ†æ\n")

    success = await crawl_and_save()

    logger.info("\n" + "=" * 60)
    if success:
        logger.info("âœ… ä»»åŠ¡å®Œæˆ")
        logger.info("\nå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æŸ¥çœ‹ç»“æœ:")
        logger.info("  1. æ•°æ®åº“æŸ¥è¯¢: db.processed_results.find({task_id: '244746288889929728'})")
        logger.info("  2. è„šæœ¬åˆ†æ: python scripts/analyze_crawl_results.py")
    else:
        logger.info("âŒ ä»»åŠ¡å¤±è´¥")
    logger.info("=" * 60)

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
