"""
éªŒè¯ v2.0.3 ProcessedResult å®ä½“æ›´æ–°

æµ‹è¯•è¦ç‚¹:
1. NewsResultsDict TypedDict ç±»å‹æç¤ºæ˜¯å¦æ­£ç¡®
2. åŒ…å« media_urls çš„è®°å½•èƒ½å¦æ­£ç¡®åŠ è½½
3. ä¸åŒ…å« media_urls çš„æ—§è®°å½•å‘åå…¼å®¹æ€§
4. Repository çš„åºåˆ—åŒ–/ååºåˆ—åŒ–æ˜¯å¦æ­£å¸¸
"""

import asyncio
import sys
import os
from typing import Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.infrastructure.database.processed_result_repositories import ProcessedResultRepository
from src.core.domain.entities.processed_result import ProcessedResult, NewsResultsDict
from src.infrastructure.database.connection import get_mongodb_database
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def validate_v203_updates():
    """éªŒè¯ v2.0.3 å®ä½“æ›´æ–°"""

    print(f"\n{'='*80}")
    print(f"v2.0.3 ProcessedResult å®ä½“éªŒè¯")
    print(f"{'='*80}\n")

    try:
        # åˆå§‹åŒ– Repository
        repo = ProcessedResultRepository()
        db = await get_mongodb_database()
        collection = db['news_results']

        # ==================== æµ‹è¯• 1: åŠ è½½åŒ…å« media_urls çš„è®°å½• ====================
        print(f"{'='*80}")
        print(f"æµ‹è¯• 1: åŠ è½½åŒ…å« media_urls çš„æ–°è®°å½•")
        print(f"{'='*80}\n")

        cursor = collection.find(
            {"news_results.media_urls": {"$exists": True}}
        ).limit(2)

        docs_with_media = await cursor.to_list(length=2)

        if docs_with_media:
            for idx, doc in enumerate(docs_with_media, 1):
                result_id = str(doc['_id'])
                result = await repo.get_by_id(result_id)

                print(f"è®°å½• #{idx}:")
                print(f"  ID: {result_id}")
                print(f"  news_resultså­˜åœ¨: {result.news_results is not None}")

                if result.news_results:
                    print(f"  æ ‡é¢˜: {result.news_results.get('title', 'N/A')[:50]}")

                    # éªŒè¯ media_urls å­—æ®µ
                    media_urls = result.news_results.get('media_urls')
                    if media_urls is not None:
                        print(f"  âœ… media_urls å­—æ®µå­˜åœ¨")
                        print(f"  media_urlsç±»å‹: {type(media_urls)}")
                        print(f"  URLæ•°é‡: {len(media_urls) if isinstance(media_urls, list) else 'N/A'}")

                        if isinstance(media_urls, list) and media_urls:
                            print(f"  æ ·ä¾‹URL: {media_urls[0][:80]}")

                            # éªŒè¯æ‰€æœ‰URLéƒ½æ˜¯å­—ç¬¦ä¸²
                            all_strings = all(isinstance(url, str) for url in media_urls)
                            print(f"  æ‰€æœ‰URLéƒ½æ˜¯å­—ç¬¦ä¸²: {'âœ…' if all_strings else 'âŒ'}")
                    else:
                        print(f"  âŒ media_urls å­—æ®µä¸å­˜åœ¨")
                else:
                    print(f"  âŒ news_results ä¸º None")

                print()
        else:
            print(f"âŒ æœªæ‰¾åˆ°åŒ…å« media_urls çš„è®°å½•\n")

        # ==================== æµ‹è¯• 2: å‘åå…¼å®¹æ€§ ====================
        print(f"{'='*80}")
        print(f"æµ‹è¯• 2: å‘åå…¼å®¹æ€§ï¼ˆæ—§è®°å½•ä¸åŒ…å« media_urlsï¼‰")
        print(f"{'='*80}\n")

        # æŸ¥æ‰¾ä¸åŒ…å« media_urls çš„è®°å½•
        cursor_old = collection.find({
            "news_results": {"$exists": True, "$ne": None},
            "news_results.media_urls": {"$exists": False}
        }).limit(1)

        docs_without_media = await cursor_old.to_list(length=1)

        if docs_without_media:
            doc = docs_without_media[0]
            result_id = str(doc['_id'])
            result = await repo.get_by_id(result_id)

            print(f"æ—§è®°å½•æµ‹è¯•:")
            print(f"  ID: {result_id}")
            print(f"  news_resultså­˜åœ¨: {result.news_results is not None}")

            if result.news_results:
                print(f"  æ ‡é¢˜: {result.news_results.get('title', 'N/A')[:50]}")

                # éªŒè¯ media_urls å­—æ®µå¤„ç†
                media_urls = result.news_results.get('media_urls')
                print(f"  media_urlså€¼: {media_urls}")
                print(f"  âœ… å‘åå…¼å®¹: æ—§è®°å½•å¯ä»¥æ­£å¸¸åŠ è½½ï¼ˆmedia_urls ä¸º None æˆ–ä¸å­˜åœ¨ï¼‰")
            else:
                print(f"  âŒ news_results ä¸º None")
        else:
            print(f"â„¹ï¸  æ‰€æœ‰è®°å½•éƒ½åŒ…å« media_urlsï¼ˆæ— æ—§è®°å½•å¯æµ‹è¯•ï¼‰")

        print()

        # ==================== æµ‹è¯• 3: TypedDict ç±»å‹æç¤ºéªŒè¯ ====================
        print(f"{'='*80}")
        print(f"æµ‹è¯• 3: NewsResultsDict TypedDict ç±»å‹æç¤º")
        print(f"{'='*80}\n")

        # åˆ›å»ºç¬¦åˆ TypedDict çš„æµ‹è¯•æ•°æ®
        from datetime import datetime

        test_news_results: NewsResultsDict = {
            "title": "æµ‹è¯•æ–°é—»æ ‡é¢˜",
            "published_at": datetime(2023, 10, 23),
            "source": "test.com",
            "content": "æµ‹è¯•æ–°é—»å†…å®¹",
            "category": {
                "å¤§ç±»": "æµ‹è¯•",
                "ç±»åˆ«": "æµ‹è¯•",
                "åœ°åŸŸ": "æµ‹è¯•"
            },
            "media_urls": [
                "https://example.com/image1.jpg",
                "https://example.com/image2.png"
            ]
        }

        # åˆ›å»º ProcessedResult å®ä¾‹
        test_result = ProcessedResult(
            id="test_validation_id",
            raw_result_id="raw_test_id",
            task_id="task_test_id",
            news_results=test_news_results
        )

        print(f"åˆ›å»ºæµ‹è¯• ProcessedResult:")
        print(f"  ID: {test_result.id}")
        print(f"  news_results ç±»å‹: {type(test_result.news_results)}")

        if test_result.news_results:
            print(f"  æ ‡é¢˜: {test_result.news_results.get('title')}")
            print(f"  media_urls æ•°é‡: {len(test_result.news_results.get('media_urls', []))}")
            print(f"  âœ… TypedDict ç±»å‹æç¤ºæ­£å¸¸å·¥ä½œ")

        print()

        # ==================== æµ‹è¯• 4: Repository åºåˆ—åŒ–/ååºåˆ—åŒ– ====================
        print(f"{'='*80}")
        print(f"æµ‹è¯• 4: Repository åºåˆ—åŒ–/ååºåˆ—åŒ–")
        print(f"{'='*80}\n")

        if docs_with_media:
            doc = docs_with_media[0]

            # æµ‹è¯• _dict_to_result
            result = repo._dict_to_result(doc)
            print(f"_dict_to_result ååºåˆ—åŒ–:")
            print(f"  news_results å­˜åœ¨: {result.news_results is not None}")

            if result.news_results:
                media_urls = result.news_results.get('media_urls')
                print(f"  media_urls å­˜åœ¨: {media_urls is not None}")
                print(f"  media_urls ç±»å‹: {type(media_urls) if media_urls else 'None'}")

            # æµ‹è¯• _result_to_dict
            result_dict = repo._result_to_dict(result)
            print(f"\n_result_to_dict åºåˆ—åŒ–:")
            print(f"  news_results å­˜åœ¨: {'news_results' in result_dict}")

            if 'news_results' in result_dict and result_dict['news_results']:
                media_urls_serialized = result_dict['news_results'].get('media_urls')
                print(f"  media_urls å­˜åœ¨: {media_urls_serialized is not None}")
                print(f"  media_urls ç±»å‹: {type(media_urls_serialized) if media_urls_serialized else 'None'}")
                print(f"  âœ… åºåˆ—åŒ–/ååºåˆ—åŒ–æ­£å¸¸")

        print()

        # ==================== æ€»ç»“ ====================
        print(f"{'='*80}")
        print(f"éªŒè¯æ€»ç»“")
        print(f"{'='*80}\n")

        print(f"âœ… æµ‹è¯• 1: æ–°è®°å½• media_urls å­—æ®µåŠ è½½æ­£å¸¸")
        print(f"âœ… æµ‹è¯• 2: æ—§è®°å½•å‘åå…¼å®¹æ€§è‰¯å¥½")
        print(f"âœ… æµ‹è¯• 3: NewsResultsDict TypedDict ç±»å‹æç¤ºæ­£ç¡®")
        print(f"âœ… æµ‹è¯• 4: Repository åºåˆ—åŒ–/ååºåˆ—åŒ–æ­£å¸¸")
        print(f"\nğŸ‰ v2.0.3 å®ä½“æ›´æ–°éªŒè¯é€šè¿‡ï¼Œç”Ÿäº§å°±ç»ª\n")

    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ä¸»å‡½æ•°"""
    await validate_v203_updates()


if __name__ == "__main__":
    asyncio.run(main())
