#!/usr/bin/env python
"""
æµ‹è¯•å®Œæ•´å†…å®¹æŠ“å–

éªŒè¯Firecrawl APIä½¿ç”¨scrapeOptionsåèƒ½å¦è·å–å®Œæ•´ç½‘é¡µå†…å®¹
"""

import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.infrastructure.search.firecrawl_search_adapter import FirecrawlSearchAdapter
from src.core.domain.entities.search_config import UserSearchConfig
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def test_content_scraping():
    """æµ‹è¯•å†…å®¹æŠ“å–åŠŸèƒ½"""

    logger.info("=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•: Firecrawl API å®Œæ•´å†…å®¹æŠ“å–")
    logger.info("=" * 80)

    try:
        # åˆå§‹åŒ–é€‚é…å™¨
        adapter = FirecrawlSearchAdapter()

        # åˆ›å»ºæœç´¢é…ç½®ï¼Œé™åˆ¶ä¸º2æ¡ç»“æœä»¥èŠ‚çœAPIé¢åº¦
        user_config = UserSearchConfig(
            template_name="default",
            overrides={
                "limit": 2,  # åªè¯·æ±‚2æ¡ç»“æœ
                "scrape_formats": ["markdown", "html", "links"],  # è·å–å®Œæ•´å†…å®¹
                "only_main_content": True  # åªè¦ä¸»è¦å†…å®¹
            }
        )

        # æ‰§è¡Œæœç´¢
        logger.info("\nğŸ“Š æ‰§è¡Œæœç´¢:")
        logger.info(f"   - æŸ¥è¯¢: 'Python best practices 2024'")
        logger.info(f"   - é™åˆ¶: 2æ¡ç»“æœ")
        logger.info(f"   - æŠ“å–æ ¼å¼: markdown, html, links")
        logger.info("\nâ³ æ­£åœ¨è°ƒç”¨APIï¼ˆå¯èƒ½éœ€è¦10-30ç§’ï¼Œå› ä¸ºè¦æŠ“å–å®Œæ•´å†…å®¹ï¼‰...\n")

        batch = await adapter.search(
            query="Python best practices 2024",
            user_config=user_config,
            task_id="content_test"
        )

        # æ£€æŸ¥ç»“æœ
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š APIè°ƒç”¨ç»“æœ:")
        logger.info("=" * 80)
        logger.info(f"\nâœ… è°ƒç”¨çŠ¶æ€: {'æˆåŠŸ' if batch.success else 'å¤±è´¥'}")
        logger.info(f"   - æ‰§è¡Œæ—¶é—´: {batch.execution_time_ms}ms")
        logger.info(f"   - æ¶ˆè€—ç§¯åˆ†: {batch.credits_used}")
        logger.info(f"   - è¿”å›ç»“æœæ•°: {batch.returned_count}")

        if not batch.success:
            logger.error(f"\nâŒ é”™è¯¯ä¿¡æ¯: {batch.error_message}")
            return False

        # æ£€æŸ¥å†…å®¹
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” å†…å®¹æ£€æŸ¥:")
        logger.info("=" * 80)

        for i, result in enumerate(batch.results, 1):
            logger.info(f"\nã€ç»“æœ {i}ã€‘")
            logger.info(f"   ğŸ“„ æ ‡é¢˜: {result.title}")
            logger.info(f"   ğŸ”— URL: {result.url}")
            logger.info(f"   ğŸ“ æ‘˜è¦é•¿åº¦: {len(result.snippet) if result.snippet else 0} å­—ç¬¦")

            # æ£€æŸ¥contentå­—æ®µ
            logger.info(f"\n   ğŸ“¦ content å­—æ®µ:")
            if result.content:
                logger.info(f"      âœ… æœ‰å†…å®¹ - é•¿åº¦: {len(result.content)} å­—ç¬¦")
                logger.info(f"      é¢„è§ˆ (å‰300å­—ç¬¦):")
                logger.info(f"      {'-' * 60}")
                logger.info(f"      {result.content[:300]}...")
                logger.info(f"      {'-' * 60}")
            else:
                logger.warning(f"      âŒ contentä¸ºç©º")

            # æ£€æŸ¥markdown_contentå­—æ®µ
            logger.info(f"\n   ğŸ“¦ markdown_content å­—æ®µ:")
            if result.markdown_content:
                logger.info(f"      âœ… æœ‰markdown - é•¿åº¦: {len(result.markdown_content)} å­—ç¬¦")
            else:
                logger.warning(f"      âŒ markdown_contentä¸ºç©º")

            # æ£€æŸ¥metadataä¸­çš„htmlå’Œlinks
            logger.info(f"\n   ğŸ“¦ metadata å­—æ®µ:")
            if result.metadata.get('html_content'):
                logger.info(f"      âœ… æœ‰html_content - é•¿åº¦: {len(result.metadata['html_content'])} å­—ç¬¦")
            else:
                logger.warning(f"      âŒ æ²¡æœ‰html_content")

            if result.metadata.get('extracted_links'):
                links_count = len(result.metadata['extracted_links'])
                logger.info(f"      âœ… æå–äº† {links_count} ä¸ªé“¾æ¥")
                if links_count > 0:
                    logger.info(f"      å‰3ä¸ªé“¾æ¥: {result.metadata['extracted_links'][:3]}")
            else:
                logger.warning(f"      âŒ æ²¡æœ‰æå–é“¾æ¥")

        # æœ€ç»ˆè¯„ä¼°
        logger.info("\n" + "=" * 80)
        logger.info("âœ¨ æµ‹è¯•è¯„ä¼°:")
        logger.info("=" * 80)

        has_content = any(r.content for r in batch.results)
        has_markdown = any(r.markdown_content for r in batch.results)

        if has_content and has_markdown:
            logger.info("\nâœ… æµ‹è¯•é€šè¿‡:")
            logger.info("   âœ… APIæˆåŠŸè¿”å›å®Œæ•´å†…å®¹")
            logger.info("   âœ… contentå­—æ®µæœ‰æ•°æ®")
            logger.info("   âœ… markdown_contentå­—æ®µæœ‰æ•°æ®")
            logger.info("\nğŸ’¡ è¯´æ˜:")
            logger.info("   - Firecrawl APIçš„scrapeOptionså‚æ•°å·¥ä½œæ­£å¸¸")
            logger.info("   - ç°åœ¨å¯ä»¥è·å–å®Œæ•´çš„ç½‘é¡µå†…å®¹ç”¨äºAIåˆ†æ")
            return True
        else:
            logger.error("\nâŒ æµ‹è¯•å¤±è´¥:")
            if not has_content:
                logger.error("   âŒ contentå­—æ®µä»ç„¶ä¸ºç©º")
            if not has_markdown:
                logger.error("   âŒ markdown_contentå­—æ®µä¸ºç©º")
            logger.error("\nğŸ’¡ å¯èƒ½åŸå› :")
            logger.error("   - scrapeOptionså‚æ•°æ²¡æœ‰æ­£ç¡®ä¼ é€’")
            logger.error("   - APIå“åº”æ ¼å¼æœ‰å˜åŒ–")
            logger.error("   - æŸäº›ç½‘ç«™é˜»æ­¢äº†å†…å®¹æŠ“å–")
            return False

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    success = await test_content_scraping()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
